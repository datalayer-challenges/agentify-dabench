# Evaluation Results

This folder contains Pydantic evaluation reports generated by the Green Agent.

## File Naming Convention

Files are named with the pattern: `pydantic_eval_report_YYYYMMDD_HHMMSS_GREEN_MODEL_vs_WHITE_MODEL_XYZcases.json`

- `YYYYMMDD_HHMMSS`: Timestamp when the evaluation was completed
- `GREEN_MODEL`: Model used by the Green Agent (evaluator)
- `WHITE_MODEL`: Model used by the White Agent (agent under test)
- `XYZ`: Number of test cases evaluated

Example: `pydantic_eval_report_20251127_131133_gpt_4o_vs_gpt_4o_257cases.json`

## File Structure

Each JSON file contains:

```json
{
  "metadata": {
    "evaluation_time_seconds": 291.8,
    "total_cases": 5,
    "timestamp": "20251120_143052",
    "success_rate": 0.2,
    "generated_by": "Green Agent - Pydantic Eval"
  },
  "report": {
    // Detailed pydantic evaluation report with:
    // - Individual case results
    // - LLM judge evaluations
    // - Assertions and scoring
    // - Duration and performance metrics
  }
}
```

## Usage

These reports can be used for:
- Performance analysis of the white agent
- Tracking improvement over time
- Detailed debugging of specific test case failures
- Generating summary statistics and trends