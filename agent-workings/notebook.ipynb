{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "6be22d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 400 entries, 0 to 399\n",
       "Data columns (total 12 columns):\n",
       " #   Column      Non-Null Count  Dtype  \n",
       "---  ------      --------------  -----  \n",
       " 0   Unnamed: 0  400 non-null    int64  \n",
       " 1   Income      400 non-null    float64\n",
       " 2   Limit       400 non-null    int64  \n",
       " 3   Rating      400 non-null    int64  \n",
       " 4   Cards       400 non-null    int64  \n",
       " 5   Age         400 non-null    int64  \n",
       " 6   Education   400 non-null    int64  \n",
       " 7   Gender      400 non-null    object \n",
       " 8   Student     400 non-null    object \n",
       " 9   Married     400 non-null    object \n",
       " 10  Ethnicity   400 non-null    object \n",
       " 11  Balance     400 non-null    int64  \n",
       "dtypes: float64(1), int64(7), object(4)\n",
       "memory usage: 37.6+ KB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "    Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       " 0           1   14.891   3606     283      2   34         11    Male      No   \n",
       " 1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       " 2           3  104.593   7075     514      4   71         11    Male      No   \n",
       " 3           4  148.924   9504     681      3   36         11  Female      No   \n",
       " 4           5   55.882   4897     357      2   68         16    Male      No   \n",
       " \n",
       "   Married  Ethnicity  Balance  \n",
       " 0     Yes  Caucasian      333  \n",
       " 1     Yes      Asian      903  \n",
       " 2      No      Asian      580  \n",
       " 3      No      Asian      964  \n",
       " 4     Yes  Caucasian      331  )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Credit.csv dataset\n",
    "file_path = './data/Credit.csv'\n",
    "credit_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic info and first few rows to understand the data\n",
    "credit_data.info(), credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "b802b0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean and standard deviation of the Balance column\n",
    "balance_mean = credit_data['Balance'].mean()\n",
    "balance_std = credit_data['Balance'].std()\n",
    "\n",
    "# Calculate Z-scores for Balance column\n",
    "credit_data['Balance_Z'] = (credit_data['Balance'] - balance_mean) / balance_std\n",
    "\n",
    "# Identify outliers: absolute value of z-score greater than 3\n",
    "outliers = credit_data[np.abs(credit_data['Balance_Z']) > 3]\n",
    "outliers_count = outliers.shape[0]\n",
    "outliers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5e97ea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "3           4  148.924   9504     681      3   36         11  Female      No   \n",
       "4           5   55.882   4897     357      2   68         16    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  \n",
       "0     Yes  Caucasian      333  \n",
       "1     Yes      Asian      903  \n",
       "2      No      Asian      580  \n",
       "3      No      Asian      964  \n",
       "4     Yes  Caucasian      331  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Credit.csv file\n",
    "df = pd.read_csv('./data/Credit.csv')\n",
    "\n",
    "# Display the first few rows to understand the structure of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "37936ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.86)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the Pearson correlation coefficient between 'Limit' and 'Balance'\n",
    "correlation = df['Limit'].corr(df['Balance'])\n",
    "\n",
    "# Round the result to two decimal places\n",
    "correlation_rounded = round(correlation, 2)\n",
    "correlation_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "1caf573d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    34\n",
       "1    82\n",
       "2    71\n",
       "3    36\n",
       "4    68\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the data\n",
    "file_path = './data/Credit.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data['Age'].head()  # Show first few entries of Age column to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "14b6cee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Normal'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the Shapiro-Wilk test for normality\n",
    "stat, p_value = shapiro(data['Age'])\n",
    "\n",
    "# Define significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if the distribution is normal or not\n",
    "is_normal = \"Normal\" if p_value > alpha else \"Not Normal\"\n",
    "\n",
    "is_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "acd04eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "3           4  148.924   9504     681      3   36         11  Female      No   \n",
       "4           5   55.882   4897     357      2   68         16    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  \n",
       "0     Yes  Caucasian      333  \n",
       "1     Yes      Asian      903  \n",
       "2      No      Asian      580  \n",
       "3      No      Asian      964  \n",
       "4     Yes  Caucasian      331  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Credit.csv file\n",
    "credit_data = pd.read_csv('./data/Credit.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to verify loading\n",
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "89f2bb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(45.22), np.float64(35.24))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation of the \"Income\" column\n",
    "mean_income = round(credit_data['Income'].mean(), 2)\n",
    "std_dev_income = round(credit_data['Income'].std(), 2)\n",
    "\n",
    "mean_income, std_dev_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "9d307a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   year       pop  lifeexp   gdppercap      country continent\n",
       "0  1952   8425333   28.801  779.445314  afghanistan      asia\n",
       "1  1957   9240934   30.332  820.853030  afghanistan      asia\n",
       "2  1962  10267083   31.997  853.100710  afghanistan      asia\n",
       "3  1967  11537966   34.020  836.197138  afghanistan      asia\n",
       "4  1972  13079460   36.088  739.981106  afghanistan      asia"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/gapminder_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a40c7ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1704.000000\n",
       "mean        0.014439\n",
       "std         0.055204\n",
       "min         0.000000\n",
       "25%         0.000452\n",
       "50%         0.001723\n",
       "75%         0.008171\n",
       "max         1.000000\n",
       "Name: normalized_feature, dtype: float64\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "        pop   gdppercap  normalized_feature\n",
       "0   8425333  779.445314            0.000504\n",
       "1   9240934  820.853030            0.000582\n",
       "2  10267083  853.100710            0.000673\n",
       "3  11537966  836.197138            0.000742\n",
       "4  13079460  739.981106            0.000744"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the new feature\n",
    "new_feature = df['pop'] * df['gdppercap']\n",
    "\n",
    "# Min-max normalization\n",
    "def min_max_scaling(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "normalized_feature = min_max_scaling(new_feature)\n",
    "\n",
    "# Save to dataframe\n",
    "df['normalized_feature'] = normalized_feature\n",
    "\n",
    "# Display statistics and first rows\n",
    "print(df['normalized_feature'].describe())\n",
    "df[['pop', 'gdppercap', 'normalized_feature']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "70bb4375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shapiro-Wilk test p-value: 6.0224960176690504e-64\n",
       "Does the normalized feature follow a normal distribution? no\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Shapiro-Wilk test\n",
    "stat, p_value = shapiro(df['normalized_feature'])\n",
    "\n",
    "# Check if the normalized feature follows a normal distribution\n",
    "is_normal = 'yes' if p_value > 0.05 else 'no'\n",
    "\n",
    "print(f\"Shapiro-Wilk test p-value: {p_value}\")\n",
    "print(f\"Does the normalized feature follow a normal distribution? {is_normal}\")\n",
    "\n",
    "is_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "eac8c2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asia': (np.float64(0.38), np.float64(0.0), 'non-significant'),\n",
       " 'europe': (np.float64(0.78), np.float64(0.0), 'significant'),\n",
       " 'africa': (np.float64(0.43), np.float64(0.0), 'non-significant'),\n",
       " 'americas': (np.float64(0.56), np.float64(0.0), 'significant'),\n",
       " 'oceania': (np.float64(0.96), np.float64(0.0), 'significant')}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform correlation analysis for each continent\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "results = {}\n",
    "\n",
    "for cont in continents:\n",
    "    subset = gapminder[gapminder['continent'] == cont]\n",
    "    r, p = pearsonr(subset['lifeexp'], subset['gdppercap'])\n",
    "    significance = 'significant' if p < 0.05 and abs(r) >= 0.5 else 'non-significant'\n",
    "    results[cont] = (round(r, 2), round(p, 4), significance)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "a6404661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   year       pop  lifeexp   gdppercap      country continent\n",
       "0  1952   8425333   28.801  779.445314  afghanistan      asia\n",
       "1  1957   9240934   30.332  820.853030  afghanistan      asia\n",
       "2  1962  10267083   31.997  853.100710  afghanistan      asia\n",
       "3  1967  11537966   34.020  836.197138  afghanistan      asia\n",
       "4  1972  13079460   36.088  739.981106  afghanistan      asia"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/gapminder_cleaned.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "0f415e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.False_, np.float64(3.54), np.float64(0.54))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new feature representing the GDP per capita in logarithmic scale (base 10)\n",
    "# Assuming all GDP per capita values are positive as per the constraint\n",
    "\n",
    "df['log10_gdppercap'] = np.log10(df['gdppercap'])\n",
    "\n",
    "# Check for NaN values, calculate mean and std for the new feature\n",
    "has_nan_values = df['log10_gdppercap'].isna().any()\n",
    "new_feature_mean = round(df['log10_gdppercap'].mean(), 2)\n",
    "new_feature_std = round(df['log10_gdppercap'].std(), 2)\n",
    "\n",
    "has_nan_values, new_feature_mean, new_feature_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "dc62f75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   year       pop  lifeexp   gdppercap      country continent\n",
       "0  1952   8425333   28.801  779.445314  afghanistan      asia\n",
       "1  1957   9240934   30.332  820.853030  afghanistan      asia\n",
       "2  1962  10267083   31.997  853.100710  afghanistan      asia\n",
       "3  1967  11537966   34.020  836.197138  afghanistan      asia\n",
       "4  1972  13079460   36.088  739.981106  afghanistan      asia"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/gapminder_cleaned.csv')\n",
    "\n",
    "# Display the first few rows to understand the structure of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "fee49564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.03), np.float64(0.2909))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns to the case-sensitive correct names for clarity\n",
    "# (As the question uses \"Pop\" and \"Gdppercap\" indicating case-sensitivity, we check the dataset for exact names first)\n",
    "df.rename(columns={'pop':'Pop', 'gdppercap':'Gdppercap'}, inplace=True)\n",
    "\n",
    "# Calculate Pearson correlation coefficient and p-value for Pop and Gdppercap\n",
    "r_value, p_value = pearsonr(df['Pop'], df['Gdppercap'])\n",
    "\n",
    "# Print results rounded to requested decimal places\n",
    "r_value_rounded = round(r_value, 2)\n",
    "p_value_rounded = round(p_value, 4)\n",
    "r_value_rounded, p_value_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "8a609db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   year       pop  lifeexp   gdppercap      country continent\n",
       "0  1952   8425333   28.801  779.445314  afghanistan      asia\n",
       "1  1957   9240934   30.332  820.853030  afghanistan      asia\n",
       "2  1962  10267083   31.997  853.100710  afghanistan      asia\n",
       "3  1967  11537966   34.020  836.197138  afghanistan      asia\n",
       "4  1972  13079460   36.088  739.981106  afghanistan      asia\n",
       "Index(['year', 'pop', 'lifeexp', 'gdppercap', 'country', 'continent'], dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/gapminder_cleaned.csv')\n",
    "# Display the first few rows and column names to understand the data structure\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "efed28c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not normal'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the normality test on the 'gdppercap' column\n",
    "stat, p_value = normaltest(df['gdppercap'])\n",
    "\n",
    "# Determine distribution type based on p-value\n",
    "distribution_type = \"normal\" if p_value > 0.05 else \"not normal\"\n",
    "distribution_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e3697ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/auto-mpg.csv')\n",
    "\n",
    "# Display the first few rows to check the structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "89d7a39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((313, 2), (79, 2), (313,), (79,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features and target variable\n",
    "X = df[['weight', 'acceleration']]\n",
    "y = df['mpg']\n",
    "\n",
    "# Split the dataset into training and testing sets with the ratio 8:2 and random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "407c8228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a linear regression model on the training set\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "47030cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the testing set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "e2242f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.66"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean squared error (MSE) on the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "491b3350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/auto-mpg.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "73232471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
       "       'acceleration', 'modelyear', 'origin'],\n",
       "      dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/auto-mpg.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and columns info to understand the structure and check for relevant columns\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "dd5f9cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  JAMES LOGAN           Unnamed: 1           Unnamed: 2        Unnamed: 3  \\\n",
       "0        Wins                 Name                 Code            School   \n",
       "1           5     Gatlin & Ramarao  Archbishop Mitty GR  Archbishop Mitty   \n",
       "2           5  Lahiri & Ponnuswamy  Archbishop Mitty LP  Archbishop Mitty   \n",
       "3           4    Gupta & Chatradhi  Archbishop Mitty GC  Archbishop Mitty   \n",
       "4           4     Patwa & Aggarwal  Archbishop Mitty PA  Archbishop Mitty   \n",
       "\n",
       "   Unnamed: 4  Unnamed: 5          Unnamed: 6  Unnamed: 7 Unnamed: 8  \n",
       "0         NaN         NaN  STANDARD TEAM NAME  NUM ROUNDS  WEIGHTING  \n",
       "1         NaN         NaN            Mitty GR           5        0.8  \n",
       "2         NaN         NaN            Mitty PL         NaN        NaN  \n",
       "3         NaN         NaN          Mitty GuCh         NaN        NaN  \n",
       "4         NaN         NaN            Mitty PA         NaN        NaN  \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/Current_Logan.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure and check the relevant columns\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "f2e14578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   temp  humidity  pressure    description          dt  \\\n",
       "0           0  32.18        54      1010  moderate rain  1437730851   \n",
       "1           1  32.37        62      1010  moderate rain  1437734494   \n",
       "2           2  32.79        75      1009  moderate rain  1437738046   \n",
       "3           3  32.75        79      1009  moderate rain  1437741580   \n",
       "4           4  32.72        70      1008  moderate rain  1437745188   \n",
       "\n",
       "   wind_speed  wind_deg     city                  day  dist  \n",
       "0        2.11   330.003  Ravenna  2015-07-24 11:40:51     8  \n",
       "1        2.60    20.000  Ravenna  2015-07-24 12:41:34     8  \n",
       "2        3.60    70.000  Ravenna  2015-07-24 13:40:46     8  \n",
       "3        5.10    70.000  Ravenna  2015-07-24 14:39:40     8  \n",
       "4        3.10    10.000  Ravenna  2015-07-24 15:39:48     8  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df = pd.read_csv('./data/ravenna_250715.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "e7b781b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([], dtype=float64))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Z-scores for wind_speed\n",
    "wind_speed_zscores = zscore(df['wind_speed'])\n",
    "\n",
    "# Identify outliers based on Z-score threshold\n",
    "outliers = df[(wind_speed_zscores > 3) | (wind_speed_zscores < -3)]\n",
    "number_of_outliers = len(outliers)\n",
    "outlier_values = outliers['wind_speed'].values\n",
    "\n",
    "number_of_outliers, outlier_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ac0b0edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.29), np.float64(1.15))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since there are no outliers by Z-score > 3 or < -3, the mean and std dev are calculated on original dataset\n",
    "mean_wind_speed = round(df['wind_speed'].mean(), 2)\n",
    "std_deviation_wind_speed = round(df['wind_speed'].std(), 2)\n",
    "\n",
    "mean_wind_speed, std_deviation_wind_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "8feefba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95779505.343"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing sets (70% train, 30% test) and perform linear regression training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define features and target\n",
    "X = synth_df[['MedInc', 'AveRooms', 'Population', 'Latitude', 'Longitude']]\n",
    "y = synth_df['MedianHouseValue']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Present result rounded to three decimal places\n",
    "mse_rounded = round(mse, 3)\n",
    "mse_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "9ee8beea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  0.9298      36.0  3.676162   1.100450      2664.0  3.994003     33.93   \n",
       "1  2.7006      17.0  4.499388   1.039780      3331.0  2.038556     32.79   \n",
       "2  5.0286      30.0  6.184375   1.068750       999.0  3.121875     34.89   \n",
       "3  3.9038      21.0  3.586357   0.982583      1486.0  2.156749     37.36   \n",
       "4  7.1754      52.0  7.429487   1.051282       779.0  2.496795     37.90   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -118.25             1.000  \n",
       "1    -117.03             1.663  \n",
       "2    -120.43             1.580  \n",
       "3    -122.02             2.438  \n",
       "4    -122.28             3.629  \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/my_test_01.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "afb8e802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.1226, 1.221)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean and standard deviation of the MedianHouseValue column\n",
    "median_house_value_list = df['MedianHouseValue'].tolist()\n",
    "\n",
    "mean_value = mean(median_house_value_list)\n",
    "std_dev = stdev(median_house_value_list)\n",
    "\n",
    "# Round to four decimal places\n",
    "mean_value_rounded = round(mean_value, 4)\n",
    "std_dev_rounded = round(std_dev, 4)\n",
    "\n",
    "mean_value_rounded, std_dev_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4e14eb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 2176 entries, 0 to 2175\n",
       "Data columns (total 7 columns):\n",
       " #   Column     Non-Null Count  Dtype  \n",
       "---  ------     --------------  -----  \n",
       " 0   Date       2176 non-null   object \n",
       " 1   Open       2175 non-null   float64\n",
       " 2   High       2175 non-null   float64\n",
       " 3   Low        2175 non-null   float64\n",
       " 4   Close      2175 non-null   float64\n",
       " 5   Adj Close  2175 non-null   float64\n",
       " 6   Volume     2175 non-null   float64\n",
       "dtypes: float64(6), object(1)\n",
       "memory usage: 119.1+ KB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(         Date        Open        High         Low       Close   Adj Close  \\\n",
       " 0  2014-09-17  465.864014  468.174011  452.421997  457.334015  457.334015   \n",
       " 1  2014-09-18  456.859985  456.859985  413.104004  424.440002  424.440002   \n",
       " 2  2014-09-19  424.102997  427.834991  384.532013  394.795990  394.795990   \n",
       " 3  2014-09-20  394.673004  423.295990  389.882996  408.903992  408.903992   \n",
       " 4  2014-09-21  408.084991  412.425995  393.181000  398.821014  398.821014   \n",
       " \n",
       "        Volume  \n",
       " 0  21056800.0  \n",
       " 1  34483200.0  \n",
       " 2  37919700.0  \n",
       " 3  36863600.0  \n",
       " 4  26580100.0  ,\n",
       " None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/YAHOO-BTC_USD_D.csv')\n",
    "# Display the first few rows and info to understand data structure\n",
    "(df.head(), df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "511fac32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a scatter plot to visualize the relationship between 'High' and 'Low' prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='High', y='Low', data=df_clean)\n",
    "plt.title('Scatter Plot of High vs Low Prices')\n",
    "plt.xlabel('High Price')\n",
    "plt.ylabel('Low Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e79d21ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          JDTDB              Calendar Date (TDB)             X             Y  \\\n",
       "0  2.451215e+06   A.D. 1999-Feb-05 00:58:29.0000 -2.887479e+08  2.475795e+07   \n",
       "1  2.451216e+06   A.D. 1999-Feb-06 00:58:29.0000 -2.894466e+08  2.303593e+07   \n",
       "2  2.451217e+06   A.D. 1999-Feb-07 00:58:29.0000 -2.901336e+08  2.131300e+07   \n",
       "3  2.451218e+06   A.D. 1999-Feb-08 00:58:29.0000 -2.908092e+08  1.958922e+07   \n",
       "4  2.451219e+06   A.D. 1999-Feb-09 00:58:29.0000 -2.914731e+08  1.786466e+07   \n",
       "\n",
       "              Z  Unnamed: 5  \n",
       "0  2.212686e+07         NaN  \n",
       "1  2.286679e+07         NaN  \n",
       "2  2.360580e+07         NaN  \n",
       "3  2.434388e+07         NaN  \n",
       "4  2.508099e+07         NaN  \n",
       "Index(['JDTDB', 'Calendar Date (TDB)', 'X', 'Y', 'Z', 'Unnamed: 5'], dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/DES=+2006261.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and columns for quick inspection\n",
    "print(df.head())\n",
    "\n",
    "# Check for the existence of the X-coordinate column\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "dc28caa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          JDTDB              Calendar Date (TDB)             X             Y  \\\n",
       "0  2.451215e+06   A.D. 1999-Feb-05 00:58:29.0000 -2.887479e+08  2.475795e+07   \n",
       "1  2.451216e+06   A.D. 1999-Feb-06 00:58:29.0000 -2.894466e+08  2.303593e+07   \n",
       "2  2.451217e+06   A.D. 1999-Feb-07 00:58:29.0000 -2.901336e+08  2.131300e+07   \n",
       "3  2.451218e+06   A.D. 1999-Feb-08 00:58:29.0000 -2.908092e+08  1.958922e+07   \n",
       "4  2.451219e+06   A.D. 1999-Feb-09 00:58:29.0000 -2.914731e+08  1.786466e+07   \n",
       "\n",
       "              Z  Unnamed: 5  \n",
       "0  2.212686e+07         NaN  \n",
       "1  2.286679e+07         NaN  \n",
       "2  2.360580e+07         NaN  \n",
       "3  2.434388e+07         NaN  \n",
       "4  2.508099e+07         NaN  \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/DES=+2006261.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure and check the relevant columns\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9e278017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "ZeroDivisionError                         Traceback (most recent call last)\n",
       "Cell In[413], line 18\n",
       "     15             x_coords.append(float(row['X-coordinate']))\n",
       "     17 # Calculate mean\n",
       "---> 18 mean_x = sum(x_coords) / len(x_coords)\n",
       "     20 # Calculate standard deviation\n",
       "     21 variance = sum((x - mean_x) ** 2 for x in x_coords) / len(x_coords)\n",
       "\n",
       "ZeroDivisionError: division by zero"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "# Path to the data file\n",
    "data_file = './data/DES=+2006261.csv'\n",
    "\n",
    "# Variables to store values of X-coordinate\n",
    "x_coords = []\n",
    "\n",
    "# Read the CSV file and extract X-coordinate data\n",
    "with open(data_file, mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if 'X-coordinate' in row and row['X-coordinate']:\n",
    "            x_coords.append(float(row['X-coordinate']))\n",
    "\n",
    "# Calculate mean\n",
    "mean_x = sum(x_coords) / len(x_coords)\n",
    "\n",
    "# Calculate standard deviation\n",
    "variance = sum((x - mean_x) ** 2 for x in x_coords) / len(x_coords)\n",
    "std_dev_x = math.sqrt(variance)\n",
    "\n",
    "# Round to three decimal places\n",
    "mean_x_rounded = round(mean_x, 3)\n",
    "std_dev_x_rounded = round(std_dev_x, 3)\n",
    "\n",
    "(mean_x_rounded, std_dev_x_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "4796d6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-259162995.016, 53447767.921)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust the code to use the correct column name 'X' for X-coordinate\n",
    "x_coords = []\n",
    "\n",
    "with open(data_file, mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if 'X' in row and row['X']:\n",
    "            x_coords.append(float(row['X']))\n",
    "\n",
    "mean_x = sum(x_coords) / len(x_coords)\n",
    "variance = sum((x - mean_x) ** 2 for x in x_coords) / len(x_coords)\n",
    "std_dev_x = math.sqrt(variance)\n",
    "\n",
    "mean_x_rounded = round(mean_x, 3)\n",
    "std_dev_x_rounded = round(std_dev_x, 3)\n",
    "\n",
    "(mean_x_rounded, std_dev_x_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "2e2e22be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         userID  trajID  poiID   startTime     endTime  #photo  trajLen  \\\n",
       "0  10297518@N00       1     20  1277719324  1277720832       6        1   \n",
       "1  10307040@N08       2      6  1382608644  1382608644       1        4   \n",
       "2  10307040@N08       2      8  1382607812  1382607812       1        4   \n",
       "3  10307040@N08       2     21  1382607761  1382607774       2        4   \n",
       "4  10307040@N08       2     22  1382607879  1382608628       9        4   \n",
       "\n",
       "   poiDuration  \n",
       "0         1508  \n",
       "1            0  \n",
       "2            0  \n",
       "3           13  \n",
       "4          749  \n",
       "Index(['userID', 'trajID', 'poiID', 'startTime', 'endTime', '#photo',\n",
       "       'trajLen', 'poiDuration'],\n",
       "      dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(1253.61), np.float64(2513.69))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/traj-Osak.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check first few rows and the column for trajectory duration\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "# Extract the trajectory duration column (assuming 'poiDuration' represents duration)\n",
    "durations = df['poiDuration']\n",
    "\n",
    "# Calculate Z-scores for the duration\n",
    "z_scores = zscore(durations)\n",
    "\n",
    "# Define threshold for outlier detection\n",
    "threshold = 2.5\n",
    "\n",
    "# Identify outliers\n",
    "outliers = (z_scores < -threshold) | (z_scores > threshold)\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_durations = durations[~outliers]\n",
    "\n",
    "# Calculate new mean and std dev after removing outliers\n",
    "mean_new = np.round(np.mean(filtered_durations), 2)\n",
    "std_dev_new = np.round(np.std(filtered_durations, ddof=0), 2)\n",
    "\n",
    "mean_new, std_dev_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "f0cb9b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight  Rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the abalone data\n",
    "abalone_df = pd.read_csv('./data/abalone.csv')\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "138b50db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/titanic_test.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e9f6368f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@correlation_coefficient[0.31], @p_value[0.0000], @relationship_type[nonlinear]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Pearson correlation coefficient (r) and p-value between SibSp and Parch\n",
    "r, p_value = pearsonr(df['SibSp'], df['Parch'])\n",
    "\n",
    "# Determine the relationship type based on constraints\n",
    "if p_value < 0.05:\n",
    "    if abs(r) >= 0.5:\n",
    "        relationship_type = 'linear'\n",
    "    else:\n",
    "        relationship_type = 'nonlinear'\n",
    "else:\n",
    "    relationship_type = 'none'\n",
    "\n",
    "# Format the results\n",
    "result = f\"@correlation_coefficient[{r:.2f}], @p_value[{p_value:.4f}], @relationship_type[{relationship_type}]\"\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "75208366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and show first few rows to verify the structure\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './data/titanic.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "6f2d0ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title\n",
       "Mr       517\n",
       "Miss     182\n",
       "Mrs      125\n",
       "Other     67\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the title from the 'Name' column as per the given constraints\n",
    "import re\n",
    "\n",
    "def extract_title(name):\n",
    "    match = re.search(r',\\s*([^\\s]+)\\.', name)\n",
    "    if match:\n",
    "        title = match.group(1)\n",
    "        if title in ['Mr', 'Mrs', 'Miss']:\n",
    "            return title\n",
    "    return 'Other'\n",
    "\n",
    "# Create the 'Title' column\n",
    "df['Title'] = df['Name'].apply(extract_title)\n",
    "\n",
    "# Check the distribution of the new 'Title' feature\n",
    "distribution = df['Title'].value_counts()\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "cfb87774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(38.29), np.float64(0.0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze if 'Title' is correlated with 'Pclass' using a chi-square test\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table\n",
    "table = pd.crosstab(df['Title'], df['Pclass'])\n",
    "\n",
    "# Perform Chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(table)\n",
    "\n",
    "# Output the results rounded as per instructions\n",
    "chi2_rounded = round(chi2, 2)\n",
    "p_rounded = round(p, 4)\n",
    "\n",
    "(chi2_rounded, p_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "40d2450f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   hotel_id                    hotel_name      city_name  star_rating  \\\n",
       "0     75737        Night Theater District  New York City          4.0   \n",
       "1     93401  Heritage Hotel New York City  New York City          2.5   \n",
       "2    224217     Clarion Hotel Park Avenue  New York City          2.5   \n",
       "3    488793             Solita Soho Hotel  New York City          3.0   \n",
       "4   1028569               Greenwich Hotel  New York City          5.0   \n",
       "\n",
       "   bubble_score  review_count hotel_type                       brand_name  \\\n",
       "0          40.0          2291      Hotel                      Night Hotel   \n",
       "1          35.0           968      Hotel                              NaN   \n",
       "2          35.0           462      Hotel                          Clarion   \n",
       "3          40.0           520      Hotel                Ascend Collection   \n",
       "4          45.0           582      Hotel  The Leading Hotels of the World   \n",
       "\n",
       "                      parent_brand_name  \n",
       "0                   Wyndham Hotel Group  \n",
       "1                                   NaN  \n",
       "2     Choice Hotels International, Inc.  \n",
       "3     Choice Hotels International, Inc.  \n",
       "4  The Leading Hotels of the World, Ltd  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/hotel_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c6afbd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   hotel_id                    hotel_name      city_name  star_rating  \\\n",
       " 0     75737        Night Theater District  New York City          4.0   \n",
       " 1     93401  Heritage Hotel New York City  New York City          2.5   \n",
       " 2    224217     Clarion Hotel Park Avenue  New York City          2.5   \n",
       " 3    488793             Solita Soho Hotel  New York City          3.0   \n",
       " 4   1028569               Greenwich Hotel  New York City          5.0   \n",
       " \n",
       "    bubble_score  review_count hotel_type                       brand_name  \\\n",
       " 0          40.0          2291      Hotel                      Night Hotel   \n",
       " 1          35.0           968      Hotel                              NaN   \n",
       " 2          35.0           462      Hotel                          Clarion   \n",
       " 3          40.0           520      Hotel                Ascend Collection   \n",
       " 4          45.0           582      Hotel  The Leading Hotels of the World   \n",
       " \n",
       "                       parent_brand_name  \n",
       " 0                   Wyndham Hotel Group  \n",
       " 1                                   NaN  \n",
       " 2     Choice Hotels International, Inc.  \n",
       " 3     Choice Hotels International, Inc.  \n",
       " 4  The Leading Hotels of the World, Ltd  ,\n",
       " Index(['hotel_id', 'hotel_name', 'city_name', 'star_rating', 'bubble_score',\n",
       "        'review_count', 'hotel_type', 'brand_name', 'parent_brand_name'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the hotel_data.csv dataset\n",
    "file_path = './data/hotel_data.csv'\n",
    "hotel_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and columns info to understand the structure\n",
    "hotel_data.head(), hotel_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "40ffaeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   hotel_id                    hotel_name      city_name  star_rating  \\\n",
       "0     75737        Night Theater District  New York City          4.0   \n",
       "1     93401  Heritage Hotel New York City  New York City          2.5   \n",
       "2    224217     Clarion Hotel Park Avenue  New York City          2.5   \n",
       "3    488793             Solita Soho Hotel  New York City          3.0   \n",
       "4   1028569               Greenwich Hotel  New York City          5.0   \n",
       "\n",
       "   bubble_score  review_count hotel_type                       brand_name  \\\n",
       "0          40.0          2291      Hotel                      Night Hotel   \n",
       "1          35.0           968      Hotel                              NaN   \n",
       "2          35.0           462      Hotel                          Clarion   \n",
       "3          40.0           520      Hotel                Ascend Collection   \n",
       "4          45.0           582      Hotel  The Leading Hotels of the World   \n",
       "\n",
       "                      parent_brand_name  \n",
       "0                   Wyndham Hotel Group  \n",
       "1                                   NaN  \n",
       "2     Choice Hotels International, Inc.  \n",
       "3     Choice Hotels International, Inc.  \n",
       "4  The Leading Hotels of the World, Ltd  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/hotel_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "e6a68d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0    name   dates_active  max_storm_cat  max_sust_wind   min_p  \\\n",
       "0           0  ARLENE  April19 21              1       43.44880   990.0   \n",
       "1           1    BRET   June19 20              1       43.44880  1007.0   \n",
       "2           2   CINDY   June20 23              1       52.13856   991.0   \n",
       "3           3    FOUR     July5 7              0       26.06928  1009.0   \n",
       "4           4     DON   July17 18              1       43.44880  1005.0   \n",
       "\n",
       "                                      areas_affected  damage_USD  deaths  \\\n",
       "0                                                NaN         0.0     0.0   \n",
       "1  Guyana, Venezuela, Trinidad and Tobago, Windwa...   3000000.0     2.0   \n",
       "2  Honduras, Belize, Cayman Islands, Yucatn Peni...  25000000.0     2.0   \n",
       "3                                                NaN         0.0     0.0   \n",
       "4    Windward Islands, Barbados, Trinidad and Tobago         0.0     0.0   \n",
       "\n",
       "   year  damage_imputed  \n",
       "0  2017               0  \n",
       "1  2017               0  \n",
       "2  2017               0  \n",
       "3  2017               0  \n",
       "4  2017               0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load dataset\n",
    "file_path = './data/cost_data_with_errors.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "9c223607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Date    Open    High     Low   Close     Volume   Market Cap\n",
       "0  Sep 17, 2017  109.75  110.94  102.81  106.84  5,350,380  737,226,000\n",
       "1  Sep 16, 2017  111.11  116.01  105.02  109.85  5,683,580  744,652,000\n",
       "2  Sep 15, 2017   97.42  113.75   89.36  111.22  8,539,660  652,107,000\n",
       "3  Sep 14, 2017  115.97  117.38   96.71   96.71  6,367,800  775,543,000\n",
       "4  Sep 13, 2017  123.14  123.70  112.60  115.97  6,315,510  822,282,000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/bitconnect_price.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "e3cbb1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       "   Feature  Importance\n",
       " 0    Open         0.0\n",
       " 1    High         0.0\n",
       " 2     Low         0.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Random Forest Classifier with default parameters\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Get feature importances (Gini importance)\n",
    "importances = clf.feature_importances_\n",
    "feature_names = X.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "accuracy, importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "e05c2180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Date    Open    High     Low   Close     Volume   Market Cap\n",
       "0  Sep 17, 2017  109.75  110.94  102.81  106.84  5,350,380  737,226,000\n",
       "1  Sep 16, 2017  111.11  116.01  105.02  109.85  5,683,580  744,652,000\n",
       "2  Sep 15, 2017   97.42  113.75   89.36  111.22  8,539,660  652,107,000\n",
       "3  Sep 14, 2017  115.97  117.38   96.71   96.71  6,367,800  775,543,000\n",
       "4  Sep 13, 2017  123.14  123.70  112.60  115.97  6,315,510  822,282,000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load the data\n",
    "file_path = './data/bitconnect_price.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "a97305fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(37.296604953216374), np.float64(38.48679881428571))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'Date' column to datetime format\n",
    "# The date format looks like: 'Sep 17, 2017'\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%b %d, %Y')\n",
    "\n",
    "# Extract weekday name\n",
    "# Monday=0, ..., Sunday=6\n",
    "# Weekday (Monday to Friday) vs Weekend (Saturday and Sunday)\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "\n",
    "# Mark as weekday or weekend\n",
    "# weekday 0-4, weekend 5-6\n",
    "s = data['DayOfWeek'].apply(lambda x: 'Weekday' if x < 5 else 'Weekend')\n",
    "data['DayType'] = s\n",
    "\n",
    "# Convert 'Open' column to numeric, forcing errors to NaN and drop those rows\n",
    "data['Open'] = pd.to_numeric(data['Open'], errors='coerce')\n",
    "data = data.dropna(subset=['Open'])\n",
    "\n",
    "# Calculate mean opening prices\n",
    "weekday_mean = data.loc[data['DayType'] == 'Weekday', 'Open'].mean()\n",
    "weekend_mean = data.loc[data['DayType'] == 'Weekend', 'Open'].mean()\n",
    "\n",
    "weekday_mean, weekend_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "86b6cb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8462824189035127)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the opening price values for weekdays and weekends\n",
    "o_weekday = data.loc[data['DayType'] == 'Weekday', 'Open']\n",
    "o_weekend = data.loc[data['DayType'] == 'Weekend', 'Open']\n",
    "\n",
    "# Conduct two-sample t-test\n",
    "# Null hypothesis: The means of weekday and weekend opening prices are equal\n",
    "# Alternative hypothesis: The means are different\n",
    "t_stat, p_value = ttest_ind(o_weekday, o_weekend, equal_var=False)  # Welch's t-test\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "fd4b98a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Titanic data and display first few rows to understand the structure\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/titanic_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "cc9e69fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass\n",
       "1    38.23\n",
       "2    29.88\n",
       "3    25.14\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average age of passengers in each Pclass, ignoring rows with missing age\n",
    "average_ages = df[df['Age'].notna()].groupby('Pclass')['Age'].mean().round(2)\n",
    "average_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d4d0cccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the Titanic dataset\n",
    "file_path = './data/titanic_train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "fa7dac37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/titanic_train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "424b2d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Trips over the past 24-hours (midnight to 11:59pm)',\n",
       "       'Cumulative trips (since launch):',\n",
       "       'Miles traveled today (midnight to 11:59 pm)',\n",
       "       'Miles traveled to date:', 'Total Annual Memberships Sold',\n",
       "       'Annual Member Sign-Ups (midnight to 11:59 pm)',\n",
       "       '24-Hour Passes Purchased (midnight to 11:59 pm)',\n",
       "       '7-Day Passes Purchased (midnight to 11:59 pm)'],\n",
       "      dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "        Date  Trips over the past 24-hours (midnight to 11:59pm)  \\\n",
       "0  10/1/2014                                              31197    \n",
       "1  10/2/2014                                              38286    \n",
       "2  10/3/2014                                              38956    \n",
       "3  10/4/2014                                              15088    \n",
       "4  10/5/2014                                              26247    \n",
       "\n",
       "   Cumulative trips (since launch):  \\\n",
       "0                          13296973   \n",
       "1                          13335259   \n",
       "2                          13374215   \n",
       "3                          13389303   \n",
       "4                          13415550   \n",
       "\n",
       "   Miles traveled today (midnight to 11:59 pm)  Miles traveled to date:  \\\n",
       "0                                        44612                 23121175   \n",
       "1                                        60639                 23181814   \n",
       "2                                        65739                 23247553   \n",
       "3                                        24254                 23271807   \n",
       "4                                        48930                 23320737   \n",
       "\n",
       "   Total Annual Memberships Sold  \\\n",
       "0                         124846   \n",
       "1                         124959   \n",
       "2                         125024   \n",
       "3                         125058   \n",
       "4                         125109   \n",
       "\n",
       "  Annual Member Sign-Ups (midnight to 11:59 pm)  \\\n",
       "0                                           112   \n",
       "1                                           113   \n",
       "2                                            65   \n",
       "3                                            34   \n",
       "4                                            51   \n",
       "\n",
       "  24-Hour Passes Purchased (midnight to 11:59 pm)  \\\n",
       "0                                             330   \n",
       "1                                             602   \n",
       "2                                            1276   \n",
       "3                                             617   \n",
       "4                                            1470   \n",
       "\n",
       "  7-Day Passes Purchased (midnight to 11:59 pm)  \n",
       "0                                            48  \n",
       "1                                            86  \n",
       "2                                           107  \n",
       "3                                            26  \n",
       "4                                            90  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/2014_q4.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the columns and first few rows to identify the target column\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "0b5c9011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9543095491220899),\n",
       " np.float64(0.0027291466836339875),\n",
       " np.float64(0.1520317726967334),\n",
       " np.float64(-1.1336054960924962))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the relevant column for analysis\n",
    "trips = df['Trips over the past 24-hours (midnight to 11:59pm)'].astype(int)\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "shapiro_test_stat, shapiro_p_value = shapiro(trips)\n",
    "\n",
    "# Calculate skewness and kurtosis\n",
    "trip_skewness = skew(trips)\n",
    "trip_kurtosis = kurtosis(trips)\n",
    "\n",
    "(shapiro_test_stat, shapiro_p_value, trip_skewness, trip_kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "7d213ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   time  sun  temperature  chill  humidity wind direction  \\\n",
       "0  2017-01-01T00:00:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "1  2017-01-01T00:10:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "2  2017-01-01T00:20:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "3  2017-01-01T00:30:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "4  2017-01-01T00:40:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "\n",
       "   wind speed  visibility  air pressure  \n",
       "0         3.0       197.0        1026.0  \n",
       "1         3.0       195.0        1025.8  \n",
       "2         3.0       271.0        1025.6  \n",
       "3         3.0       316.0        1025.4  \n",
       "4         3.0       334.0        1025.3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the weather dataset\n",
    "weather_df = pd.read_csv('./data/weather_train.csv')\n",
    "\n",
    "# Display first few rows to understand the structure\n",
    "display(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "1fd31dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate z-scores for the wind speed column\n",
    "weather_df['wind_speed_zscore'] = zscore(weather_df['wind speed'])\n",
    "\n",
    "# Identify outliers with |z-score| > 3\n",
    "outliers = weather_df[(weather_df['wind_speed_zscore'] > 3) | (weather_df['wind_speed_zscore'] < -3)]\n",
    "outlier_count = outliers.shape[0]\n",
    "\n",
    "# Create a new dataframe without outliers\n",
    "weather_df_no_outliers = weather_df[(weather_df['wind_speed_zscore'] <= 3) & (weather_df['wind_speed_zscore'] >= -3)].copy()\n",
    "\n",
    "# Remove the z-score column from the new dataframe\n",
    "weather_df_no_outliers.drop(columns=['wind_speed_zscore'], inplace=True)\n",
    "\n",
    "outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "d3c05bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   time  sun  temperature  chill  humidity wind direction  \\\n",
       "0  2017-01-01T00:00:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "1  2017-01-01T00:10:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "2  2017-01-01T00:20:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "3  2017-01-01T00:30:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "4  2017-01-01T00:40:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "\n",
       "   wind speed  visibility  air pressure  \n",
       "0         3.0       197.0        1026.0  \n",
       "1         3.0       195.0        1025.8  \n",
       "2         3.0       271.0        1025.6  \n",
       "3         3.0       316.0        1025.4  \n",
       "4         3.0       334.0        1025.3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "weather_df = pd.read_csv('./data/weather_train.csv')\n",
    "\n",
    "# Display the first few rows and columns to understand the data\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "797a331f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Pclass  Sex  Age  Fare  Embarked  Title  IsAlone  Age*Pclass\n",
       "0       3    0    2     0         2      1        1           6\n",
       "1       3    1    2     0         0      3        0           6\n",
       "2       2    0    3     1         2      1        1           6\n",
       "3       3    0    1     1         0      1        1           3\n",
       "4       3    1    1     1         0      3        0           3\n",
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 418 entries, 0 to 417\n",
       "Data columns (total 8 columns):\n",
       " #   Column      Non-Null Count  Dtype\n",
       "---  ------      --------------  -----\n",
       " 0   Pclass      418 non-null    int64\n",
       " 1   Sex         418 non-null    int64\n",
       " 2   Age         418 non-null    int64\n",
       " 3   Fare        418 non-null    int64\n",
       " 4   Embarked    418 non-null    int64\n",
       " 5   Title       418 non-null    int64\n",
       " 6   IsAlone     418 non-null    int64\n",
       " 7   Age*Pclass  418 non-null    int64\n",
       "dtypes: int64(8)\n",
       "memory usage: 26.3 KB\n",
       "None\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/test_x.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and columns info to understand its structure\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "bba5f5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    GAME_ID  EVENTNUM  EVENTMSGTYPE  EVENTMSGACTIONTYPE  PERIOD WCTIMESTRING  \\\n",
       "0  20200722         0            12                   0       1      7:13 PM   \n",
       "1  20200722         1            10                   0       1      7:14 PM   \n",
       "2  20200722         2             5                   1       1      7:15 PM   \n",
       "3  20200722         3             5                   2       1      7:16 PM   \n",
       "4  20200722         4             2                   5       1      7:16 PM   \n",
       "\n",
       "  PCTIMESTRING                                  HOMEDESCRIPTION  \\\n",
       "0        12:00                                              NaN   \n",
       "1        12:00  Jump Ball Ilgauskas vs. Garnett: Tip to Palacio   \n",
       "2        11:46              Ilgauskas Bad Pass Turnover (P1.T1)   \n",
       "3        11:43                                              NaN   \n",
       "4        11:22                            MISS Ilgauskas  Layup   \n",
       "\n",
       "   NEUTRALDESCRIPTION                 VISITORDESCRIPTION SCORE SCOREMARGIN  \n",
       "0                 NaN                                NaN   NaN         NaN  \n",
       "1                 NaN                                NaN   NaN         NaN  \n",
       "2                 NaN               Peeler STEAL (1 STL)   NaN         NaN  \n",
       "3                 NaN  Peeler Lost Ball Turnover (P1.T1)   NaN         NaN  \n",
       "4                 NaN           Nesterovic BLOCK (1 BLK)   NaN         NaN  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/0020200722.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e2e32853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 TRUE_TIME  TIME        USFLUX  MEANGAM  MEANGBT  MEANGBZ  \\\n",
       "0  2014.03.23_20:24:00_TAI  11.6  3.246502e+21   21.786   93.013   92.809   \n",
       "1  2014.03.23_20:36:00_TAI  11.8  3.908340e+21   21.740   89.953   89.779   \n",
       "2  2014.03.23_20:48:00_TAI  12.0  4.041844e+21   21.797   89.552   89.566   \n",
       "3  2014.03.23_21:00:00_TAI  12.2  4.096817e+21   21.654   89.355   89.499   \n",
       "4  2014.03.23_21:12:00_TAI  12.4  4.197154e+21   21.732   87.089   87.454   \n",
       "\n",
       "   MEANGBH   MEANJZD       TOTUSJZ   MEANJZH  TOTUSJH  ABSNJZH       SAVNCPP  \\\n",
       "0   31.210  0.087461  3.141588e+12  0.002863  143.341   14.092  2.248874e+11   \n",
       "1   31.535  0.151386  3.745627e+12  0.003097  173.704   18.216  4.651086e+11   \n",
       "2   30.425  0.139126  3.790352e+12  0.002931  174.009   18.001  4.464203e+11   \n",
       "3   30.440  0.234519  3.604093e+12  0.003071  164.412   19.141  7.636783e+11   \n",
       "4   29.875  0.266657  3.622492e+12  0.003420  163.141   22.204  9.045709e+11   \n",
       "\n",
       "    MEANPOT        TOTPOT MEANSHR  SHRGT45  R_VALUE   AREA_ACR  \n",
       "0  1185.247  7.747525e+21  18.695    0.061      0.0  69.264130  \n",
       "1  1155.593  9.025444e+21  18.172    0.000      0.0  83.896141  \n",
       "2  1132.300  9.235995e+21  18.322    0.016      0.0  86.314224  \n",
       "3  1100.275  9.107749e+21  18.134    0.048      0.0  87.762978  \n",
       "4  1032.512  8.903345e+21  17.850    0.046      0.0  84.621979  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/3901.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "bd5a656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 TRUE_TIME  TIME        USFLUX  MEANGAM  MEANGBT  MEANGBZ  \\\n",
       "0  2014.03.23_20:24:00_TAI  11.6  3.246502e+21   21.786   93.013   92.809   \n",
       "1  2014.03.23_20:36:00_TAI  11.8  3.908340e+21   21.740   89.953   89.779   \n",
       "2  2014.03.23_20:48:00_TAI  12.0  4.041844e+21   21.797   89.552   89.566   \n",
       "3  2014.03.23_21:00:00_TAI  12.2  4.096817e+21   21.654   89.355   89.499   \n",
       "4  2014.03.23_21:12:00_TAI  12.4  4.197154e+21   21.732   87.089   87.454   \n",
       "\n",
       "   MEANGBH   MEANJZD       TOTUSJZ   MEANJZH  TOTUSJH  ABSNJZH       SAVNCPP  \\\n",
       "0   31.210  0.087461  3.141588e+12  0.002863  143.341   14.092  2.248874e+11   \n",
       "1   31.535  0.151386  3.745627e+12  0.003097  173.704   18.216  4.651086e+11   \n",
       "2   30.425  0.139126  3.790352e+12  0.002931  174.009   18.001  4.464203e+11   \n",
       "3   30.440  0.234519  3.604093e+12  0.003071  164.412   19.141  7.636783e+11   \n",
       "4   29.875  0.266657  3.622492e+12  0.003420  163.141   22.204  9.045709e+11   \n",
       "\n",
       "    MEANPOT        TOTPOT MEANSHR  SHRGT45  R_VALUE   AREA_ACR  \n",
       "0  1185.247  7.747525e+21  18.695    0.061      0.0  69.264130  \n",
       "1  1155.593  9.025444e+21  18.172    0.000      0.0  83.896141  \n",
       "2  1132.300  9.235995e+21  18.322    0.016      0.0  86.314224  \n",
       "3  1100.275  9.107749e+21  18.134    0.048      0.0  87.762978  \n",
       "4  1032.512  8.903345e+21  17.850    0.046      0.0  84.621979  \n",
       "Index(['TRUE_TIME', 'TIME', 'USFLUX', 'MEANGAM', 'MEANGBT', 'MEANGBZ',\n",
       "       'MEANGBH', 'MEANJZD', 'TOTUSJZ', 'MEANJZH', 'TOTUSJH', 'ABSNJZH',\n",
       "       'SAVNCPP', 'MEANPOT', 'TOTPOT', 'MEANSHR', 'SHRGT45', 'R_VALUE',\n",
       "       'AREA_ACR'],\n",
       "      dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, zscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/3901.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display first few rows and columns to confirm data load and availability of relevant columns\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "240720a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   salary_in_thousands_of_dollars  batting_average  on_base_percentage  \\\n",
       "0                            3300            0.272               0.302   \n",
       "1                            2600            0.269               0.335   \n",
       "2                            2500            0.249               0.337   \n",
       "3                            2475            0.260               0.292   \n",
       "4                            2313            0.273               0.346   \n",
       "\n",
       "   number_of_runs  number_of_hits  number_of_doubles  number_of_triples  \\\n",
       "0              69             153                 21                  4   \n",
       "1              58             111                 17                  2   \n",
       "2              54             115                 15                  1   \n",
       "3              59             128                 22                  7   \n",
       "4              87             169                 28                  5   \n",
       "\n",
       "   number_of_home_runs  number_of_runs_batted_in  number_of_walks  \\\n",
       "0                 31.0                       104               22   \n",
       "1                 18.0                        66               39   \n",
       "2                 17.0                        73               63   \n",
       "3                 12.0                        50               23   \n",
       "4                  8.0                        58               70   \n",
       "\n",
       "   number_of_strike_outs  number_of_stolen_bases  number_of_errors  \\\n",
       "0                   80.0                       4               3.0   \n",
       "1                   69.0                       0               3.0   \n",
       "2                  116.0                       6               5.0   \n",
       "3                   64.0                      21              21.0   \n",
       "4                   53.0                       3               8.0   \n",
       "\n",
       "   indicator_of_free_agency_eligibility  indicator_of_free_agent_in_1991_1992  \\\n",
       "0                                   1.0                                     0   \n",
       "1                                   1.0                                     1   \n",
       "2                                   1.0                                     0   \n",
       "3                                   0.0                                     0   \n",
       "4                                   0.0                                     0   \n",
       "\n",
       "   indicator_of_arbitration_eligibility  indicator_of_arbitration_in_1991_1992  \n",
       "0                                   0.0                                    0.0  \n",
       "1                                   0.0                                    0.0  \n",
       "2                                   0.0                                    0.0  \n",
       "3                                   1.0                                    0.0  \n",
       "4                                   1.0                                    0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and examine the first few rows to understand its structure\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "baseball_data = pd.read_csv('./data/baseball_data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "baseball_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6da84bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the relevant variables: number_of_doubles and salary_in_thousands_of_dollars\n",
    "x = baseball_data['number_of_doubles']\n",
    "y = baseball_data['salary_in_thousands_of_dollars']\n",
    "\n",
    "# Check for missing values in these columns\n",
    "missing_values = x.isnull().sum() + y.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "cfa8edeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.5774226751694127), np.float64(2.3477411044561924e-31))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since there are no missing values, calculate the Pearson correlation coefficient and p-value\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlation_coefficient, p_value = pearsonr(x, y)\n",
    "correlation_coefficient, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a452235c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 566 entries, 0 to 565\n",
       "Data columns (total 6 columns):\n",
       " #   Column            Non-Null Count  Dtype  \n",
       "---  ------            --------------  -----  \n",
       " 0   site              566 non-null    object \n",
       " 1   abs_diffsel       566 non-null    float64\n",
       " 2   positive_diffsel  566 non-null    float64\n",
       " 3   negative_diffsel  566 non-null    float64\n",
       " 4   max_diffsel       566 non-null    float64\n",
       " 5   min_diffsel       566 non-null    float64\n",
       "dtypes: float64(5), object(1)\n",
       "memory usage: 26.7+ KB\n",
       "None\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "       site  abs_diffsel  positive_diffsel  negative_diffsel  max_diffsel  \\\n",
       "0  (HA2)121     9.026365          4.147102         -4.879263     1.578739   \n",
       "1       326     9.002765          3.615601         -5.387164     0.716922   \n",
       "2       280     8.418638          5.146938         -3.271700     0.971071   \n",
       "3         9     8.185717          4.420441         -3.765276     1.000554   \n",
       "4       210     8.058663          5.824386         -2.234277     1.378964   \n",
       "\n",
       "   min_diffsel  \n",
       "0    -1.004167  \n",
       "1    -1.218422  \n",
       "2    -1.018267  \n",
       "3    -0.847152  \n",
       "4    -1.240547  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = './data/ferret-Pitt-2-preinf-lib2-100_sitediffsel.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Show basic info and the first few rows\n",
    "print(data.info())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "225e7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 891 entries, 0 to 890\n",
       "Data columns (total 12 columns):\n",
       " #   Column       Non-Null Count  Dtype  \n",
       "---  ------       --------------  -----  \n",
       " 0   PassengerId  891 non-null    int64  \n",
       " 1   Survived     891 non-null    int64  \n",
       " 2   Pclass       891 non-null    int64  \n",
       " 3   Name         891 non-null    object \n",
       " 4   Sex          891 non-null    object \n",
       " 5   Age          714 non-null    float64\n",
       " 6   SibSp        891 non-null    int64  \n",
       " 7   Parch        891 non-null    int64  \n",
       " 8   Ticket       891 non-null    object \n",
       " 9   Fare         891 non-null    float64\n",
       " 10  Cabin        204 non-null    object \n",
       " 11  Embarked     889 non-null    object \n",
       "dtypes: float64(2), int64(5), object(5)\n",
       "memory usage: 83.7+ KB\n",
       "None\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/titanic.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and info for initial examination\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "9d472d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Display the first few rows and columns info to understand data\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "0b8e2aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    wage     lwage  belavg  abvavg  exper  looks  union  goodhlth  black  \\\n",
       "0   5.73  1.745715       0       1     30      4      0         1      0   \n",
       "1   4.28  1.453953       0       0     28      3      0         1      0   \n",
       "2   7.96  2.074429       0       1     35      4      0         1      0   \n",
       "3  11.57  2.448416       0       0     38      3      0         1      0   \n",
       "4  11.42  2.435366       0       0     27      3      0         1      0   \n",
       "\n",
       "   female  married  south  bigcity  smllcity  service  expersq  educ  \\\n",
       "0       1        1      0        0         1        1      900    14   \n",
       "1       1        1      1        0         1        0      784    12   \n",
       "2       1        0      0        0         1        0     1225    10   \n",
       "3       0        1      0        1         0        1     1444    16   \n",
       "4       0        1      0        0         1        0      729    16   \n",
       "\n",
       "   Nservice  Slooks  \n",
       "0         0       4  \n",
       "1         1       0  \n",
       "2         1       0  \n",
       "3         0       3  \n",
       "4         1       0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/beauty and the labor market.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6ea3619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520\n",
       "Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'], dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/insurance.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display first few rows and columns to understand data\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "705ccb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 1338 entries, 0 to 1337\n",
       "Data columns (total 7 columns):\n",
       " #   Column    Non-Null Count  Dtype  \n",
       "---  ------    --------------  -----  \n",
       " 0   age       1338 non-null   int64  \n",
       " 1   sex       1338 non-null   object \n",
       " 2   bmi       1338 non-null   float64\n",
       " 3   children  1338 non-null   int64  \n",
       " 4   smoker    1338 non-null   object \n",
       " 5   region    1338 non-null   object \n",
       " 6   charges   1338 non-null   float64\n",
       "dtypes: float64(2), int64(2), object(3)\n",
       "memory usage: 73.3+ KB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "    age     sex     bmi  children smoker     region      charges\n",
       " 0   19  female  27.900         0    yes  southwest  16884.92400\n",
       " 1   18    male  33.770         1     no  southeast   1725.55230\n",
       " 2   28    male  33.000         3     no  southeast   4449.46200\n",
       " 3   33    male  22.705         0     no  northwest  21984.47061\n",
       " 4   32    male  28.880         0     no  northwest   3866.85520)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/insurance.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display initial info and head for understanding the data structure\n",
    "(df.info(), df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e06acad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/insurance.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "2c4d0198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the insurance dataset\n",
    "file_path = './data/insurance.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "24d72645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "charges     0\n",
       "children    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in charges and children columns\n",
    "missing_values = df[['charges', 'children']].isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8fd915b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.07)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows with missing values in charges and children columns (not necessary here as no missing values)\n",
    "df_filtered = df.dropna(subset=['charges', 'children'])\n",
    "\n",
    "# Calculate the Pearson correlation coefficient between charges and children\n",
    "correlation = df_filtered['charges'].corr(df_filtered['children'])\n",
    "correlation_rounded = round(correlation, 2)\n",
    "correlation_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "933bbe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Symbol Series         Date  Prev Close  Open Price  High Price  \\\n",
       "0  GODREJIND     EQ  15-May-2017      564.60      581.00      584.00   \n",
       "1  GODREJIND     EQ  16-May-2017      578.55      581.45      589.00   \n",
       "2  GODREJIND     EQ  17-May-2017      584.80      583.00      594.00   \n",
       "3  GODREJIND     EQ  18-May-2017      588.60      582.00      588.85   \n",
       "4  GODREJIND     EQ  19-May-2017      574.60      581.00      585.80   \n",
       "\n",
       "   Low Price  Last Price  Close Price  Average Price  Total Traded Quantity  \\\n",
       "0     568.50      578.90       578.55         578.09                 797171   \n",
       "1     572.25      583.80       584.80         583.60                 500223   \n",
       "2     576.85      584.90       588.60         588.74                 504155   \n",
       "3     571.20      572.25       574.60         580.90                 223583   \n",
       "4     567.55      579.85       578.00         577.31                 245436   \n",
       "\n",
       "       Turnover  No. of Trades  Deliverable Qty  % Dly Qt to Traded Qty  \n",
       "0  4.608362e+08          21649           360927                   45.28  \n",
       "1  2.919302e+08          17204           210364                   42.05  \n",
       "2  2.968149e+08           8567           261667                   51.90  \n",
       "3  1.298786e+08           7144            99785                   44.63  \n",
       "4  1.416925e+08           4969            68041                   27.72  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset to examine its structure and column names\n",
    "df = pd.read_csv('data/GODREJIND.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d95966a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Symbol Series         Date  Prev Close  Open Price  High Price  \\\n",
       "0  GODREJIND     EQ  15-May-2017      564.60      581.00      584.00   \n",
       "1  GODREJIND     EQ  16-May-2017      578.55      581.45      589.00   \n",
       "2  GODREJIND     EQ  17-May-2017      584.80      583.00      594.00   \n",
       "3  GODREJIND     EQ  18-May-2017      588.60      582.00      588.85   \n",
       "4  GODREJIND     EQ  19-May-2017      574.60      581.00      585.80   \n",
       "\n",
       "   Low Price  Last Price  Close Price  Average Price  Total Traded Quantity  \\\n",
       "0     568.50      578.90       578.55         578.09                 797171   \n",
       "1     572.25      583.80       584.80         583.60                 500223   \n",
       "2     576.85      584.90       588.60         588.74                 504155   \n",
       "3     571.20      572.25       574.60         580.90                 223583   \n",
       "4     567.55      579.85       578.00         577.31                 245436   \n",
       "\n",
       "       Turnover  No. of Trades  Deliverable Qty  % Dly Qt to Traded Qty  \n",
       "0  4.608362e+08          21649           360927                   45.28  \n",
       "1  2.919302e+08          17204           210364                   42.05  \n",
       "2  2.968149e+08           8567           261667                   51.90  \n",
       "3  1.298786e+08           7144            99785                   44.63  \n",
       "4  1.416925e+08           4969            68041                   27.72  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/GODREJIND.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9ea873c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    0\n",
       " dtype: int64,\n",
       " 0    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the relevant columns\n",
    "cases = df['No. of cases']\n",
    "deaths = df['No. of deaths']\n",
    "\n",
    "# Clean the data: remove brackets and convert to integers\n",
    "cases = cases.str.extract('^(\\d+)').astype('Int64')\n",
    "deaths = deaths.str.extract('^(\\d+)').astype('Int64')\n",
    "\n",
    "# Check for null values\n",
    "null_cases = cases.isnull().sum()\n",
    "null_deaths = deaths.isnull().sum()\n",
    "null_cases, null_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5a2fe2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.252)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new feature 'experience_score' multiplying 'exper' and 'looks' columns\n",
    "\n",
    "data['experience_score'] = data['exper'] * data['looks']\n",
    "\n",
    "# Calculate the Pearson correlation coefficient between 'experience_score' and 'wage'\n",
    "from scipy.stats import pearsonr\n",
    "correlation, _ = pearsonr(data['experience_score'], data['wage'])\n",
    "\n",
    "# Round the correlation to three decimal places\n",
    "correlation = round(correlation, 3)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "5a784f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9916, 0.0, 'linear')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the results\n",
    "correlation_coefficient = 0.9916\n",
    "p_value = 0.0000\n",
    "\n",
    "# Determine the relationship type based on the p-value and correlation coefficient\n",
    "if p_value < 0.05:\n",
    "    if abs(correlation_coefficient) >= 0.5:\n",
    "        relationship_type = 'linear'\n",
    "    else:\n",
    "        relationship_type = 'nonlinear'\n",
    "else:\n",
    "    relationship_type = 'none'\n",
    "\n",
    "correlation_coefficient, p_value, relationship_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9470e9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9915815807847342), np.float64(0.0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate the Pearson correlation coefficient between 'High Price' and 'Low Price'\n",
    "correlation_coefficient, p_value = pearsonr(data['High Price'], data['Low Price'])\n",
    "correlation_coefficient, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e80fe25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "42a93191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Titanic dataset again after importing necessary libraries\n",
    "data_path = './data/titanic_train.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "effa0988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(40.82), np.float64(71.36), np.float64(21.96), np.float64(32.41))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering data for male passengers who survived and who did not survive\n",
    "male_survived = data[(data['Sex'] == 'male') & (data['Survived'] == 1)]['Fare']\n",
    "male_not_survived = data[(data['Sex'] == 'male') & (data['Survived'] == 0)]['Fare']\n",
    "\n",
    "# Calculating mean and standard deviation\n",
    "mean_survived = round(male_survived.mean(), 2)\n",
    "std_survived = round(male_survived.std(), 2)\n",
    "mean_not_survived = round(male_not_survived.mean(), 2)\n",
    "std_not_survived = round(male_not_survived.std(), 2)\n",
    "\n",
    "mean_survived, std_survived, mean_not_survived, std_not_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "797faeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_69840/2442936848.py:16: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
       "  mean_fare_group = data.groupby('AgeGroup')['Fare'].mean().round(2)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(31.09), np.float64(31.98), np.float64(35.17), np.float64(43.47))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create AgeGroup column based on the Age\n",
    "conditions = [\n",
    "    (data['Age'] >= 0) & (data['Age'] <= 12),\n",
    "    (data['Age'] >= 13) & (data['Age'] <= 19),\n",
    "    (data['Age'] >= 20) & (data['Age'] <= 59),\n",
    "    (data['Age'] >= 60)\n",
    "]\n",
    "\n",
    "age_groups = ['Child', 'Teenager', 'Adult', 'Elderly']\n",
    "\n",
    "# Assign AgeGroup based on conditions\n",
    "data['AgeGroup'] = pd.Series(pd.cut(data['Age'], bins=[-1, 12, 19, 59, 100],\n",
    "labels=age_groups))\n",
    "\n",
    "# Calculate mean fare for each age group\n",
    "mean_fare_group = data.groupby('AgeGroup')['Fare'].mean().round(2)\n",
    "\n",
    "mean_fare_child = mean_fare_group['Child']\n",
    "mean_fare_teenager = mean_fare_group['Teenager']\n",
    "mean_fare_adult = mean_fare_group['Adult']\n",
    "mean_fare_elderly = mean_fare_group['Elderly']\n",
    "\n",
    "mean_fare_child, mean_fare_teenager, mean_fare_adult, mean_fare_elderly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2f3ed97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgeGroup\n",
       "Adult       35.17\n",
       "Child       31.09\n",
       "Elderly     43.47\n",
       "Teenager    31.98\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating AgeGroup column and calculating mean fares by age group\n",
    "\n",
    "def age_group(age):\n",
    "    if age <= 12:\n",
    "        return 'Child'\n",
    "    elif age <= 19:\n",
    "        return 'Teenager'\n",
    "    elif age <= 59:\n",
    "        return 'Adult'\n",
    "    else:\n",
    "        return 'Elderly'\n",
    "\n",
    "# Apply the age_group function to create a new column\n",
    "\n",
    "df['AgeGroup'] = df['Age'].apply(age_group)\n",
    "\n",
    "# Calculate the mean fare for each age group\n",
    "mean_fares = df.groupby('AgeGroup')['Fare'].mean().round(2)\n",
    "\n",
    "mean_fares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bcf5ba6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.008049376708637247), 'significant')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing independent sample t-test\n",
    "t_stat, p_value = ttest_ind(male_survived, male_not_survived, equal_var=False)\n",
    "significance = 'significant' if p_value < 0.05 else 'not significant'\n",
    "p_value, significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7ed3cbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   year       pop  lifeexp   gdppercap      country continent\n",
       "0  1952   8425333   28.801  779.445314  afghanistan      asia\n",
       "1  1957   9240934   30.332  820.853030  afghanistan      asia\n",
       "2  1962  10267083   31.997  853.100710  afghanistan      asia\n",
       "3  1967  11537966   34.020  836.197138  afghanistan      asia\n",
       "4  1972  13079460   36.088  739.981106  afghanistan      asia"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/gapminder_cleaned.csv'\n",
    "gapminder_data = pd.read_csv(data_path)\n",
    "gapminder_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "278ac9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.03), np.float64(0.2909))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Pearson correlation coefficient between 'pop' and 'gdppercap'\n",
    "population = gapminder_data['pop']\n",
    "gdppercap = gapminder_data['gdppercap']\n",
    "r_value, p_value = pearsonr(population, gdppercap)\n",
    "\n",
    "# Round results to two and four decimal places respectively\n",
    "r_value_rounded = round(r_value, 2)\n",
    "p_value_rounded = round(p_value, 4)\n",
    "\n",
    "r_value_rounded, p_value_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aacc57e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   year       pop  lifeexp   gdppercap      country continent\n",
       " 0  1952   8425333   28.801  779.445314  afghanistan      asia\n",
       " 1  1957   9240934   30.332  820.853030  afghanistan      asia\n",
       " 2  1962  10267083   31.997  853.100710  afghanistan      asia\n",
       " 3  1967  11537966   34.020  836.197138  afghanistan      asia\n",
       " 4  1972  13079460   36.088  739.981106  afghanistan      asia,\n",
       " year         0\n",
       " pop          0\n",
       " lifeexp      0\n",
       " gdppercap    0\n",
       " country      0\n",
       " continent    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/gapminder_cleaned.csv'\n",
    "gapminder_df = pd.read_csv(data_path)\n",
    "\n",
    "# Inspect the first few rows of the dataset and check for missing values\n",
    "gapminder_df.head(), gapminder_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8bf6b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries and reattempt calculation\n",
    "import pandas as pd\n",
    "\n",
    "data_path = './data/test_ave.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calculate the mean fare using pandas.mean method\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383ed210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "# Calculate mean fare using pandas.mean(), rounded to 2 decimal places\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8891465a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load necessary libraries and the dataset\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/insurance.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253c3c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, np.float64(42103.95), np.float64(40974.16))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify outliers using Z-score method with 1.5xIQR rule\n",
    "charges = data['charges']\n",
    "# Calculate the first and third quartile\n",
    "Q1 = charges.quantile(0.25)\n",
    "Q3 = charges.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers based on bounds\n",
    "outliers = charges[(charges < lower_bound) | (charges > upper_bound)]\n",
    "\n",
    "# Calculate required metrics for outliers\n",
    "total_outliers = len(outliers)\n",
    "mean_charges_outliers = outliers.mean()\n",
    "median_charges_outliers = outliers.median()\n",
    "\n",
    "round(total_outliers, 2), round(mean_charges_outliers, 2), round(median_charges_outliers, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96f3c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@bmi_distribution[normal]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/insurance.csv')\n",
    "\n",
    "# Calculate skewness of BMI\n",
    "bmi_skewness = skew(data['bmi'])\n",
    "\n",
    "# Check if absolute skewness is less than 0.5\n",
    "status = 'normal' if abs(bmi_skewness) < 0.5 else 'not_normal'\n",
    "\n",
    "# Output result\n",
    "f\"@bmi_distribution[{status}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84206ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0  PassengerId  Survived  Pclass  \\\n",
       "0           0            1         0       3   \n",
       "1           1            2         1       1   \n",
       "2           2            3         1       3   \n",
       "3           3            4         1       1   \n",
       "4           4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  AgeBand  \n",
       "0      0         A/5 21171   7.2500   NaN        S        2  \n",
       "1      0          PC 17599  71.2833   C85        C        3  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S        2  \n",
       "3      0            113803  53.1000  C123        S        3  \n",
       "4      0            373450   8.0500   NaN        S        3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f83ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "4d4e3087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Load the Titanic dataset\n",
    "file_path = './data/titanic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Perform the Shapiro-Wilk test on the 'Fare' variable\n",
    "statistic, p_value = stats.shapiro(data['Fare'].dropna())\n",
    "\n",
    "# Check if the p-value is less than 0.05\n",
    "normality_result = p_value >= 0.05\n",
    "normality_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121eb075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas module\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "\n",
    "# Calculate the mean fare column using pandas\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "\n",
    "# Output result\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9564a273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.65"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the required dataset\n",
    "data_path = './data/test_ave.csv'\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calculate the mean fare\n",
    "if 'Fare' in data.columns:\n",
    "    mean_fare = round(statistics.mean(data['Fare']), 2)\n",
    "else:\n",
    "    mean_fare = None\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "309d6fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "\n",
    "# Calculate the mean fare\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36794a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@is_normal[no]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "# Load the data file\n",
    "data = pd.read_csv('./data/GODREJIND.csv')\n",
    "\n",
    "# Check for the existence of the required column\n",
    "required_column = 'Total Traded Quantity'\n",
    "if required_column not in data.columns:\n",
    "    raise ValueError(f\"Column '{required_column}' not found in the dataset\")\n",
    "\n",
    "# Perform Shapiro-Wilk test\n",
    "stat, p = shapiro(data[required_column])\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    result = '@is_normal[yes]'\n",
    "else:\n",
    "    result = '@is_normal[no]'\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba6d9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e7fe66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/test_ave.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calculate the mean of fares\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4412e625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "NameError                                 Traceback (most recent call last)\n",
       "Cell In[1], line 3\n",
       "      1 # Load the dataset\n",
       "      2 data_path = './data/test_ave.csv'\n",
       "----> 3 data = pd.read_csv(data_path)\n",
       "      5 # Calculate the mean fare using pandas.mean method\n",
       "      6 import pandas as pd\n",
       "\n",
       "NameError: name 'pd' is not defined"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = './data/test_ave.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calculate the mean fare using pandas.mean method\n",
    "import pandas as pd\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c12a4383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "\n",
    "# Calculate the mean fare\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e310912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17e4e20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "\n",
    "# Calculate the mean fare using Pandas built-in method\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa6ec171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@is_normal[No]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Load the data file\n",
    "data_path = './data/unemployement_industry.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the \"Mar.2020\" column\n",
    "mar_2020_values = data[\"Mar.2020\"].dropna()\n",
    "\n",
    "# Compute skewness\n",
    "mar_2020_skewness = round(skew(mar_2020_values), 2)\n",
    "\n",
    "# Check if skewness is between -0.5 and 0.5\n",
    "is_normal = \"Yes\" if -0.5 <= mar_2020_skewness <= 0.5 else \"No\"\n",
    "\n",
    "# Output result\n",
    "print(f\"@is_normal[{is_normal}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "529e4e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_27593/1168619446.py:20: SettingWithCopyWarning: \n",
       "A value is trying to be set on a copy of a slice from a DataFrame\n",
       "\n",
       "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
       "  X.fillna(X.mean(), inplace=True)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7762237762237763"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load required libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/test_ave.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Encode 'Sex' and 'Embarked' using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Extract relevant features and target variable\n",
    "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "X = data_encoded[features]\n",
    "y = data_encoded['Survived']\n",
    "\n",
    "# Handle missing values (if present)\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the values on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eda7fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "TypeError                                 Traceback (most recent call last)\n",
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_27718/2228859191.py in ?()\n",
       "     11     standardized_data = (data - np.mean(data)) / np.std(data)\n",
       "     12     test_statistic, p_value = ks_1samp(standardized_data, 'norm')\n",
       "     13     return p_value\n",
       "     14 # Apply the test\n",
       "---> 15 p_value = ks_normality_test(selected_column)\n",
       "     16 # Decision based on the p-value\n",
       "     17 alpha = 0.05\n",
       "     18 decision = \"normally distributed\" if p_value >= alpha else \"not normally distributed\"\n",
       "\n",
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_27718/2228859191.py in ?(data)\n",
       "     10 def ks_normality_test(data):\n",
       "     11     standardized_data = (data - np.mean(data)) / np.std(data)\n",
       "---> 12     test_statistic, p_value = ks_1samp(standardized_data, 'norm')\n",
       "     13     return p_value\n",
       "\n",
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py in ?(***failed resolving arguments***)\n",
       "    575                     res = np.full(n_out, NaN)\n",
       "    576                     res = _add_reduced_axes(res, reduced_axes, keepdims)\n",
       "    577                     return tuple_to_result(*res)\n",
       "    578 \n",
       "--> 579                 res = hypotest_fun_out(*samples, **kwds)\n",
       "    580                 res = result_to_tuple(res, n_out)\n",
       "    581                 res = _add_reduced_axes(res, reduced_axes, keepdims)\n",
       "    582                 return tuple_to_result(*res)\n",
       "\n",
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/scipy/_lib/_util.py in ?(*args, **kwargs)\n",
       "   1002                     message = (f\"{fun.__name__}() got multiple values for \"\n",
       "   1003                                f\"argument now known as `{new_name}`\")\n",
       "   1004                     raise TypeError(message)\n",
       "   1005                 kwargs[new_name] = kwargs.pop(old_name)\n",
       "-> 1006             return fun(*args, **kwargs)\n",
       "\n",
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/scipy/stats/_stats_py.py in ?(x, cdf, args, alternative, method)\n",
       "   7776         raise ValueError(f\"Unexpected value {alternative=}\")\n",
       "   7777 \n",
       "   7778     N = len(x)\n",
       "   7779     x = np.sort(x)\n",
       "-> 7780     cdfvals = cdf(x, *args)\n",
       "   7781     np_one = np.int8(1)\n",
       "   7782 \n",
       "   7783     if alternative == 'greater':\n",
       "\n",
       "TypeError: 'str' object is not callable"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ks_1samp\n",
    "import numpy as np\n",
    "# Load the dataset\n",
    "file_path = './data/imp.score.ldlr.metabolome.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# Extract the relevant column and drop NaN values\n",
    "selected_column = data['row m/z'].dropna()\n",
    "# Perform Kolmogorov-Smirnov test assuming normal distribution\n",
    "def ks_normality_test(data):\n",
    "    standardized_data = (data - np.mean(data)) / np.std(data)\n",
    "    test_statistic, p_value = ks_1samp(standardized_data, 'norm')\n",
    "    return p_value\n",
    "# Apply the test\n",
    "p_value = ks_normality_test(selected_column)\n",
    "# Decision based on the p-value\n",
    "alpha = 0.05\n",
    "decision = \"normally distributed\" if p_value >= alpha else \"not normally distributed\"\n",
    "\n",
    "p_value, decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d7f5e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.5306805770163674e-05), 'not normally distributed')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kstest\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/imp.score.ldlr.metabolome.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the relevant column and drop NaN values\n",
    "selected_column = data['row m/z'].dropna()\n",
    "\n",
    "# Standardize the data\n",
    "standardized_data = (selected_column - np.mean(selected_column)) / np.std(selected_column)\n",
    "\n",
    "# Perform Kolmogorov-Smirnov test\n",
    "ks_statistic, p_value = kstest(standardized_data, 'norm')\n",
    "\n",
    "# Decision based on the p-value\n",
    "alpha = 0.05\n",
    "decision = \"normally distributed\" if p_value >= alpha else \"not normally distributed\"\n",
    "\n",
    "p_value, decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "927cc6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "file_path = './data/insurance.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd2d3e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "TypeError                                 Traceback (most recent call last)\n",
       "Cell In[25], line 13\n",
       "     10 df[['age', 'bmi', 'children', 'charges']] = scaler.fit_transform(df[['age', 'bmi', 'children', 'charges']])\n",
       "     12 # Calculate the mean for each column after preprocessing\n",
       "---> 13 mean_values = df.mean(axis=0).round(4)\n",
       "     14 mean_values\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:11720, in DataFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n",
       "  11712 @doc(make_doc(\"mean\", ndim=2))\n",
       "  11713 def mean(\n",
       "  11714     self,\n",
       "   (...)  11718     **kwargs,\n",
       "  11719 ):\n",
       "> 11720     result = super().mean(axis, skipna, numeric_only, **kwargs)\n",
       "  11721     if isinstance(result, Series):\n",
       "  11722         result = result.__finalize__(self, method=\"mean\")\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/generic.py:12485, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n",
       "  12478 def mean(\n",
       "  12479     self,\n",
       "  12480     axis: Axis | None = 0,\n",
       "   (...)  12483     **kwargs,\n",
       "  12484 ) -> Series | float:\n",
       "> 12485     return self._stat_function(\n",
       "  12486         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n",
       "  12487     )\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/generic.py:12442, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n",
       "  12438 nv.validate_func(name, (), kwargs)\n",
       "  12440 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n",
       "> 12442 return self._reduce(\n",
       "  12443     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n",
       "  12444 )\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:11589, in DataFrame._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n",
       "  11585     df = df.T\n",
       "  11587 # After possibly _get_data and transposing, we are now in the\n",
       "  11588 #  simple case where we can use BlockManager.reduce\n",
       "> 11589 res = df._mgr.reduce(blk_func)\n",
       "  11590 out = df._constructor_from_mgr(res, axes=res.axes).iloc[0]\n",
       "  11591 if out_dtype is not None and out.dtype != \"boolean\":\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/internals/managers.py:1519, in BlockManager.reduce(self, func)\n",
       "   1517 res_blocks: list[Block] = []\n",
       "   1518 for blk in self.blocks:\n",
       "-> 1519     nbs = blk.reduce(func)\n",
       "   1520     res_blocks.extend(nbs)\n",
       "   1522 index = Index([None])  # placeholder\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/internals/blocks.py:406, in Block.reduce(self, func)\n",
       "    400 @final\n",
       "    401 def reduce(self, func) -> list[Block]:\n",
       "    402     # We will apply the function and reshape the result into a single-row\n",
       "    403     #  Block with the same mgr_locs; squeezing will be done at a higher level\n",
       "    404     assert self.ndim == 2\n",
       "--> 406     result = func(self.values)\n",
       "    408     if self.values.ndim == 1:\n",
       "    409         res_values = result\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:11508, in DataFrame._reduce.<locals>.blk_func(values, axis)\n",
       "  11506         return np.array([result])\n",
       "  11507 else:\n",
       "> 11508     return op(values, axis=axis, skipna=skipna, **kwds)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:147, in bottleneck_switch.__call__.<locals>.f(values, axis, skipna, **kwds)\n",
       "    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n",
       "    146 else:\n",
       "--> 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n",
       "    149 return result\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:404, in _datetimelike_compat.<locals>.new_func(values, axis, skipna, mask, **kwargs)\n",
       "    401 if datetimelike and mask is None:\n",
       "    402     mask = isna(values)\n",
       "--> 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n",
       "    406 if datetimelike:\n",
       "    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:720, in nanmean(values, axis, skipna, mask)\n",
       "    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n",
       "    719 the_sum = values.sum(axis, dtype=dtype_sum)\n",
       "--> 720 the_sum = _ensure_numeric(the_sum)\n",
       "    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n",
       "    723     count = cast(np.ndarray, count)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:1686, in _ensure_numeric(x)\n",
       "   1683 inferred = lib.infer_dtype(x)\n",
       "   1684 if inferred in [\"string\", \"mixed\"]:\n",
       "   1685     # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n",
       "-> 1686     raise TypeError(f\"Could not convert {x} to numeric\")\n",
       "   1687 try:\n",
       "   1688     x = x.astype(np.complex128)\n",
       "\n",
       "TypeError: Could not convert ['southwestsoutheastsoutheastnorthwestnorthwestsoutheastsoutheastnorthwestnortheastnorthwestnortheastsoutheastsouthwestsoutheastsoutheastsouthwestnortheastnortheastsouthwestsouthwestnortheastsouthwestsoutheastnortheastnorthwestsoutheastnortheastnorthwestnorthwestsouthwestsouthwestnortheastsouthwestnorthwestsouthwestnorthwestnorthwestsouthwestnortheastsouthwestnortheastsoutheastsoutheastsoutheastnortheastsouthwestnortheastnorthwestsoutheastsoutheastnortheastnorthwestsouthwestsoutheastnorthwestnorthwestnortheastsoutheastsoutheastnorthwestnortheastsoutheastnorthwestnorthwestnorthwestsouthwestsouthwestnorthwestsoutheastsoutheastsoutheastnortheastsouthwestsoutheastsouthwestnorthwestsoutheastsoutheastnortheastnorthwestnortheastnortheastsoutheastnorthwestsouthwestnorthwestnorthwestsouthwestnorthwestnorthwestsoutheastnorthwestnortheastnorthwestsouthwestsoutheastsouthwestsoutheastnortheastsouthwestsouthwestnortheastnortheastsoutheastsouthwestnorthwestsouthwestnorthwestsoutheastsoutheastnorthwestsouthwestsouthwestnorthwestnortheastnortheastsoutheastsoutheastsoutheastnorthwestsouthwestnortheastnorthwestnortheastnorthwestnortheastsouthwestsouthwestnorthwestsouthwestnortheastnortheastsouthwestnorthwestnortheastsoutheastsouthwestnorthwestsoutheastsouthwestnortheastnortheastsoutheastnorthwestnorthwestsoutheastnorthwestsoutheastnorthwestsouthwestnorthwestsoutheastnortheastnortheastnortheastnorthwestsoutheastnortheastsoutheastsoutheastnorthwestsoutheastsouthwestsouthwestnorthwestnortheastsouthwestnorthwestnorthwestnortheastsoutheastsouthwestnortheastsouthwestnorthwestsouthwestnorthwestsouthwestsouthwestnortheastnorthwestsoutheastnortheastnorthwestsoutheastnortheastsoutheastsouthwestsouthwestnorthwestsoutheastsouthwestsoutheastnorthwestsoutheastnorthwestsouthwestsoutheastnorthwestnortheastnorthwestsoutheastnorthwestsoutheastsouthwestnortheastsoutheastnortheastsouthwestnortheastsouthwestnorthwestnorthwestsoutheastsouthwestsouthwestnorthwestsoutheastsoutheastsoutheastsouthwestnortheastsouthwestsouthwestsoutheastsoutheastsoutheastsoutheastnortheastnortheastnorthwestsoutheastsouthwestsouthwestnorthwestsoutheastsoutheastsoutheastnorthwestsoutheastnortheastnortheastsouthwestsouthwestnortheastnorthwestsoutheastsoutheastsouthwestnortheastnortheastsouthwestsoutheastsouthwestnortheastnortheastnorthwestsoutheastnorthwestnorthwestsouthwestsoutheastnortheastnorthwestsoutheastsoutheastsoutheastnortheastsouthwestnortheastsoutheastsouthwestnorthwestnortheastnorthwestnortheastnorthwestsouthwestsoutheastsoutheastnortheastnortheastnortheastnortheastsouthwestsoutheastnortheastnorthwestnortheastsoutheastsouthwestnortheastsoutheastsoutheastsouthwestnortheastsouthwestsoutheastnorthwestnorthwestnortheastnortheastsoutheastsoutheastsouthwestnorthwestsouthwestsoutheastnortheastnorthwestsouthwestsouthwestsoutheastsoutheastsouthwestnortheastnorthwestnortheastnorthwestnortheastnorthwestnortheastsouthwestnortheastsouthwestnortheastsoutheastnorthwestsouthwestsouthwestnortheastnorthwestnorthwestnortheastnortheastsouthwestsoutheastnorthwestnortheastsoutheastsouthwestnorthwestnortheastnortheastsoutheastsoutheastsoutheastnortheastsoutheastnorthwestnorthwestsouthwestsouthwestnortheastsoutheastsouthwestsoutheastnorthwestsoutheastsoutheastnortheastsouthwestsouthwestsouthwestsoutheastnortheastnortheastnorthwestnorthwestnortheastnorthwestnortheastnortheastsouthwestsoutheastnorthwestnortheastsoutheastnorthwestsoutheastnortheastnortheastsoutheastsoutheastnortheastsouthwestsoutheastnorthwestnorthwestnorthwestnortheastnorthwestnortheastnortheastnortheastnorthwestsouthwestsoutheastsouthwestsoutheastsouthwestsoutheastnorthwestnorthwestsouthwestnortheastsoutheastsouthwestsoutheastsoutheastnorthwestnortheastnortheastsouthwestnorthwestsoutheastsoutheastsouthwestsoutheastnorthwestsoutheastsoutheastnortheastnortheastsouthwestsoutheastnortheastnortheastnortheastnorthwestsouthwestnorthwestsouthwestsouthwestnorthwestsoutheastnortheastsouthwestsoutheastnortheastnorthwestsouthwestsoutheastsoutheastnorthwestsouthwestnortheastnorthwestsouthwestsouthwestsouthwestnorthwestsouthwestnorthwestsoutheastsouthwestsoutheastnorthwestsouthwestsoutheastsoutheastsouthwestnortheastnortheastnorthwestsoutheastsouthwestnorthwestnortheastsoutheastsoutheastnortheastsouthwestnortheastsouthwestnorthwestnortheastnorthwestsoutheastsoutheastnorthwestsoutheastsoutheastsouthwestsouthwestnortheastnorthwestsouthwestsoutheastnorthwestsouthwestsoutheastnortheastsouthwestsouthwestnortheastsouthwestsouthwestsoutheastsouthwestsouthwestnortheastsoutheastsoutheastsoutheastnorthwestnorthwestnorthwestnortheastsouthwestnortheastsoutheastnortheastsouthwestsouthwestsouthwestsoutheastnorthwestsouthwestnortheastnortheastsoutheastnortheastsoutheastsoutheastsoutheastnorthwestsouthwestnortheastnortheastsoutheastnortheastsoutheastsoutheastsoutheastnortheastsouthwestsouthwestsoutheastsoutheastsouthwestsoutheastsoutheastsoutheastnorthwestnorthwestnortheastsouthwestnortheastsoutheastsouthwestsoutheastsouthwestnorthwestnortheastsouthwestnortheastsoutheastnorthwestnorthwestnorthwestnortheastsouthwestsoutheastsoutheastnorthwestnorthwestnorthwestsouthwestnorthwestsouthwestsoutheastsoutheastnortheastnortheastnorthwestsoutheastnortheastsouthwestnortheastnortheastnorthwestsoutheastsoutheastsouthwestsoutheastnortheastnorthwestnortheastsoutheastsouthwestnorthwestsoutheastnortheastsoutheastnortheastsoutheastnortheastsouthwestnorthwestsoutheastnorthwestsouthwestsoutheastnorthwestsoutheastnortheastnorthwestnortheastsouthwestsoutheastsouthwestnortheastnortheastsoutheastsoutheastnortheastsouthwestsoutheastsouthwestsouthwestsouthwestsouthwestnortheastnorthwestnorthwestnortheastsoutheastsouthwestnorthwestsouthwestsouthwestsoutheastnortheastsouthwestnortheastnorthwestnortheastnortheastsoutheastsouthwestnorthwestnortheastnorthwestsoutheastnortheastnorthwestnortheastnortheastnortheastsoutheastsoutheastsoutheastsoutheastsoutheastsoutheastsouthwestnorthwestnortheastnortheastsoutheastsoutheastnortheastsoutheastsoutheastsoutheastsouthwestnorthwestnortheastsoutheastsoutheastnortheastsoutheastsoutheastsoutheastnorthwestsoutheastnorthwestsouthwestnorthwestsouthwestsouthwestsouthwestnorthwestsouthwestnortheastnortheastsoutheastsouthwestsoutheastnortheastsouthwestnorthwestnorthwestsouthwestnorthwestnortheastsoutheastnorthwestsoutheastsoutheastnortheastsoutheastnorthwestnorthwestsouthwestsoutheastnorthwestnortheastnortheastsoutheastsoutheastnorthwestnortheastsouthwestsouthwestnorthwestnorthwestnorthwestnorthwestnortheastsouthwestsouthwestsouthwestnortheastsoutheastnorthwestnortheastnortheastsoutheastsoutheastsouthwestsouthwestnortheastsouthwestnorthwestsoutheastsouthwestnortheastsouthwestnortheastsoutheastnortheastsoutheastnorthwestnorthwestsouthwestnorthwestsouthwestnorthwestsoutheastnorthwestnorthwestsoutheastnortheastnortheastnortheastsoutheastnortheastsoutheastnortheastsouthwestsouthwestnortheastnortheastnorthwestsouthwestsouthwestsouthwestnorthwestsouthwestsouthwestnortheastnorthwestnorthwestsoutheastnorthwestnortheastsoutheastnorthwestsouthwestsoutheastsoutheastsouthwestsoutheastsouthwestnortheastnorthwestnortheastsoutheastsoutheastsouthwestnortheastsoutheastnorthwestnorthwestsoutheastnortheastsouthwestnortheastsoutheastsoutheastsouthwestsoutheastsoutheastnorthwestnorthwestnorthwestsoutheastnortheastsouthwestnorthwestsoutheastnortheastsoutheastsoutheastnorthwestsouthwestnortheastnorthwestsouthwestnorthwestsoutheastsoutheastnorthwestnortheastsoutheastnortheastnortheastnorthwestsouthwestnorthwestnorthwestnorthwestnorthwestsoutheastsouthwestnortheastnortheastnorthwestsouthwestnortheastsoutheastsoutheastnortheastsoutheastsouthwestsoutheastsouthwestnorthwestnortheastnorthwestnortheastnortheastnortheastsouthwestsoutheastnorthwestsoutheastsouthwestsouthwestsouthwestnorthwestnortheastsouthwestsouthwestsoutheastsouthwestnortheastsouthwestsouthwestsoutheastsoutheastsouthwestnortheastnorthwestsouthwestsoutheastsouthwestsouthwestsouthwestnorthwestnortheastnortheastnorthwestsoutheastnortheastnorthwestsouthwestnorthwestnorthwestsoutheastnortheastsoutheastnortheastsouthwestnortheastnorthwestsoutheastnorthwestnortheastsoutheastnortheastsoutheastsouthwestnortheastnortheastsoutheastsouthwestsouthwestnorthwestnortheastnorthwestsouthwestnorthwestsoutheastnorthwestnortheastsouthwestsoutheastsouthwestsouthwestsouthwestnorthwestsouthwestnortheastsouthwestsouthwestsoutheastsoutheastsoutheastsouthwestsouthwestsouthwestsoutheastsouthwestnortheastnorthwestsoutheastsoutheastsoutheastsoutheastnortheastnorthwestsoutheastsouthwestsouthwestnortheastnorthwestsouthwestnortheastsoutheastnorthwestsouthwestnorthwestsoutheastsoutheastnorthwestnortheastnorthwestnorthwestsouthwestsoutheastnortheastnorthwestsouthwestnorthwestnorthwestnortheastsoutheastsoutheastnortheastnorthwestsouthwestsoutheastnortheastsoutheastsoutheastnortheastsoutheastnortheastnortheastsouthwestnortheastnortheastsouthwestnorthwestnorthwestnortheastnortheastsouthwestnortheastsouthwestsoutheastnorthwestnortheastsouthwestsoutheastnortheastnorthwestnorthwestsouthwestsouthwestsouthwestnortheastnorthwestnortheastnorthwestnortheastnortheastsouthwestsoutheastsoutheastnorthwestsouthwestnorthwestnorthwestsouthwestnorthwestnorthwestsouthwestsoutheastsoutheastsoutheastsoutheastsouthwestnorthwestnorthwestsouthwestnortheastnorthwestsoutheastnortheastnortheastnorthwestsouthwestsoutheastnorthwestnortheastnorthwestnorthwestnortheastnortheastsouthwestnortheastnorthwestnortheastsoutheastnorthwestsouthwestnorthwestnortheastnortheastsouthwestnorthwestnorthwestsouthwestsoutheastsoutheastnorthwestsoutheastsoutheastsoutheastnorthwestsouthwestsouthwestsoutheastnortheastnorthwestsoutheastsoutheastnortheastnorthwestnortheastnortheastsoutheastsouthwestnortheastsoutheastsoutheastsoutheastnorthwestnorthwestsouthwestnorthwestsouthwestnortheastnorthwestsoutheastsouthwestsoutheastnortheastsouthwestnorthwestsouthwestnortheastnortheastsoutheastnortheastsoutheastnortheastsouthwestsoutheastsoutheastsouthwestsoutheastnorthwestnorthwestsouthwestsoutheastnortheastsoutheastsoutheastnorthwestnortheastsoutheastnortheastsoutheastsoutheastnorthwestsouthwestsoutheastnorthwestnortheastnortheastnorthwestsouthwestsoutheastsouthwestsouthwestsoutheastsouthwestnortheastnorthwestnorthwestnorthwestsouthwestnorthwestsoutheastnorthwestsoutheastsouthwestsoutheastsoutheastsouthwestnorthwestsouthwestnorthwestsouthwestsouthwestnortheastnorthwestsoutheastnorthwestnorthwestnortheastsoutheastnorthwestnortheastsouthwestnorthwestsoutheastsoutheastnortheastnorthwestnortheastsoutheastsouthwestsouthwestnorthwestnortheastsouthwestsoutheastnorthwestnorthwestsouthwestnorthwestsouthwestnortheastsoutheastnortheastnorthwestsouthwestnortheastsoutheastnortheastnorthwestnorthwestnortheastsouthwestnorthwestnortheastnortheastnorthwestnorthwestnorthwestnorthwestsoutheastnorthwestsouthwestnorthwestnorthwestnorthwestnortheastsoutheastnorthwestsouthwestsouthwestnortheastsouthwestnorthwestsoutheastnortheastsouthwestnorthwestnortheastsoutheastsoutheastsouthwestnorthwestnortheastsoutheastsoutheastsoutheastnortheastsoutheastnortheastsoutheastsoutheastnortheastnorthwestsouthwestnorthwestsouthwestsoutheastnorthwestnortheastnorthwestnortheastsoutheastsoutheastsoutheastnorthwestsoutheastsoutheastsouthwestsouthwestsouthwestsoutheastnortheastnortheastsouthwestsouthwestsouthwestsoutheastsouthwestnorthwestnorthwestnorthwestnortheastnortheastsouthwestsoutheastsouthwestnortheastsoutheastsouthwestnortheastsouthwestsouthwestnorthwestnorthwestsoutheastsoutheastsoutheastsouthwestnortheastnorthwestnortheastnorthwestsoutheastnorthwestnortheastsoutheastsouthwestnortheastnortheastsouthwestsouthwestsoutheastnortheastsouthwestsoutheastnorthwestnortheastsouthwestnortheastsoutheastnorthwestnorthwestsoutheastnorthwestsouthwestsouthwestnortheastsoutheastnortheastnorthwestsouthwestsouthwestnorthwestnorthwestsouthwestsouthwestnorthwestnortheastsouthwestsoutheastnortheastnorthwestnorthwestnortheastsoutheastsoutheastnorthwestnortheastnortheastsoutheastnortheastsouthwestsoutheastsouthwestsouthwestnorthwestnortheastsoutheastsouthwestnorthwest'] to numeric"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values in 'age', 'sex', and 'region'\n",
    "df.dropna(subset=['age', 'sex', 'region'], inplace=True)\n",
    "\n",
    "# Convert 'sex' and 'smoker' to binary format\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "df['smoker'] = df['smoker'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Normalize 'age', 'bmi', 'children', and 'charges' columns\n",
    "scaler = MinMaxScaler()\n",
    "df[['age', 'bmi', 'children', 'charges']] = scaler.fit_transform(df[['age', 'bmi', 'children', 'charges']])\n",
    "\n",
    "# Calculate the mean for each column after preprocessing\n",
    "mean_values = df.mean(axis=0).round(4)\n",
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19671666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0.4610\n",
       "sex         0.5052\n",
       "bmi         0.3956\n",
       "children    0.2190\n",
       "smoker      0.2048\n",
       "charges     0.1939\n",
       "dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean for numeric columns only after preprocessing\n",
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "mean_values = numeric_columns.mean(axis=0).round(4)\n",
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ab4c47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             #featureID  row ID     row m/z  \\\n",
       "0    358.3677167129743_3.65612984126984     241  358.367717   \n",
       "1    423.2744890715284_4.29798541001065     695  423.274489   \n",
       "2   304.2993572401259_5.121302585521083     382  304.299357   \n",
       "3   389.2691196723436_3.383737479270316     300  389.269120   \n",
       "4  332.3307817246258_5.3103554720133594     612  332.330782   \n",
       "\n",
       "   row retention time                                          LibraryID  \\\n",
       "0            3.656130                                                NaN   \n",
       "1            4.297985                                                NaN   \n",
       "2            5.121303  Spectral Match to Benzyldodecyldimethylammoniu...   \n",
       "3            3.383737                                                NaN   \n",
       "4            5.310355  Spectral Match to Benzyltetradecyldimethylammo...   \n",
       "\n",
       "  standard_indentification_level_1                           _feature_id  \\\n",
       "0                              NaN    358.3677167129743_3.65612984126984   \n",
       "1                              NaN    423.2744890715284_4.29798541001065   \n",
       "2                              NaN   304.2993572401259_5.121302585521083   \n",
       "3                              NaN   389.2691196723436_3.383737479270316   \n",
       "4                              NaN  332.3307817246258_5.3103554720133594   \n",
       "\n",
       "   importance.score  \n",
       "0          0.067052  \n",
       "1          0.040598  \n",
       "2          0.034141  \n",
       "3          0.032521  \n",
       "4          0.032257  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/imp.score.ldlr.metabolome.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba1dabc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@correlation_coefficient[-0.04] @p_value[0.4058] @relationship_type[none]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform correlation analysis\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Drop rows with missing values in the selected columns\n",
    "data_cleaned = data[['row retention time', 'importance.score']].dropna()\n",
    "\n",
    "# Calculate Pearson correlation coefficient and p-value\n",
    "r_value, p_value = pearsonr(data_cleaned['row retention time'], data_cleaned['importance.score'])\n",
    "\n",
    "# Determine the relationship type\n",
    "if p_value < 0.05:\n",
    "    if abs(r_value) >= 0.5:\n",
    "        relationship_type = 'linear'\n",
    "    else:\n",
    "        relationship_type = 'nonlinear'\n",
    "else:\n",
    "    relationship_type = 'none'\n",
    "\n",
    "# Format results\n",
    "results = f\"@correlation_coefficient[{r_value:.2f}] @p_value[{p_value:.4f}] @relationship_type[{relationship_type}]\"\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "740bcf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Country  Year              No. of cases      No. of deaths  \\\n",
       "0  Afghanistan  2017     630308[495000-801000]       298[110-510]   \n",
       "1      Algeria  2017                         0                  0   \n",
       "2       Angola  2017  4615605[3106000-6661000]  13316[9970-16600]   \n",
       "3    Argentina  2017                         0                  0   \n",
       "4      Armenia  2017                         0                  0   \n",
       "\n",
       "   No. of cases_median  No. of cases_min  No. of cases_max  \\\n",
       "0               630308          495000.0          801000.0   \n",
       "1                    0               NaN               NaN   \n",
       "2              4615605         3106000.0         6661000.0   \n",
       "3                    0               NaN               NaN   \n",
       "4                    0               NaN               NaN   \n",
       "\n",
       "   No. of deaths_median  No. of deaths_min  No. of deaths_max  \\\n",
       "0                   298              110.0              510.0   \n",
       "1                     0                NaN                NaN   \n",
       "2                 13316             9970.0            16600.0   \n",
       "3                     0                NaN                NaN   \n",
       "4                     0                NaN                NaN   \n",
       "\n",
       "              WHO Region  \n",
       "0  Eastern Mediterranean  \n",
       "1                 Africa  \n",
       "2                 Africa  \n",
       "3               Americas  \n",
       "4                 Europe  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and inspect the first few records\n",
    "import pandas as pd\n",
    "file_path = './data/estimated_numbers.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a564846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, np.float64(-18150.0), np.float64(30730.0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect and analyze outliers using IQR method\n",
    "def detect_outliers_iqr(df, column):\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return df[(df[column] < lower_bound) | (df[column] > upper_bound)], lower_bound, upper_bound\n",
    "\n",
    "# Convert 'No. of deaths_max' to numeric (cleaning if necessary)\n",
    "data['No. of deaths_max'] = pd.to_numeric(data['No. of deaths_max'], errors='coerce')\n",
    "\n",
    "# Find outliers in 'No. of deaths_max'\n",
    "outliers, lower_bound, upper_bound = detect_outliers_iqr(data, 'No. of deaths_max')\n",
    "number_of_countries_with_outliers = outliers['Country'].nunique()\n",
    "\n",
    "number_of_countries_with_outliers, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "877ebf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(10149.43), np.float64(5779.55))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean values with and without outliers in 'No. of deaths_max'\n",
    "original_mean = round(data['No. of deaths_max'].mean(), 2)\n",
    "\n",
    "# Remove outliers from the dataset\n",
    "clean_data = data[~data['Country'].isin(outliers['Country'])]\n",
    "new_mean = round(clean_data['No. of deaths_max'].mean(), 2)\n",
    "\n",
    "original_mean, new_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed9f608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081990"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct parsing for 'No. of cases'\n",
    "import numpy as np\n",
    "\n",
    "data['No. of cases'] = data['No. of cases'].str.split('[').str[0]  # Extract numeric portion before '['\n",
    "data['No. of cases'] = pd.to_numeric(data['No. of cases'], errors='coerce')  # Convert to numeric, replace errors with NaN\n",
    "mean_cases = data['No. of cases'].dropna().mean()  # Ignore NaN and calculate mean\n",
    "mean_cases = int(mean_cases)  # Convert to integer\n",
    "mean_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6a272bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './data/estimated_numbers.csv'\n",
    "# Read the dataset\n",
    "try:\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['No. of cases'] = pd.to_numeric(data['No. of cases'], errors='coerce')  # Convert column to numeric, replacing invalids with NaN\n",
    "    mean_cases = data['No. of cases'].dropna().mean()  # Drop NaN values and calculate mean\n",
    "    mean_cases = int(mean_cases)  # Convert to integer\n",
    "    mean_cases\n",
    "except Exception as e:\n",
    "    e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76ed24ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(22607406.19), np.float64(8254791.71))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/microsoft.csv')\n",
    "mean_volume = round(data['Volume'].mean(), 2)\n",
    "std_dev_volume = round(data['Volume'].std(), 2)\n",
    "mean_volume, std_dev_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0d2d975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "ModuleNotFoundError                       Traceback (most recent call last)\n",
       "Cell In[68], line 4\n",
       "      2 from sklearn.linear_model import LinearRegression\n",
       "      3 from sklearn.metrics import r2_score\n",
       "----> 4 import statsmodels.api as sm\n",
       "      6 # Load the dataset\n",
       "      7 data_path = './data/country_vaccinations.csv'\n",
       "\n",
       "ModuleNotFoundError: No module named 'statsmodels'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/country_vaccinations.csv'\n",
    "vaccination_data = pd.read_csv(data_path)\n",
    "\n",
    "# Filter data to only include rows without nulls in the required columns\n",
    "filtered_data = vaccination_data.dropna(subset=['people_fully_vaccinated_per_hundred', 'total_vaccinations', 'people_vaccinated_per_hundred'])\n",
    "\n",
    "# Define the predictors (independent variables) and the dependent variable\n",
    "dependent_var = filtered_data['people_fully_vaccinated_per_hundred']\n",
    "independent_vars = filtered_data[['total_vaccinations', 'people_vaccinated_per_hundred']]\n",
    "\n",
    "# Fit the multiple linear regression model\n",
    "independent_vars_with_const = sm.add_constant(independent_vars) # Add constant for the intercept term\n",
    "model = sm.OLS(dependent_var, independent_vars_with_const).fit()\n",
    "\n",
    "# Get the p-values for the predictors and R-squared value\n",
    "p_values = model.pvalues[1:] # Exclude the constant term\n",
    "r_squared = model.rsquared\n",
    "\n",
    "# Determine significant predictors based on alpha=0.05\n",
    "significance = ['yes' if p < 0.05 else 'no' for p in p_values]\n",
    "\n",
    "(p_values, significance, r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0d8d5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(total_vaccinations                6.689236e-03\n",
       " people_vaccinated_per_hundred    1.730515e-237\n",
       " dtype: float64,\n",
       " ['yes', 'yes'],\n",
       " np.float64(0.6059296776010953))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/country_vaccinations.csv'\n",
    "vaccination_data = pd.read_csv(data_path)\n",
    "\n",
    "# Filter data to only include rows without nulls in the required columns\n",
    "filtered_data = vaccination_data.dropna(subset=['people_fully_vaccinated_per_hundred', 'total_vaccinations', 'people_vaccinated_per_hundred'])\n",
    "\n",
    "# Define the predictors (independent variables) and the dependent variable\n",
    "dependent_var = filtered_data['people_fully_vaccinated_per_hundred']\n",
    "independent_vars = filtered_data[['total_vaccinations', 'people_vaccinated_per_hundred']]\n",
    "\n",
    "# Fit the multiple linear regression model\n",
    "independent_vars_with_const = sm.add_constant(independent_vars) # Add constant for the intercept term\n",
    "model = sm.OLS(dependent_var, independent_vars_with_const).fit()\n",
    "\n",
    "# Get the p-values for the predictors and R-squared value\n",
    "p_values = model.pvalues[1:] # Exclude the constant term\n",
    "r_squared = model.rsquared\n",
    "\n",
    "# Determine significant predictors based on alpha=0.05\n",
    "significance = ['yes' if p < 0.05 else 'no' for p in p_values]\n",
    "\n",
    "(p_values, significance, r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba3779b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(7.337348958673594e-08), np.False_)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Titanic dataset and ignore null values in the Age column\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "age_data = data['Age'].dropna()\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "stat, p_value = shapiro(age_data)\n",
    "\n",
    "# Check if data follow a normal distribution\n",
    "is_normal = p_value > 0.05\n",
    "\n",
    "p_value, is_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d102f9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Calculate Z-scores for fare column\n",
    "fare_mean = data['Fare'].mean()\n",
    "fare_std = data['Fare'].std()\n",
    "data['Z_score_fare'] = (data['Fare'] - fare_mean) / fare_std\n",
    "\n",
    "# Identify outliers\n",
    "outliers = data[(data['Z_score_fare'] > 3) | (data['Z_score_fare'] < -3)]\n",
    "outlier_count = len(outliers)\n",
    "outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3660a49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Country          Region  Happiness Rank  Happiness Score  \\\n",
       "0  Switzerland  Western Europe               1            7.587   \n",
       "1      Iceland  Western Europe               2            7.561   \n",
       "2      Denmark  Western Europe               3            7.527   \n",
       "3       Norway  Western Europe               4            7.522   \n",
       "4       Canada   North America               5            7.427   \n",
       "\n",
       "   Standard Error  Economy (GDP per Capita)   Family  \\\n",
       "0         0.03411                   1.39651  1.34951   \n",
       "1         0.04884                   1.30232  1.40223   \n",
       "2         0.03328                   1.32548  1.36058   \n",
       "3         0.03880                   1.45900  1.33095   \n",
       "4         0.03553                   1.32629  1.32261   \n",
       "\n",
       "   Health (Life Expectancy)  Freedom  Trust (Government Corruption)  \\\n",
       "0                   0.94143  0.66557                        0.41978   \n",
       "1                   0.94784  0.62877                        0.14145   \n",
       "2                   0.87464  0.64938                        0.48357   \n",
       "3                   0.88521  0.66973                        0.36503   \n",
       "4                   0.90563  0.63297                        0.32957   \n",
       "\n",
       "   Generosity  Dystopia Residual  \n",
       "0     0.29678            2.51738  \n",
       "1     0.43630            2.70201  \n",
       "2     0.34139            2.49204  \n",
       "3     0.34699            2.46531  \n",
       "4     0.45811            2.45176  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset named 2015.csv and inspect the first few rows\n",
    "data = pd.read_csv('./data/2015.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform correlation analysis\n",
    "# Compute correlation coefficients between Happiness Score and numerical variables\n",
    "correlations = data.corr(numeric_only=True)['Happiness Score'].sort_values(ascending=False)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45ab5562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './data/test_ave.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "mean_fare = round(df['Fare'].mean(), 2)\n",
    "\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75065d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2051038255697283)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "# Add a 'FamilySize' column\n",
    "data['FamilySize'] = data['SibSp'] + data['Parch']\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation_coefficient = data['FamilySize'].corr(data['Fare'])\n",
    "correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d49af1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.6770809651939094e-05), 'not normally distributed')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kstest, norm\n",
    "\n",
    "data = pd.read_csv('./data/imp.score.ldlr.metabolome.csv')\n",
    "\n",
    "# Extract the `row m/z` column (ignoring nulls)\n",
    "row_mz = data['row m/z'].dropna()\n",
    "\n",
    "# Perform the Kolmogorov-Smirnov test\n",
    "ks_stat, p_value = kstest(row_mz, 'norm', args=(row_mz.mean(), row_mz.std()))\n",
    "\n",
    "# Decide based on the p-value\n",
    "if p_value >= 0.05:\n",
    "    decision = \"normally distributed\"\n",
    "else:\n",
    "    decision = \"not normally distributed\"\n",
    "\n",
    "p_value, decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0278522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Country  Year              No. of cases      No. of deaths  \\\n",
       "0  Afghanistan  2017     630308[495000-801000]       298[110-510]   \n",
       "1      Algeria  2017                         0                  0   \n",
       "2       Angola  2017  4615605[3106000-6661000]  13316[9970-16600]   \n",
       "3    Argentina  2017                         0                  0   \n",
       "4      Armenia  2017                         0                  0   \n",
       "\n",
       "   No. of cases_median  No. of cases_min  No. of cases_max  \\\n",
       "0               630308          495000.0          801000.0   \n",
       "1                    0               NaN               NaN   \n",
       "2              4615605         3106000.0         6661000.0   \n",
       "3                    0               NaN               NaN   \n",
       "4                    0               NaN               NaN   \n",
       "\n",
       "   No. of deaths_median  No. of deaths_min  No. of deaths_max  \\\n",
       "0                   298              110.0              510.0   \n",
       "1                     0                NaN                NaN   \n",
       "2                 13316             9970.0            16600.0   \n",
       "3                     0                NaN                NaN   \n",
       "4                     0                NaN                NaN   \n",
       "\n",
       "              WHO Region  \n",
       "0  Eastern Mediterranean  \n",
       "1                 Africa  \n",
       "2                 Africa  \n",
       "3               Americas  \n",
       "4                 Europe  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset estimated_numbers.csv and inspect the data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"./data/estimated_numbers.csv\"\n",
    "dataset = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7735ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Nigeria', np.int64(2010))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter dataset to keep records with valid (non-null/non-empty) 'No. of deaths', and convert it from string to int.\n",
    "filtered_dataset = dataset[dataset['No. of deaths'].notnull() & (dataset['No. of deaths'] != \"\")]\n",
    "\n",
    "# Remove square brackets and text ranges from 'No. of deaths', converting to int\n",
    "filtered_dataset['No. of deaths'] = filtered_dataset['No. of deaths'].str.split('[').str[0].str.replace(',', '').astype(int)\n",
    "\n",
    "# Calculate the maximum value and corresponding country, year\n",
    "max_deaths_row = filtered_dataset.loc[filtered_dataset['No. of deaths'].idxmax()]\n",
    "\n",
    "max_deaths_country = max_deaths_row['Country']\n",
    "max_deaths_year = max_deaths_row['Year']\n",
    "max_deaths_country, max_deaths_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc12b78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Country                           Region  Happiness Rank  \\\n",
       "0    Switzerland                   Western Europe               1   \n",
       "1        Iceland                   Western Europe               2   \n",
       "2        Denmark                   Western Europe               3   \n",
       "3         Norway                   Western Europe               4   \n",
       "4         Canada                    North America               5   \n",
       "..           ...                              ...             ...   \n",
       "153       Rwanda               Sub-Saharan Africa             154   \n",
       "154        Benin               Sub-Saharan Africa             155   \n",
       "155        Syria  Middle East and Northern Africa             156   \n",
       "156      Burundi               Sub-Saharan Africa             157   \n",
       "157         Togo               Sub-Saharan Africa             158   \n",
       "\n",
       "     Happiness Score  Standard Error  Economy (GDP per Capita)   Family  \\\n",
       "0              7.587         0.03411                   1.39651  1.34951   \n",
       "1              7.561         0.04884                   1.30232  1.40223   \n",
       "2              7.527         0.03328                   1.32548  1.36058   \n",
       "3              7.522         0.03880                   1.45900  1.33095   \n",
       "4              7.427         0.03553                   1.32629  1.32261   \n",
       "..               ...             ...                       ...      ...   \n",
       "153            3.465         0.03464                   0.22208  0.77370   \n",
       "154            3.340         0.03656                   0.28665  0.35386   \n",
       "155            3.006         0.05015                   0.66320  0.47489   \n",
       "156            2.905         0.08658                   0.01530  0.41587   \n",
       "157            2.839         0.06727                   0.20868  0.13995   \n",
       "\n",
       "     Health (Life Expectancy)  Freedom  Trust (Government Corruption)  \\\n",
       "0                     0.94143  0.66557                        0.41978   \n",
       "1                     0.94784  0.62877                        0.14145   \n",
       "2                     0.87464  0.64938                        0.48357   \n",
       "3                     0.88521  0.66973                        0.36503   \n",
       "4                     0.90563  0.63297                        0.32957   \n",
       "..                        ...      ...                            ...   \n",
       "153                   0.42864  0.59201                        0.55191   \n",
       "154                   0.31910  0.48450                        0.08010   \n",
       "155                   0.72193  0.15684                        0.18906   \n",
       "156                   0.22396  0.11850                        0.10062   \n",
       "157                   0.28443  0.36453                        0.10731   \n",
       "\n",
       "     Generosity  Dystopia Residual  \n",
       "0       0.29678            2.51738  \n",
       "1       0.43630            2.70201  \n",
       "2       0.34139            2.49204  \n",
       "3       0.34699            2.46531  \n",
       "4       0.45811            2.45176  \n",
       "..          ...                ...  \n",
       "153     0.22628            0.67042  \n",
       "154     0.18260            1.63328  \n",
       "155     0.47179            0.32858  \n",
       "156     0.19727            1.83302  \n",
       "157     0.16681            1.56726  \n",
       "\n",
       "[158 rows x 12 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/2015.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b48a0c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the Z-scores for the happiness scores\n",
    "z_scores = zscore(data['Happiness Score'], ddof=0)\n",
    "\n",
    "# Identify countries with Z-scores greater than 3 or less than -3\n",
    "outlier_indices = (z_scores > 3) | (z_scores < -3)\n",
    "outlier_countries = data['Country'][outlier_indices].tolist()\n",
    "outlier_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbabee4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Create the 'IsAlone' feature\n",
    "data['IsAlone'] = (data['SibSp'] + data['Parch'] == 0).astype(int)\n",
    "\n",
    "# Define features and target\n",
    "X = data[['IsAlone']]\n",
    "y = data['Survived']\n",
    "\n",
    "# Perform a 70:30 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "round(accuracy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0129eff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(8.797424981276622e-22), 'Yes')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Filter the data for 1st and 3rd passenger classes and remove rows with null Age\n",
    "filtered_data = data[data['Pclass'].isin([1, 3]) & data['Age'].notnull()]\n",
    "\n",
    "# Split ages by passenger class\n",
    "class_1_ages = filtered_data[filtered_data['Pclass'] == 1]['Age']\n",
    "class_3_ages = filtered_data[filtered_data['Pclass'] == 3]['Age']\n",
    "\n",
    "# Perform Mann-Whitney U test\n",
    "stat, p_value = mannwhitneyu(class_1_ages, class_3_ages, alternative='two-sided')\n",
    "\n",
    "# Determine the significance at alpha = 0.05\n",
    "significant = \"Yes\" if p_value < 0.05 else \"No\"\n",
    "\n",
    "p_value, significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef2c415a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0           author  \\\n",
       "0           0         ABC News   \n",
       "1           1         ABC News   \n",
       "2           2      Karma Allen   \n",
       "3           3         ABC News   \n",
       "4           4  Meghan Keneally   \n",
       "\n",
       "                                         description          publishedAt  \\\n",
       "0  Updates to the app include an \"about this acco...  2018-08-29 10:44:48   \n",
       "1  Arizona primary voters Carlos Medina and Kaitl...  2018-08-29 01:22:02   \n",
       "2  She works at a Texas hospital where a toddler ...  2018-08-28 11:04:51   \n",
       "3  Arpaio, the controversial former Maricopa Coun...  2018-08-28 02:31:59   \n",
       "4  Kelli Ward is one of three Republicans vying f...  2018-08-28 02:27:43   \n",
       "\n",
       "     source                                              title  \\\n",
       "0  abc-news  WATCH: Instagram rolls out new features aimed ...   \n",
       "1  abc-news  WATCH: In the heat of primary day, young Arizo...   \n",
       "2  abc-news  Nurse under investigation after posting on ant...   \n",
       "3  abc-news  WATCH: Arizona Senate candidates Joe Arpaio, K...   \n",
       "4  abc-news  'Political correctness is like a cancer': AZ G...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://abcnews.go.com/Technology/video/instag...   \n",
       "1  https://abcnews.go.com/Politics/video/heat-pri...   \n",
       "2  https://abcnews.go.com/US/texas-nurse-investig...   \n",
       "3  https://abcnews.go.com/Nightline/video/arizona...   \n",
       "4  https://abcnews.go.com/Politics/political-corr...   \n",
       "\n",
       "                                          urlToImage  \\\n",
       "0  https://s.abcnews.com/images/Technology/180829...   \n",
       "1  https://s.abcnews.com/images/Politics/180828_v...   \n",
       "2  https://s.abcnews.com/images/Politics/measles-...   \n",
       "3  https://s.abcnews.com/images/Nightline/180827_...   \n",
       "4  https://s.abcnews.com/images/Politics/kelli-wa...   \n",
       "\n",
       "                                                text    neg    neu    pos  \\\n",
       "0  Coming up in the next {{countdown}} {{countdow...  0.067  0.733  0.200   \n",
       "1  Coming up in the next {{countdown}} {{countdow...  0.062  0.735  0.204   \n",
       "2  \\nA nurse at a hospital in Texas where a toddl...  0.051  0.847  0.102   \n",
       "3  Coming up in the next {{countdown}} {{countdow...  0.068  0.762  0.169   \n",
       "4  \\nOne of the Republicans running for Senate in...  0.101  0.851  0.048   \n",
       "\n",
       "   compound  \n",
       "0    0.9746  \n",
       "1    0.9869  \n",
       "2    0.9875  \n",
       "3    0.9799  \n",
       "4   -0.9872  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "file_path = './data/fb_articles_20180822_20180829_df.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "486573b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify columns of type 'object'\n",
    "object_columns = data.select_dtypes(include=['object'])\n",
    "\n",
    "# Check for missing values ('NaN', 'na', 'null', or '')\n",
    "missing_columns = object_columns.columns[object_columns.isin([\"NaN\", \"na\", \"null\", \"\", None]).any()].tolist()\n",
    "\n",
    "# Sort alphabetically\n",
    "missing_columns.sort()\n",
    "\n",
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9e7e9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Not Normal', np.float64(0.0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/3901.csv')\n",
    "\n",
    "# Extract the MEANPOT column\n",
    "meanpot_data = data['MEANPOT']\n",
    "\n",
    "# Perform the Shapiro-Wilk test for normality\n",
    "shapiro_test_stat, p_value = shapiro(meanpot_data)\n",
    "\n",
    "# Determine normality based on p-value\n",
    "normality_test_result = 'Normal' if p_value > 0.05 else 'Not Normal'\n",
    "\n",
    "normality_test_result, round(p_value, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65c7d9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEUTRALDESCRIPTION'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/0020200722.csv')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "# Get column with the highest number of missing values\n",
    "max_missing_column = missing_columns.idxmax()\n",
    "\n",
    "max_missing_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bdea9d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(21144.08), np.float64(19711.0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/2014_q4.csv')\n",
    "\n",
    "# Calculate mean and median for the specified column\n",
    "mean_value = round(data['Trips over the past 24-hours (midnight to 11:59pm)'].mean(), 2)\n",
    "median_value = round(data['Trips over the past 24-hours (midnight to 11:59pm)'].median(), 2)\n",
    "\n",
    "mean_value, median_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e2d92d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'date'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[1], line 8\n",
       "      5 data = pd.read_csv('./data/bitconnect_price.csv')\n",
       "      7 # Convert 'date' column to datetime format, and add day_of_week column\n",
       "----> 8 data['date'] = pd.to_datetime(data['date'])\n",
       "      9 data['day_of_week'] = data['date'].dt.dayofweek\n",
       "     11 # Separate weekdays (Monday: 0 to Friday: 4) and weekends (Saturday: 5, Sunday: 6)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'date'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/bitconnect_price.csv')\n",
    "\n",
    "# Convert 'date' column to datetime format, and add day_of_week column\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "\n",
    "# Separate weekdays (Monday: 0 to Friday: 4) and weekends (Saturday: 5, Sunday: 6)\n",
    "weekday_data = data[data['day_of_week'] < 5]\n",
    "weekend_data = data[data['day_of_week'] >= 5]\n",
    "\n",
    "# Calculate mean opening prices\n",
    "weekday_mean_price = round(weekday_data['open'].mean(), 2)\n",
    "weekend_mean_price = round(weekend_data['open'].mean(), 2)\n",
    "\n",
    "# Two-sample t-test\n",
    "t_stat, p_value = ttest_ind(weekday_data['open'], weekend_data['open'], equal_var=False)\n",
    "\n",
    "p_value = round(p_value, 4)\n",
    "significant_or_not = 'Yes' if p_value < 0.05 else 'No'\n",
    "\n",
    "weekday_mean_price, weekend_mean_price, p_value, significant_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5b3276d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "ValueError                                Traceback (most recent call last)\n",
       "Cell In[3], line 14\n",
       "     11 weekend_data = data[data['day_of_week'] >= 5]\n",
       "     13 # Ensure the 'Open' column is converted to numeric after dropping any ','\n",
       "---> 14 data['Open'] = data['Open'].replace({',': ''}, regex=True).astype(float, errors='coerce')\n",
       "     15 weekday_data['Open'] = pd.to_numeric(weekday_data['Open'], errors='coerce')\n",
       "     16 weekend_data['Open'] = pd.to_numeric(weekend_data['Open'], errors='coerce')\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/generic.py:6665, in NDFrame.astype(self, dtype, copy, errors)\n",
       "   6659     results = [\n",
       "   6660         ser.astype(dtype, copy=copy, errors=errors) for _, ser in self.items()\n",
       "   6661     ]\n",
       "   6663 else:\n",
       "   6664     # else, only a single dtype is given\n",
       "-> 6665     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n",
       "   6666     res = self._constructor_from_mgr(new_data, axes=new_data.axes)\n",
       "   6667     return res.__finalize__(self, method=\"astype\")\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/internals/managers.py:449, in BaseBlockManager.astype(self, dtype, copy, errors)\n",
       "    446 elif using_copy_on_write():\n",
       "    447     copy = False\n",
       "--> 449 return self.apply(\n",
       "    450     \"astype\",\n",
       "    451     dtype=dtype,\n",
       "    452     copy=copy,\n",
       "    453     errors=errors,\n",
       "    454     using_cow=using_copy_on_write(),\n",
       "    455 )\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/internals/managers.py:363, in BaseBlockManager.apply(self, f, align_keys, **kwargs)\n",
       "    361         applied = b.apply(f, **kwargs)\n",
       "    362     else:\n",
       "--> 363         applied = getattr(b, f)(**kwargs)\n",
       "    364     result_blocks = extend_blocks(applied, result_blocks)\n",
       "    366 out = type(self).from_blocks(result_blocks, self.axes)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/internals/blocks.py:784, in Block.astype(self, dtype, copy, errors, using_cow, squeeze)\n",
       "    781         raise ValueError(\"Can not squeeze with more than one column.\")\n",
       "    782     values = values[0, :]  # type: ignore[call-overload]\n",
       "--> 784 new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n",
       "    786 new_values = maybe_coerce_values(new_values)\n",
       "    788 refs = None\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:222, in astype_array_safe(values, dtype, copy, errors)\n",
       "    217 if errors not in errors_legal_values:\n",
       "    218     invalid_arg = (\n",
       "    219         \"Expected value of kwarg 'errors' to be one of \"\n",
       "    220         f\"{list(errors_legal_values)}. Supplied value is '{errors}'\"\n",
       "    221     )\n",
       "--> 222     raise ValueError(invalid_arg)\n",
       "    224 if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):\n",
       "    225     msg = (\n",
       "    226         f\"Expected an instance of {dtype.__name__}, \"\n",
       "    227         \"but got the class instead. Try instantiating 'dtype'.\"\n",
       "    228     )\n",
       "\n",
       "ValueError: Expected value of kwarg 'errors' to be one of ['raise', 'ignore']. Supplied value is 'coerce'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load the dataset with correct 'Date' column used\n",
    "data = pd.read_csv('./data/bitconnect_price.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['day_of_week'] = data['Date'].dt.dayofweek\n",
    "\n",
    "# Separate weekdays (Monday: 0 to Friday: 4) and weekends (Saturday: 5, Sunday: 6)\n",
    "weekday_data = data[data['day_of_week'] < 5]\n",
    "weekend_data = data[data['day_of_week'] >= 5]\n",
    "\n",
    "# Ensure the 'Open' column is converted to numeric after dropping any ','\n",
    "data['Open'] = data['Open'].replace({',': ''}, regex=True).astype(float, errors='coerce')\n",
    "weekday_data['Open'] = pd.to_numeric(weekday_data['Open'], errors='coerce')\n",
    "weekend_data['Open'] = pd.to_numeric(weekend_data['Open'], errors='coerce')\n",
    "\n",
    "# Calculate mean opening prices\n",
    "weekday_mean_price = round(weekday_data['Open'].mean(), 2)\n",
    "weekend_mean_price = round(weekend_data['Open'].mean(), 2)\n",
    "\n",
    "# Two-sample t-test\n",
    "t_stat, p_value = ttest_ind(weekday_data['Open'].dropna(), weekend_data['Open'].dropna(), equal_var=False)\n",
    "\n",
    "p_value = round(p_value, 4)\n",
    "significant_or_not = 'Yes' if p_value < 0.05 else 'No'\n",
    "\n",
    "weekday_mean_price, weekend_mean_price, p_value, significant_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9875653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_30428/4200369053.py:15: SettingWithCopyWarning: \n",
       "A value is trying to be set on a copy of a slice from a DataFrame.\n",
       "Try using .loc[row_indexer,col_indexer] = value instead\n",
       "\n",
       "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
       "  weekday_data['Open'] = pd.to_numeric(weekday_data['Open'], errors='coerce')\n",
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_30428/4200369053.py:16: SettingWithCopyWarning: \n",
       "A value is trying to be set on a copy of a slice from a DataFrame.\n",
       "Try using .loc[row_indexer,col_indexer] = value instead\n",
       "\n",
       "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
       "  weekend_data['Open'] = pd.to_numeric(weekend_data['Open'], errors='coerce')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(37.3), np.float64(38.49), np.float64(0.8463), 'No')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load the dataset with correct 'Date' column used\n",
    "data = pd.read_csv('./data/bitconnect_price.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['day_of_week'] = data['Date'].dt.dayofweek\n",
    "\n",
    "# Separate weekdays (Monday: 0 to Friday: 4) and weekends (Saturday: 5, Sunday: 6)\n",
    "weekday_data = data[data['day_of_week'] < 5]\n",
    "weekend_data = data[data['day_of_week'] >= 5]\n",
    "\n",
    "# Ensure the 'Open' column is converted to numeric after dropping any ','\n",
    "data['Open'] = data['Open'].replace({',': ''}, regex=True).astype(float)\n",
    "weekday_data['Open'] = pd.to_numeric(weekday_data['Open'], errors='coerce')\n",
    "weekend_data['Open'] = pd.to_numeric(weekend_data['Open'], errors='coerce')\n",
    "\n",
    "# Calculate mean opening prices\n",
    "weekday_mean_price = round(weekday_data['Open'].mean(), 2)\n",
    "weekend_mean_price = round(weekend_data['Open'].mean(), 2)\n",
    "\n",
    "# Two-sample t-test\n",
    "t_stat, p_value = ttest_ind(weekday_data['Open'].dropna(), weekend_data['Open'].dropna(), equal_var=False)\n",
    "\n",
    "p_value = round(p_value, 4)\n",
    "significant_or_not = 'Yes' if p_value < 0.05 else 'No'\n",
    "\n",
    "weekday_mean_price, weekend_mean_price, p_value, significant_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f5e8191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date  Trips over the past 24-hours (midnight to 11:59pm)  \\\n",
       "0  10/1/2014                                              31197    \n",
       "1  10/2/2014                                              38286    \n",
       "2  10/3/2014                                              38956    \n",
       "3  10/4/2014                                              15088    \n",
       "4  10/5/2014                                              26247    \n",
       "\n",
       "   Cumulative trips (since launch):  \\\n",
       "0                          13296973   \n",
       "1                          13335259   \n",
       "2                          13374215   \n",
       "3                          13389303   \n",
       "4                          13415550   \n",
       "\n",
       "   Miles traveled today (midnight to 11:59 pm)  Miles traveled to date:  \\\n",
       "0                                        44612                 23121175   \n",
       "1                                        60639                 23181814   \n",
       "2                                        65739                 23247553   \n",
       "3                                        24254                 23271807   \n",
       "4                                        48930                 23320737   \n",
       "\n",
       "   Total Annual Memberships Sold  \\\n",
       "0                         124846   \n",
       "1                         124959   \n",
       "2                         125024   \n",
       "3                         125058   \n",
       "4                         125109   \n",
       "\n",
       "  Annual Member Sign-Ups (midnight to 11:59 pm)  \\\n",
       "0                                           112   \n",
       "1                                           113   \n",
       "2                                            65   \n",
       "3                                            34   \n",
       "4                                            51   \n",
       "\n",
       "  24-Hour Passes Purchased (midnight to 11:59 pm)  \\\n",
       "0                                             330   \n",
       "1                                             602   \n",
       "2                                            1276   \n",
       "3                                             617   \n",
       "4                                            1470   \n",
       "\n",
       "  7-Day Passes Purchased (midnight to 11:59 pm)  \n",
       "0                                            48  \n",
       "1                                            86  \n",
       "2                                           107  \n",
       "3                                            26  \n",
       "4                                            90  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "file_path = './data/2014_q4.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09323c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       92.000000\n",
       "mean     21144.076087\n",
       "std       9889.925052\n",
       "min       4064.000000\n",
       "25%      13723.000000\n",
       "50%      19711.000000\n",
       "75%      30658.750000\n",
       "max      38956.000000\n",
       "Name: Trips over the past 24-hours (midnight to 11:59pm), dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relevant column selection for analysis\n",
    "relevant_col = 'Trips over the past 24-hours (midnight to 11:59pm)'\n",
    "data[relevant_col].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05d91386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9543095491220899),\n",
       " np.float64(0.0027291466836339875),\n",
       " np.float64(0.1520317726967334),\n",
       " np.float64(-1.1336054960924962))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "shapiro_stat, shapiro_p_value = shapiro(data[relevant_col])\n",
    "\n",
    "# Calculate skewness and kurtosis\n",
    "skewness_value = skew(data[relevant_col])\n",
    "kurtosis_value = kurtosis(data[relevant_col])\n",
    "\n",
    "# Display results\n",
    "shapiro_stat, shapiro_p_value, skewness_value, kurtosis_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a78907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   time  sun  temperature  chill  humidity wind direction  \\\n",
       "0  2017-01-01T00:00:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "1  2017-01-01T00:10:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "2  2017-01-01T00:20:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "3  2017-01-01T00:30:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "4  2017-01-01T00:40:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "\n",
       "   wind speed  visibility  air pressure  \n",
       "0         3.0       197.0        1026.0  \n",
       "1         3.0       195.0        1025.8  \n",
       "2         3.0       271.0        1025.6  \n",
       "3         3.0       316.0        1025.4  \n",
       "4         3.0       334.0        1025.3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/weather_train.csv')\n",
    "\n",
    "# Inspect the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52df4ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "ValueError                                Traceback (most recent call last)\n",
       "Cell In[270], line 13\n",
       "     11 # Initialize and train the Linear Regression model\n",
       "     12 model = LinearRegression()\n",
       "---> 13 model.fit(X_train, y_train)\n",
       "     15 # Make predictions\n",
       "     16 y_pred = model.predict(X_test)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/base.py:1365, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\n",
       "   1358     estimator._validate_params()\n",
       "   1360 with config_context(\n",
       "   1361     skip_parameter_validation=(\n",
       "   1362         prefer_skip_nested_validation or global_skip_validation\n",
       "   1363     )\n",
       "   1364 ):\n",
       "-> 1365     return fit_method(estimator, *args, **kwargs)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/linear_model/_base.py:618, in LinearRegression.fit(self, X, y, sample_weight)\n",
       "    614 n_jobs_ = self.n_jobs\n",
       "    616 accept_sparse = False if self.positive else [\"csr\", \"csc\", \"coo\"]\n",
       "--> 618 X, y = validate_data(\n",
       "    619     self,\n",
       "    620     X,\n",
       "    621     y,\n",
       "    622     accept_sparse=accept_sparse,\n",
       "    623     y_numeric=True,\n",
       "    624     multi_output=True,\n",
       "    625     force_writeable=True,\n",
       "    626 )\n",
       "    628 has_sw = sample_weight is not None\n",
       "    629 if has_sw:\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:2971, in validate_data(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\n",
       "   2969         y = check_array(y, input_name=\"y\", **check_y_params)\n",
       "   2970     else:\n",
       "-> 2971         X, y = check_X_y(X, y, **check_params)\n",
       "   2972     out = X, y\n",
       "   2974 if not no_val_X and check_params.get(\"ensure_2d\", True):\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:1368, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n",
       "   1362     raise ValueError(\n",
       "   1363         f\"{estimator_name} requires y to be passed, but the target y is None\"\n",
       "   1364     )\n",
       "   1366 ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n",
       "-> 1368 X = check_array(\n",
       "   1369     X,\n",
       "   1370     accept_sparse=accept_sparse,\n",
       "   1371     accept_large_sparse=accept_large_sparse,\n",
       "   1372     dtype=dtype,\n",
       "   1373     order=order,\n",
       "   1374     copy=copy,\n",
       "   1375     force_writeable=force_writeable,\n",
       "   1376     ensure_all_finite=ensure_all_finite,\n",
       "   1377     ensure_2d=ensure_2d,\n",
       "   1378     allow_nd=allow_nd,\n",
       "   1379     ensure_min_samples=ensure_min_samples,\n",
       "   1380     ensure_min_features=ensure_min_features,\n",
       "   1381     estimator=estimator,\n",
       "   1382     input_name=\"X\",\n",
       "   1383 )\n",
       "   1385 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n",
       "   1387 check_consistent_length(X, y)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:1105, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n",
       "   1099     raise ValueError(\n",
       "   1100         f\"Found array with dim {array.ndim},\"\n",
       "   1101         f\" while dim <= 2 is required{context}.\"\n",
       "   1102     )\n",
       "   1104 if ensure_all_finite:\n",
       "-> 1105     _assert_all_finite(\n",
       "   1106         array,\n",
       "   1107         input_name=input_name,\n",
       "   1108         estimator_name=estimator_name,\n",
       "   1109         allow_nan=ensure_all_finite == \"allow-nan\",\n",
       "   1110     )\n",
       "   1112 if copy:\n",
       "   1113     if _is_numpy_namespace(xp):\n",
       "   1114         # only make a copy if `array` and `array_orig` may share memory`\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:120, in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)\n",
       "    117 if first_pass_isfinite:\n",
       "    118     return\n",
       "--> 120 _assert_all_finite_element_wise(\n",
       "    121     X,\n",
       "    122     xp=xp,\n",
       "    123     allow_nan=allow_nan,\n",
       "    124     msg_dtype=msg_dtype,\n",
       "    125     estimator_name=estimator_name,\n",
       "    126     input_name=input_name,\n",
       "    127 )\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:169, in _assert_all_finite_element_wise(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\n",
       "    152 if estimator_name and input_name == \"X\" and has_nan_error:\n",
       "    153     # Improve the error message on how to handle missing values in\n",
       "    154     # scikit-learn.\n",
       "    155     msg_err += (\n",
       "    156         f\"\\n{estimator_name} does not accept missing values\"\n",
       "    157         \" encoded as NaN natively. For supervised learning, you might want\"\n",
       "   (...)    167         \"#estimators-that-handle-nan-values\"\n",
       "    168     )\n",
       "--> 169 raise ValueError(msg_err)\n",
       "\n",
       "ValueError: Input X contains NaN.\n",
       "LinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the relevant features and target column for the model\n",
    "features = ['temperature', 'humidity', 'wind speed']\n",
    "target = 'sun'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training set (70%) and testing set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35594436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temperature    41\n",
       "humidity       41\n",
       "wind speed     41\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataset for missing values across the relevant columns\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67c46346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_30130/57667267.py:2: SettingWithCopyWarning: \n",
       "A value is trying to be set on a copy of a slice from a DataFrame\n",
       "\n",
       "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
       "  X.fillna(X.mean(), inplace=True)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "ValueError                                Traceback (most recent call last)\n",
       "Cell In[272], line 8\n",
       "      5 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
       "      7 model = LinearRegression()\n",
       "----> 8 model.fit(X_train, y_train)\n",
       "     10 # Make predictions\n",
       "     11 y_pred = model.predict(X_test)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/base.py:1365, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\n",
       "   1358     estimator._validate_params()\n",
       "   1360 with config_context(\n",
       "   1361     skip_parameter_validation=(\n",
       "   1362         prefer_skip_nested_validation or global_skip_validation\n",
       "   1363     )\n",
       "   1364 ):\n",
       "-> 1365     return fit_method(estimator, *args, **kwargs)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/linear_model/_base.py:618, in LinearRegression.fit(self, X, y, sample_weight)\n",
       "    614 n_jobs_ = self.n_jobs\n",
       "    616 accept_sparse = False if self.positive else [\"csr\", \"csc\", \"coo\"]\n",
       "--> 618 X, y = validate_data(\n",
       "    619     self,\n",
       "    620     X,\n",
       "    621     y,\n",
       "    622     accept_sparse=accept_sparse,\n",
       "    623     y_numeric=True,\n",
       "    624     multi_output=True,\n",
       "    625     force_writeable=True,\n",
       "    626 )\n",
       "    628 has_sw = sample_weight is not None\n",
       "    629 if has_sw:\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:2971, in validate_data(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\n",
       "   2969         y = check_array(y, input_name=\"y\", **check_y_params)\n",
       "   2970     else:\n",
       "-> 2971         X, y = check_X_y(X, y, **check_params)\n",
       "   2972     out = X, y\n",
       "   2974 if not no_val_X and check_params.get(\"ensure_2d\", True):\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:1385, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n",
       "   1366 ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n",
       "   1368 X = check_array(\n",
       "   1369     X,\n",
       "   1370     accept_sparse=accept_sparse,\n",
       "   (...)   1382     input_name=\"X\",\n",
       "   1383 )\n",
       "-> 1385 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n",
       "   1387 check_consistent_length(X, y)\n",
       "   1389 return X, y\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:1395, in _check_y(y, multi_output, y_numeric, estimator)\n",
       "   1393 \"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\n",
       "   1394 if multi_output:\n",
       "-> 1395     y = check_array(\n",
       "   1396         y,\n",
       "   1397         accept_sparse=\"csr\",\n",
       "   1398         ensure_all_finite=True,\n",
       "   1399         ensure_2d=False,\n",
       "   1400         dtype=None,\n",
       "   1401         input_name=\"y\",\n",
       "   1402         estimator=estimator,\n",
       "   1403     )\n",
       "   1404 else:\n",
       "   1405     estimator_name = _check_estimator_name(estimator)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:1105, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n",
       "   1099     raise ValueError(\n",
       "   1100         f\"Found array with dim {array.ndim},\"\n",
       "   1101         f\" while dim <= 2 is required{context}.\"\n",
       "   1102     )\n",
       "   1104 if ensure_all_finite:\n",
       "-> 1105     _assert_all_finite(\n",
       "   1106         array,\n",
       "   1107         input_name=input_name,\n",
       "   1108         estimator_name=estimator_name,\n",
       "   1109         allow_nan=ensure_all_finite == \"allow-nan\",\n",
       "   1110     )\n",
       "   1112 if copy:\n",
       "   1113     if _is_numpy_namespace(xp):\n",
       "   1114         # only make a copy if `array` and `array_orig` may share memory`\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:120, in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)\n",
       "    117 if first_pass_isfinite:\n",
       "    118     return\n",
       "--> 120 _assert_all_finite_element_wise(\n",
       "    121     X,\n",
       "    122     xp=xp,\n",
       "    123     allow_nan=allow_nan,\n",
       "    124     msg_dtype=msg_dtype,\n",
       "    125     estimator_name=estimator_name,\n",
       "    126     input_name=input_name,\n",
       "    127 )\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:169, in _assert_all_finite_element_wise(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\n",
       "    152 if estimator_name and input_name == \"X\" and has_nan_error:\n",
       "    153     # Improve the error message on how to handle missing values in\n",
       "    154     # scikit-learn.\n",
       "    155     msg_err += (\n",
       "    156         f\"\\n{estimator_name} does not accept missing values\"\n",
       "    157         \" encoded as NaN natively. For supervised learning, you might want\"\n",
       "   (...)    167         \"#estimators-that-handle-nan-values\"\n",
       "    168     )\n",
       "--> 169 raise ValueError(msg_err)\n",
       "\n",
       "ValueError: Input y contains NaN."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing values by imputing with the mean of each column\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Re-run the linear regression process\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c87e2e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(52)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the target column for missing values\n",
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da360df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.17"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing values in the target column by imputing with mean\n",
    "y.fillna(y.mean(), inplace=True)\n",
    "\n",
    "# Re-run the linear regression process\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3d3d7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.17), np.float64(0.16), np.float64(21144.08), np.float64(19711.0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/2014_q4.csv')\n",
    "data['Trips per Membership'] = data['Trips over the past 24-hours (midnight to 11:59pm)'] / data['Total Annual Memberships Sold']\n",
    "data['Trips per Membership'] = data['Trips per Membership'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# Calculate required statistics\n",
    "tpm_mean = round(data['Trips per Membership'].mean(), 2)\n",
    "tpm_median = round(data['Trips per Membership'].median(), 2)\n",
    "tpd_mean = round(data['Trips over the past 24-hours (midnight to 11:59pm)'].mean(), 2)\n",
    "tpd_median = round(data['Trips over the past 24-hours (midnight to 11:59pm)'].median(), 2)\n",
    "\n",
    "(tpm_mean, tpm_median, tpd_mean, tpd_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdf2285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/bitconnect_price.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert 'Volume' to numeric\n",
    "df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "\n",
    "# Drop rows where 'Volume' is NaN\n",
    "df = df.dropna(subset=['Volume'])\n",
    "\n",
    "# Calculate the Z-scores for 'Volume'\n",
    "z_scores = zscore(df['Volume'])\n",
    "\n",
    "# Identify outliers (absolute Z-scores > 3)\n",
    "outliers_count = len(z_scores[abs(z_scores) > 3])\n",
    "outliers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d313d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'Date'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[5], line 9\n",
       "      6 data = pd.read_csv(data_path)\n",
       "      8 # Parse the year from the date and clean the data\n",
       "----> 9 data['Year'] = pd.to_datetime(data['Date'], errors='coerce').dt.year\n",
       "     10 # Drop rows with invalid years or missing values\n",
       "     11 data = data.dropna(subset=['Year', 'Damage']).loc[data['Year'].between(2000, 2010)]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'Date'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the data\n",
    "data_path = './data/cost_data_with_errors.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Parse the year from the date and clean the data\n",
    "data['Year'] = pd.to_datetime(data['Date'], errors='coerce').dt.year\n",
    "# Drop rows with invalid years or missing values\n",
    "data = data.dropna(subset=['Year', 'Damage']).loc[data['Year'].between(2000, 2010)]\n",
    "data['Year'] = data['Year'].astype(int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab06c708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Year        Damage\n",
       "109  2010  1.520000e+09\n",
       "110  2010  7.500100e+04\n",
       "111  2010  1.360000e+06\n",
       "112  2010  7.500100e+04\n",
       "113  2010  1.000000e+06"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean and process the data\n",
    "data['Year'] = data['year']\n",
    "data['Damage'] = data['damage_USD']\n",
    "# Filter the data for the given range of years and drop missing values\n",
    "data = data.loc[data['Year'].between(2000, 2010)].dropna(subset=['Year', 'Damage']).copy()\n",
    "data['Year'] = data['Year'].astype(int)\n",
    "data = data[['Year', 'Damage']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9cf805f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year\n",
       "2000    6.826474e+07\n",
       "2001    5.294235e+04\n",
       "2002    1.764529e+08\n",
       "2003    3.006518e+08\n",
       "2004    3.823702e+09\n",
       "2005    5.830548e+09\n",
       "2006    5.046450e+07\n",
       "2007    2.015293e+08\n",
       "2008    2.909142e+09\n",
       "2009    5.291818e+06\n",
       "2010    3.519469e+08\n",
       "Name: Damage, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average damage for each year\n",
    "average_damage = data.groupby('Year')['Damage'].mean().round(2)\n",
    "average_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e697d710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4911174888415204)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a one-way ANOVA test considering all years from 2000 to 2010\n",
    "anova_result = stats.f_oneway(*[group[\"Damage\"].values for name, group in data.groupby(\"Year\")])\n",
    "\n",
    "anova_result.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0aae346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(101)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './data/cost_data_with_errors.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "null_entries_count = data['min_p'].isnull().sum()\n",
    "null_entries_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b83048b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'WINDSPEED'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[309], line 8\n",
       "      5 data = pd.read_csv('./data/baro_2015.csv')\n",
       "      7 # Extract the wind speeds column\n",
       "----> 8 wind_speeds = data['WINDSPEED'].dropna()\n",
       "     10 # Perform the Shapiro-Wilk test\n",
       "     11 stat, p_value = shapiro(wind_speeds)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'WINDSPEED'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/baro_2015.csv')\n",
    "\n",
    "# Extract the wind speeds column\n",
    "wind_speeds = data['WINDSPEED'].dropna()\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "stat, p_value = shapiro(wind_speeds)\n",
    "\n",
    "# Prepare the result\n",
    "result = {'shapiro_wilk_pvalue': round(p_value, 4), 'normal_distribution': 'no' if p_value <= 0.05 else 'yes'}\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ace2465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DATE TIME', ' WINDSPEED', ' DIR', ' GUSTS', ' AT', ' BARO', ' RELHUM',\n",
       "       ' VIS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first load the dataset and check its columns to ensure that the required 'WINDSPEED' column exists in the dataset.\n",
    "data = pd.read_csv('./data/baro_2015.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e3a720f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DATE TIME', 'WINDSPEED', 'DIR', 'GUSTS', 'AT', 'BARO', 'RELHUM',\n",
       "       'VIS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It appears that there might be an extra whitespace in the column name for 'WINDSPEED'. Let's strip the column names.\n",
    "data.columns = data.columns.str.strip()\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b87b1a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:579: UserWarning: scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 8142.\n",
       "  res = hypotest_fun_out(*samples, **kwds)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'shapiro_wilk_pvalue': np.float64(0.0), 'normal_distribution': 'no'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that the column names are properly stripped, let's extract the 'WINDSPEED' data and perform the Shapiro-Wilk test again.\n",
    "\n",
    "# Extract the wind speeds column\n",
    "wind_speeds = data['WINDSPEED'].dropna()\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "stat, p_value = shapiro(wind_speeds)\n",
    "\n",
    "# Prepare the result\n",
    "result = {'shapiro_wilk_pvalue': round(p_value, 4), 'normal_distribution': 'no' if p_value <= 0.05 else 'yes'}\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a00a63d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  LOCATION INDICATOR   SUBJECT MEASURE FREQUENCY  TIME  Value Flag Codes\n",
       "0      AUS    EDUEXP       TRY  PC_GDP         A  2012    1.6        NaN\n",
       "1      AUS    EDUEXP       TRY  PC_GDP         A  2013    1.7        NaN\n",
       "2      AUS    EDUEXP  PRY_NTRY  PC_GDP         A  2012    4.0        NaN\n",
       "3      AUS    EDUEXP  PRY_NTRY  PC_GDP         A  2013    3.9        NaN\n",
       "4      AUT    EDUEXP       TRY  PC_GDP         A  2012    1.8        NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/oecd_education_spending.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "16a8e228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    159.000000\n",
       "mean       2.579245\n",
       "std        1.222990\n",
       "min        0.400000\n",
       "25%        1.400000\n",
       "50%        2.500000\n",
       "75%        3.700000\n",
       "max        5.700000\n",
       "Name: Value, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the Value column to check its general statistics\n",
    "value_describe = data['Value'].describe()\n",
    "value_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35427540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate IQR and find outliers by applying the IQR rule\n",
    "q1 = value_describe['25%']\n",
    "q3 = value_describe['75%']\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Compute lower and upper bounds for outliers\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Identify rows that are outliers based on these bounds\n",
    "outliers = data[(data['Value'] < lower_bound) | (data['Value'] > upper_bound)].index.tolist()\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc7a545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   hotel_id                    hotel_name      city_name  star_rating  \\\n",
       "0     75737        Night Theater District  New York City          4.0   \n",
       "1     93401  Heritage Hotel New York City  New York City          2.5   \n",
       "2    224217     Clarion Hotel Park Avenue  New York City          2.5   \n",
       "3    488793             Solita Soho Hotel  New York City          3.0   \n",
       "4   1028569               Greenwich Hotel  New York City          5.0   \n",
       "\n",
       "   bubble_score  review_count hotel_type                       brand_name  \\\n",
       "0          40.0          2291      Hotel                      Night Hotel   \n",
       "1          35.0           968      Hotel                              NaN   \n",
       "2          35.0           462      Hotel                          Clarion   \n",
       "3          40.0           520      Hotel                Ascend Collection   \n",
       "4          45.0           582      Hotel  The Leading Hotels of the World   \n",
       "\n",
       "                      parent_brand_name  \n",
       "0                   Wyndham Hotel Group  \n",
       "1                                   NaN  \n",
       "2     Choice Hotels International, Inc.  \n",
       "3     Choice Hotels International, Inc.  \n",
       "4  The Leading Hotels of the World, Ltd  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/hotel_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4018b88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1013.53)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average number of reviews per hotel\n",
    "average_reviews = round(data['review_count'].mean(), 2)\n",
    "average_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5afe70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2fac853b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          DATE TIME   WINDSPEED   DIR   GUSTS    AT    BARO   RELHUM   VIS\n",
       "0  01/01/2015 00:00        2.72   288    5.25  27.7  1023.0      NaN   NaN\n",
       "1  01/01/2015 01:00        3.89   273    7.00  26.8  1022.7      NaN   NaN\n",
       "2  01/01/2015 02:00        4.86   268    6.41  27.0  1022.1      NaN   NaN\n",
       "3  01/01/2015 03:00        4.47   294    7.19  26.6  1021.4      NaN   NaN\n",
       "4  01/01/2015 04:00        4.08   283    7.19  25.9  1020.9      NaN   NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('./data/baro_2015.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac69b0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'DIR'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[3], line 2\n",
       "      1 # Filter data for wind direction (DIR) equal to 180 degrees\n",
       "----> 2 data_dir_180 = data[data['DIR'] == 180]\n",
       "      3 data_dir_180 = data_dir_180[['WINDSPEED', 'BARO']].dropna()\n",
       "      4 data_dir_180.head()\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'DIR'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter data for wind direction (DIR) equal to 180 degrees\n",
    "data_dir_180 = data[data['DIR'] == 180]\n",
    "data_dir_180 = data_dir_180[['WINDSPEED', 'BARO']].dropna()\n",
    "data_dir_180.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a08d508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      WINDSPEED    BARO\n",
       "408        8.75  1026.1\n",
       "1244       8.16  1025.9\n",
       "2313       1.94  1019.8\n",
       "2907       2.53  1014.9\n",
       "2990       3.11  1021.8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Filter data for wind direction (DIR) equal to 180 degrees\n",
    "data_dir_180 = data[data['DIR'] == 180]\n",
    "data_dir_180 = data_dir_180[['WINDSPEED', 'BARO']].dropna()\n",
    "data_dir_180.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e5db166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.0782570468194961), np.float64(0.6756151221656983))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Pearson correlation coefficient and p-value\n",
    "r, p_value = pearsonr(data_dir_180['WINDSPEED'], data_dir_180['BARO'])\n",
    "\n",
    "r, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a2851ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.79)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load Titanic dataset\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "# Drop missing values for the 'Fare' column\n",
    "data = data.dropna(subset=['Fare'])\n",
    "\n",
    "# Calculate the skewness using Pearson's moment coefficient of skewness\n",
    "skewness = data['Fare'].skew()\n",
    "# Round to 2 decimal places\n",
    "skewness_rounded = round(skewness, 2)\n",
    "skewness_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7bc3b88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  timestamp  num. busy overflows  num. calls answered  \\\n",
       "0  Apr 13  2017 12:00:00 AM                    0                    0   \n",
       "1  Apr 13  2017 12:15:00 AM                    0                    0   \n",
       "2  Apr 13  2017 12:30:00 AM                    0                    0   \n",
       "3  Apr 13  2017 12:45:00 AM                    0                    0   \n",
       "4   Apr 13  2017 1:00:00 AM                    0                    0   \n",
       "\n",
       "   num. calls abandoned   num. calls transferred   num. calls timed out   \\\n",
       "0                      0                        0                      0   \n",
       "1                      0                        0                      0   \n",
       "2                      0                        0                      0   \n",
       "3                      0                        0                      0   \n",
       "4                      0                        0                      0   \n",
       "\n",
       "   avg. num. agents talking   avg. num. agents staffed  avg. wait time   \\\n",
       "0                        0.0                          4        00:00:00   \n",
       "1                        0.0                          4        00:00:00   \n",
       "2                        0.0                          4        00:00:00   \n",
       "3                        0.0                          4        00:00:00   \n",
       "4                        0.0                          4        00:00:00   \n",
       "\n",
       "  avg. abandonment time  \n",
       "0              00:00:00  \n",
       "1              00:00:00  \n",
       "2              00:00:00  \n",
       "3              00:00:00  \n",
       "4              00:00:00  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data file\n",
    "data_file = './data/20170413_000000_group_statistics.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "# Display the first few rows of the data to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13e86408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'num. calls abandoned'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[413], line 2\n",
       "      1 # Check the column for null or NaN values\n",
       "----> 2 data['num. calls abandoned'].isna().sum()\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'num. calls abandoned'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the column for null or NaN values\n",
    "data['num. calls abandoned'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a98ef38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'num. busy overflows', 'num. calls answered',\n",
       "       'num. calls abandoned ', 'num. calls transferred ',\n",
       "       'num. calls timed out ', 'avg. num. agents talking ',\n",
       "       'avg. num. agents staffed ', 'avg. wait time ',\n",
       "       'avg. abandonment time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the columns for spaces or any hidden issues\n",
    "columns = data.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "211f6975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strip any extra spaces in column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Rechecking the column\n",
    "data['num. calls abandoned'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb1cdac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total number of calls that were abandoned by the callers\n",
    "\n",
    "# Use sum() function\n",
    "total_abandoned_calls = data['num. calls abandoned'].sum()\n",
    "total_abandoned_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "023749b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.7022238016671697e-21), 'Not a normal distribution')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/random_stock_data.csv')\n",
    "\n",
    "# Perform the Shapiro-Wilk test on the 'Close' column\n",
    "stat, p_value = shapiro(data['Close'])\n",
    "\n",
    "# Determine the result based on significance level alpha\n",
    "alpha = 0.05\n",
    "result = \"Normal distribution\" if p_value > alpha else \"Not a normal distribution\"\n",
    "\n",
    "# Print the results\n",
    "p_value, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "111aa38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(4349.27), np.float64(3599.77), np.float64(4113.51))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/YAHOO-BTC_USD_D.csv')\n",
    "mean_close = round(data['Close'].mean(), 2)\n",
    "median_close = round(data['Close'].median(), 2)\n",
    "std_close = round(data['Close'].std(), 2)\n",
    "mean_close, median_close, std_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "413ccaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32529.47, 36218.68, 4440, 478003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "# Load the data file\n",
    "data = pd.read_csv('./data/random_stock_data.csv')\n",
    "\n",
    "# Calculate summary statistics for the \"Volume\" column\n",
    "mean_volume = round(statistics.mean(data['Volume']), 2)\n",
    "std_volume = round(statistics.stdev(data['Volume']), 2)\n",
    "min_volume = round(min(data['Volume']), 2)\n",
    "max_volume = round(max(data['Volume']), 2)\n",
    "\n",
    "mean_volume, std_volume, min_volume, max_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2939c5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   temp  humidity  pressure    description          dt  \\\n",
       "0           0  32.18        54      1010  moderate rain  1437730851   \n",
       "1           1  32.37        62      1010  moderate rain  1437734494   \n",
       "2           2  32.79        75      1009  moderate rain  1437738046   \n",
       "3           3  32.75        79      1009  moderate rain  1437741580   \n",
       "4           4  32.72        70      1008  moderate rain  1437745188   \n",
       "\n",
       "   wind_speed  wind_deg     city                  day  dist  \n",
       "0        2.11   330.003  Ravenna  2015-07-24 11:40:51     8  \n",
       "1        2.60    20.000  Ravenna  2015-07-24 12:41:34     8  \n",
       "2        3.60    70.000  Ravenna  2015-07-24 13:40:46     8  \n",
       "3        5.10    70.000  Ravenna  2015-07-24 14:39:40     8  \n",
       "4        3.10    10.000  Ravenna  2015-07-24 15:39:48     8  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/ravenna_250715.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea4f33c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.3416476900396896),\n",
       " np.float64(0.10226448339851954),\n",
       " 'not significant')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Pearson correlation coefficient and p-value\n",
    "r_value, p_value = pearsonr(data['pressure'], data['wind_speed'])\n",
    "relationship_significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "\n",
    "r_value, p_value, relationship_significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9f3d5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(23.45), np.float64(22.75))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/auto-mpg.csv')\n",
    "mean_mpg = round(data['mpg'].mean(), 2)\n",
    "median_mpg = round(data['mpg'].median(), 2)\n",
    "mean_mpg, median_mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca310a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  0.9298      36.0  3.676162   1.100450      2664.0  3.994003     33.93   \n",
       "1  2.7006      17.0  4.499388   1.039780      3331.0  2.038556     32.79   \n",
       "2  5.0286      30.0  6.184375   1.068750       999.0  3.121875     34.89   \n",
       "3  3.9038      21.0  3.586357   0.982583      1486.0  2.156749     37.36   \n",
       "4  7.1754      52.0  7.429487   1.051282       779.0  2.496795     37.90   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -118.25             1.000  \n",
       "1    -117.03             1.663  \n",
       "2    -120.43             1.580  \n",
       "3    -122.02             2.438  \n",
       "4    -122.28             3.629  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/my_test_01.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "540f77f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Replace missing values in MedInc column with the mean value\n",
    "if data['MedInc'].isnull().any():\n",
    "    medinc_mean = data['MedInc'].mean()\n",
    "    data['MedInc'].fillna(medinc_mean, inplace=True)\n",
    "\n",
    "# Check if missing values are resolved\n",
    "data['MedInc'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8a88a934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   AveOccup\n",
       "0  1.527273\n",
       "1 -1.245173\n",
       "2  0.290764\n",
       "3 -1.077598\n",
       "4 -0.595479"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Standardize the AveOccup column using z-scores\n",
    "scaler = StandardScaler()\n",
    "data['AveOccup'] = scaler.fit_transform(data[['AveOccup']])\n",
    "data[['AveOccup']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9bba9e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   RoomsPerPerson\n",
       "0        0.001380\n",
       "1        0.001351\n",
       "2        0.006191\n",
       "3        0.002413\n",
       "4        0.009537"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Create a new feature \"RoomsPerPerson\"\n",
    "data['RoomsPerPerson'] = data['AveRooms'] / data['Population']\n",
    "data[['RoomsPerPerson']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "21937f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0382)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Calculate the Pearson correlation coefficient between MedianHouseValue and RoomsPerPerson\n",
    "correlation_coefficient = np.corrcoef(data['MedianHouseValue'], data['RoomsPerPerson'])[0, 1]\n",
    "correlation_coefficient = round(correlation_coefficient, 4)\n",
    "correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "58127059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.1226), np.float64(1.2186))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Calculate mean and standard deviation of MedianHouseValue\n",
    "mean_MedianHouseValue = round(np.mean(data['MedianHouseValue']), 4)\n",
    "stddev_MedianHouseValue = round(np.std(data['MedianHouseValue']), 4)\n",
    "mean_MedianHouseValue, stddev_MedianHouseValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6b5ef47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   temp  humidity  pressure    description          dt  \\\n",
       "0           0  32.18        54      1010  moderate rain  1437730851   \n",
       "1           1  32.37        62      1010  moderate rain  1437734494   \n",
       "2           2  32.79        75      1009  moderate rain  1437738046   \n",
       "3           3  32.75        79      1009  moderate rain  1437741580   \n",
       "4           4  32.72        70      1008  moderate rain  1437745188   \n",
       "\n",
       "   wind_speed  wind_deg     city                  day  dist  \n",
       "0        2.11   330.003  Ravenna  2015-07-24 11:40:51     8  \n",
       "1        2.60    20.000  Ravenna  2015-07-24 12:41:34     8  \n",
       "2        3.60    70.000  Ravenna  2015-07-24 13:40:46     8  \n",
       "3        5.10    70.000  Ravenna  2015-07-24 14:39:40     8  \n",
       "4        3.10    10.000  Ravenna  2015-07-24 15:39:48     8  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the file\n",
    "file_path = './data/ravenna_250715.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to check its structure and column names\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "569a8cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Perform outlier detection on the 'wind_speed' column using Z-scores.\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate Z-scores for the 'wind_speed' column\n",
    "data['z_score'] = zscore(data['wind_speed'])\n",
    "\n",
    "# Identify outliers with Z-score greater than 3 or less than -3\n",
    "outliers = data[(data['z_score'] > 3) | (data['z_score'] < -3)]\n",
    "number_of_outliers = len(outliers)\n",
    "\n",
    "# Display the number of outliers and their values\n",
    "number_of_outliers, outliers['wind_speed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0c7882ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.29), np.float64(1.15))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Remove the outliers and calculate mean and standard deviation\n",
    "# Remove outliers from the dataset\n",
    "cleaned_data = data[(data['z_score'] <= 3) & (data['z_score'] >= -3)]\n",
    "\n",
    "# Calculate the mean and standard deviation of the 'wind_speed' column after outlier removal\n",
    "mean_wind_speed = round(cleaned_data['wind_speed'].mean(), 2)\n",
    "std_deviation_wind_speed = round(cleaned_data['wind_speed'].std(), 2)\n",
    "\n",
    "mean_wind_speed, std_deviation_wind_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "389fb65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.1226), np.float64(1.221))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/my_test_01.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Calculate mean and standard deviation for MedianHouseValue column\n",
    "mean_value = df['MedianHouseValue'].mean()\n",
    "std_dev = df['MedianHouseValue'].std()\n",
    "\n",
    "# Output the results\n",
    "round(mean_value, 4), round(std_dev, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "521fef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary library\\nimport pandas as pd\\nfrom scipy.stats import normaltest\\n\\n# Load the dataset\\ndata = pd.read_csv('./data/gapminder_cleaned.csv')\\n\\n# Perform normality test\\nstat, p_value = normaltest(data['Gdppercap'])\\n\\n# Determine if the distribution is normal\\ndistribution_type = \"normal\" if p_value > 0.05 else \"not normal\"\\nresult = f\"@distribution_normality[{distribution_type}]\"\\nresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0d8a9ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting with the correct column name\\nimport pandas as pd\\nfrom scipy.stats import normaltest\\n\\ndata = pd.read_csv('./data/gapminder_cleaned.csv')\\nstat, p_value = normaltest(data['gdppercap'])\\ndistribution_type = \"normal\" if p_value > 0.05 else \"not normal\"\\nresult = f\"@distribution_normality[{distribution_type}]\"\\nresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "68c3e96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset and examine\n",
    "file_path = './data/auto-mpg.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b7d1af81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg             0\n",
       "cylinders       0\n",
       "displacement    0\n",
       "horsepower      0\n",
       "weight          0\n",
       "acceleration    0\n",
       "modelyear       0\n",
       "origin          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0f31fc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refine the dataset to include only mpg, weight, and acceleration columns\n",
    "# and handle any potential invalid values like '?'\n",
    "data = data[['mpg', 'weight', 'acceleration']]\n",
    "data = data.replace('?', pd.NA)\n",
    "data = data.dropna()\n",
    "data['mpg'] = pd.to_numeric(data['mpg'])\n",
    "data['weight'] = pd.to_numeric(data['weight'])\n",
    "data['acceleration'] = pd.to_numeric(data['acceleration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "47ae104f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into features and target\n",
    "X = data[['weight', 'acceleration']]\n",
    "y = data['mpg']\n",
    "\n",
    "# Use an 8:2 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5a7a9d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.66"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions and compute Mean Squared Error (MSE)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1875cea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and examine its structure\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/auto-mpg.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "da12ec76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(15.49), np.float64(2.68))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform outlier detection and removal using Z-score method\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate Z-scores for acceleration column\n",
    "data['z_score'] = zscore(data['acceleration'])\n",
    "\n",
    "# Identify and remove outliers\n",
    "cleaned_data = data[(data['z_score'] >= -3) & (data['z_score'] <= 3)].copy()\n",
    "\n",
    "# Recalculate metrics\n",
    "mean_acceleration = round(cleaned_data['acceleration'].mean(), 2)\n",
    "std_acceleration = round(cleaned_data['acceleration'].std(ddof=0), 2)\n",
    "\n",
    "mean_acceleration, std_acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "583a29d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "3           4  148.924   9504     681      3   36         11  Female      No   \n",
       "4           5   55.882   4897     357      2   68         16    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  \n",
       "0     Yes  Caucasian      333  \n",
       "1     Yes      Asian      903  \n",
       "2      No      Asian      580  \n",
       "3      No      Asian      964  \n",
       "4     Yes  Caucasian      331  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Credit dataset\n",
    "file_path = './data/Credit.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3720fa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "3           4  148.924   9504     681      3   36         11  Female      No   \n",
       "4           5   55.882   4897     357      2   68         16    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  Ratio  \n",
       "0     Yes  Caucasian      333   0.09  \n",
       "1     Yes      Asian      903   0.14  \n",
       "2      No      Asian      580   0.08  \n",
       "3      No      Asian      964   0.10  \n",
       "4     Yes  Caucasian      331   0.07  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the new feature `Balance/Limit` as `Ratio`\n",
    "data['Ratio'] = data.apply(lambda row: round(row['Balance'] / row['Limit'], 2) if row['Limit'] > 0 else 0, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "780df591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "3           4  148.924   9504     681      3   36         11  Female      No   \n",
       "4           5   55.882   4897     357      2   68         16    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  \n",
       "0     Yes  Caucasian      333  \n",
       "1     Yes      Asian      903  \n",
       "2      No      Asian      580  \n",
       "3      No      Asian      964  \n",
       "4     Yes  Caucasian      331  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/Credit.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a2fbb789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.86)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Pearson correlation coefficient between 'Limit' and 'Balance'\n",
    "correlation = data['Limit'].corr(data['Balance'])\n",
    "round(correlation, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4e2ee0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Serial                       Industry and class of worker  Mar.2019  \\\n",
       "0       0                        Total, 16 years and over(1)    6382.0   \n",
       "1       1  Nonagricultural private wage and salary worker...    4869.0   \n",
       "2       2      Mining, quarrying, and oil and gas extraction      26.0   \n",
       "3       3                                       Construction     490.0   \n",
       "4       4                                      Manufacturing     475.0   \n",
       "\n",
       "   Mar.2020 Total Mar.2019 Total Mar.2020 Men Mar.2019 Men Mar.2020  \\\n",
       "0    7370.0            3.9            4.5          4.3          4.8   \n",
       "1    5964.0            3.8            4.6          3.9          4.9   \n",
       "2      52.0            3.6            6.2            3          6.4   \n",
       "3     658.0            5.2            6.9          5.5          7.3   \n",
       "4     636.0            2.9            4.2          2.8          4.2   \n",
       "\n",
       "  Women Mar.2019 Women Mar.2020  \n",
       "0            3.5            4.2  \n",
       "1            3.6            4.3  \n",
       "2            7.3            4.6  \n",
       "3            2.9            3.3  \n",
       "4            3.2            4.1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/unemployement_industry.csv')\n",
    "\n",
    "# Display the first few rows of the dataset for examination\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4cdba410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11439.6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the dataset by selecting relevant columns and renaming them\n",
    "# We are using 'Mar.2019' to predict 'Mar.2020'\n",
    "data_selected = data[['Mar.2019', 'Mar.2020']]\n",
    "data_selected = data_selected.dropna()\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data_selected[['Mar.2019']]\n",
    "y = data_selected['Mar.2020']\n",
    "\n",
    "# Split the dataset into training and testing sets with a 70-30 split and random seed of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b044ca53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   votes_dem  votes_gop  total_votes   per_dem  per_gop    diff  \\\n",
       "0    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "1    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "2    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "3    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "4    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "\n",
       "  per_point_diff state_abbr county_name  combined_fips  \n",
       "0         15.17%         AK      Alaska           2013  \n",
       "1         15.17%         AK      Alaska           2016  \n",
       "2         15.17%         AK      Alaska           2020  \n",
       "3         15.17%         AK      Alaska           2050  \n",
       "4         15.17%         AK      Alaska           2060  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries and load the dataset.\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "file_path = './data/election2016.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "19aa6276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not normal'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the 'per_dem' column and remove any missing values\n",
    "per_dem = data['per_dem'].dropna()\n",
    "\n",
    "# Conduct the Shapiro-Wilk test\n",
    "stat, p_value = shapiro(per_dem)\n",
    "\n",
    "# Determine the normality status\n",
    "alpha = 0.05\n",
    "normality_status = \"normal\" if p_value >= alpha else \"not normal\"\n",
    "normality_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "62d94bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_32829/4056911145.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
       "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
       "\n",
       "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
       "\n",
       "\n",
       "  data['Embarked'].fillna(mode_embarked, inplace=True)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(314),\n",
       " np.int64(577),\n",
       " np.float64(0.0),\n",
       " np.float64(1.0),\n",
       " np.float64(0.0629))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "# Handle missing values for the 'Embarked' column by filling with mode value\n",
    "mode_embarked = data['Embarked'].mode()[0]\n",
    "data['Embarked'].fillna(mode_embarked, inplace=True)\n",
    "\n",
    "# Normalize the 'Fare' column using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "data['Fare_scaled'] = scaler.fit_transform(data[['Fare']])\n",
    "\n",
    "# Create Label Encoding for 'Sex'\n",
    "encoder = LabelEncoder()\n",
    "data['Sex_encoded'] = encoder.fit_transform(data['Sex'])\n",
    "\n",
    "# Calculate required statistics\n",
    "sex_encoded_count = data['Sex_encoded'].value_counts()\n",
    "min_fare = round(data['Fare_scaled'].min(), 2)\n",
    "max_fare = round(data['Fare_scaled'].max(), 2)\n",
    "mean_fare = round(data['Fare_scaled'].mean(), 4)\n",
    "(sex_encoded_count[0], sex_encoded_count[1], min_fare, max_fare, mean_fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "18afea64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_32829/1874903606.py:11: SettingWithCopyWarning: \n",
       "A value is trying to be set on a copy of a slice from a DataFrame.\n",
       "Try using .loc[row_indexer,col_indexer] = value instead\n",
       "\n",
       "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
       "  data_cleaned['zscore_age'] = zscore(data_cleaned['Age'])\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load Titanic dataset\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Dropping rows where 'Age' is null\n",
    "data_cleaned = data.dropna(subset=['Age'])\n",
    "\n",
    "# Calculate Z-scores for the 'Age' column\n",
    "data_cleaned['zscore_age'] = zscore(data_cleaned['Age'])\n",
    "\n",
    "# Identify outliers\n",
    "outliers = data_cleaned[(data_cleaned['zscore_age'] > 3) | (data_cleaned['zscore_age'] < -3)]\n",
    "outliers_count = len(outliers)\n",
    "outliers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "60b40c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   salary_in_thousands_of_dollars  batting_average  on_base_percentage  \\\n",
       "0                            3300            0.272               0.302   \n",
       "1                            2600            0.269               0.335   \n",
       "2                            2500            0.249               0.337   \n",
       "3                            2475            0.260               0.292   \n",
       "4                            2313            0.273               0.346   \n",
       "\n",
       "   number_of_runs  number_of_hits  number_of_doubles  number_of_triples  \\\n",
       "0              69             153                 21                  4   \n",
       "1              58             111                 17                  2   \n",
       "2              54             115                 15                  1   \n",
       "3              59             128                 22                  7   \n",
       "4              87             169                 28                  5   \n",
       "\n",
       "   number_of_home_runs  number_of_runs_batted_in  number_of_walks  \\\n",
       "0                 31.0                       104               22   \n",
       "1                 18.0                        66               39   \n",
       "2                 17.0                        73               63   \n",
       "3                 12.0                        50               23   \n",
       "4                  8.0                        58               70   \n",
       "\n",
       "   number_of_strike_outs  number_of_stolen_bases  number_of_errors  \\\n",
       "0                   80.0                       4               3.0   \n",
       "1                   69.0                       0               3.0   \n",
       "2                  116.0                       6               5.0   \n",
       "3                   64.0                      21              21.0   \n",
       "4                   53.0                       3               8.0   \n",
       "\n",
       "   indicator_of_free_agency_eligibility  indicator_of_free_agent_in_1991_1992  \\\n",
       "0                                   1.0                                     0   \n",
       "1                                   1.0                                     1   \n",
       "2                                   1.0                                     0   \n",
       "3                                   0.0                                     0   \n",
       "4                                   0.0                                     0   \n",
       "\n",
       "   indicator_of_arbitration_eligibility  indicator_of_arbitration_in_1991_1992  \n",
       "0                                   0.0                                    0.0  \n",
       "1                                   0.0                                    0.0  \n",
       "2                                   0.0                                    0.0  \n",
       "3                                   1.0                                    0.0  \n",
       "4                                   1.0                                    0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/baseball_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0bb6d37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.258)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean batting average excluding missing values\n",
    "mean_batting_average = data['batting_average'].dropna().mean()\n",
    "\n",
    "# Round the result to three decimal places\n",
    "mean_batting_average = round(mean_batting_average, 3)\n",
    "\n",
    "mean_batting_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "856cd8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@correlation_coefficient[nan]\\n@p_value[nan]\\n@correlation[not correlated]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/ts-sc4-wi100000-sl25000-Qrob_Chr05.tree_table.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calculate Pearson correlation and p-value\n",
    "r_value, p_value = pearsonr(data['nsnps'], data['nsamplecov'])\n",
    "\n",
    "# Determine the correlation status\n",
    "correlation = \"correlated\" if p_value < 0.05 else \"not correlated\"\n",
    "\n",
    "# Format responses\n",
    "r_value = round(r_value, 2)\n",
    "p_value = round(p_value, 4)\n",
    "\n",
    "f\"@correlation_coefficient[{r_value}]\\n@p_value[{p_value}]\\n@correlation[{correlation}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ee11d9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "data_file = './data/titanic.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "stat, p_value = shapiro(data['Fare'].dropna())\n",
    "normality_test_result = p_value > 0.05\n",
    "normality_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "da8d171b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(21144.08), np.float64(19711.0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the data file\n",
    "data = pd.read_csv('./data/2014_q4.csv')\n",
    "\n",
    "# Calculating mean and median using built-in Python functions\n",
    "column_data = data['Trips over the past 24-hours (midnight to 11:59pm)']\n",
    "mean_value = round(column_data.mean(), 2)\n",
    "median_value = round(column_data.median(), 2)\n",
    "\n",
    "mean_value, median_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7500a2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date  Trips over the past 24-hours (midnight to 11:59pm)  \\\n",
       "0  10/1/2014                                              31197    \n",
       "1  10/2/2014                                              38286    \n",
       "2  10/3/2014                                              38956    \n",
       "3  10/4/2014                                              15088    \n",
       "4  10/5/2014                                              26247    \n",
       "\n",
       "   Cumulative trips (since launch):  \\\n",
       "0                          13296973   \n",
       "1                          13335259   \n",
       "2                          13374215   \n",
       "3                          13389303   \n",
       "4                          13415550   \n",
       "\n",
       "   Miles traveled today (midnight to 11:59 pm)  Miles traveled to date:  \\\n",
       "0                                        44612                 23121175   \n",
       "1                                        60639                 23181814   \n",
       "2                                        65739                 23247553   \n",
       "3                                        24254                 23271807   \n",
       "4                                        48930                 23320737   \n",
       "\n",
       "   Total Annual Memberships Sold  \\\n",
       "0                         124846   \n",
       "1                         124959   \n",
       "2                         125024   \n",
       "3                         125058   \n",
       "4                         125109   \n",
       "\n",
       "  Annual Member Sign-Ups (midnight to 11:59 pm)  \\\n",
       "0                                           112   \n",
       "1                                           113   \n",
       "2                                            65   \n",
       "3                                            34   \n",
       "4                                            51   \n",
       "\n",
       "  24-Hour Passes Purchased (midnight to 11:59 pm)  \\\n",
       "0                                             330   \n",
       "1                                             602   \n",
       "2                                            1276   \n",
       "3                                             617   \n",
       "4                                            1470   \n",
       "\n",
       "  7-Day Passes Purchased (midnight to 11:59 pm)  \n",
       "0                                            48  \n",
       "1                                            86  \n",
       "2                                           107  \n",
       "3                                            26  \n",
       "4                                            90  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset 2014_q4.csv\n",
    "data = pd.read_csv('./data/2014_q4.csv')\n",
    "\n",
    "# Display the first few rows to check the columns and ensure the data is loaded correctly\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1bdb5211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_32829/1511527726.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
       "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
       "\n",
       "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
       "\n",
       "\n",
       "  data['Trips per Membership'].replace([float('inf'), -float('inf')], 0, inplace=True)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.17), np.float64(0.16), np.float64(21144.08), np.float64(19711.0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature engineering\n",
    "# Adding a new column 'Trips per Membership'. This is equal to Trips over the past 24-hours divided by Total Annual Memberships Sold.\n",
    "data['Trips per Membership'] = data['Trips over the past 24-hours (midnight to 11:59pm)'] / data['Total Annual Memberships Sold']\n",
    "data['Trips per Membership'].replace([float('inf'), -float('inf')], 0, inplace=True)\n",
    "\n",
    "# Rounding off the new column to 2 decimal places\n",
    "data['Trips per Membership'] = data['Trips per Membership'].round(2)\n",
    "\n",
    "# Calculating the requested statistics\n",
    "trips_per_membership_mean = data['Trips per Membership'].mean().round(2)\n",
    "trips_per_membership_median = data['Trips per Membership'].median().round(2)\n",
    "\n",
    "# Calculating statistics for \"Trips over the past 24-hours\" column\n",
    "trips_per_day_mean = data['Trips over the past 24-hours (midnight to 11:59pm)'].mean().round(2)\n",
    "trips_per_day_median = data['Trips over the past 24-hours (midnight to 11:59pm)'].median().round(2)\n",
    "\n",
    "trips_per_membership_mean, trips_per_membership_median, trips_per_day_mean, trips_per_day_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2a5a1cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.6)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('./data/test_x.csv')\n",
    "\n",
    "# Create the FamilySize column\n",
    "data['FamilySize'] = data['IsAlone'] + 2\n",
    "\n",
    "# Calculate the average FamilySize rounded to 1 decimal place\n",
    "average_familysize = round(data['FamilySize'].mean(), 1)\n",
    "average_familysize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ae69b245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'wind_speed'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[241], line 9\n",
       "      6 data = pd.read_csv(file_path)\n",
       "      8 # Calculate Z-scores for the wind speed column\n",
       "----> 9 data['wind_speed_zscore'] = zscore(data['wind_speed'])\n",
       "     11 # Identify outliers with a Z-score threshold of 3\n",
       "     12 outliers = data[(data['wind_speed_zscore'] > 3) | (data['wind_speed_zscore'] < -3)]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'wind_speed'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/weather_train.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate Z-scores for the wind speed column\n",
    "data['wind_speed_zscore'] = zscore(data['wind_speed'])\n",
    "\n",
    "# Identify outliers with a Z-score threshold of 3\n",
    "outliers = data[(data['wind_speed_zscore'] > 3) | (data['wind_speed_zscore'] < -3)]\n",
    "outlier_count = outliers.shape[0]\n",
    "\n",
    "# Remove the outliers\n",
    "cleaned_data = data[(data['wind_speed_zscore'] <= 3) & (data['wind_speed_zscore'] >= -3)]\n",
    "\n",
    "# Drop the additional Z-score column for cleanliness\n",
    "cleaned_data = cleaned_data.drop(columns=['wind_speed_zscore'])\n",
    "outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2d078365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Rename the column for standard access\n",
    "data.rename(columns={'wind speed': 'wind_speed'}, inplace=True)\n",
    "\n",
    "# Calculate Z-scores for the wind speed column\n",
    "data['wind_speed_zscore'] = zscore(data['wind_speed'])\n",
    "\n",
    "# Identify outliers with a Z-score threshold of 3\n",
    "outliers = data[(data['wind_speed_zscore'] > 3) | (data['wind_speed_zscore'] < -3)]\n",
    "outlier_count = outliers.shape[0]\n",
    "\n",
    "# Remove the outliers\n",
    "cleaned_data = data[(data['wind_speed_zscore'] <= 3) & (data['wind_speed_zscore'] >= -3)]\n",
    "\n",
    "# Drop the additional Z-score column for cleanliness\n",
    "cleaned_data = cleaned_data.drop(columns=['wind_speed_zscore'])\n",
    "outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cceecac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.6596078626429694), 'positive')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_skewness(file_path, column_name):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Remove missing values from the specified column\n",
    "    cleaned_data = data[column_name].dropna()\n",
    "\n",
    "    # Calculate mean, median and standard deviation\n",
    "    mean_val = cleaned_data.mean()\n",
    "    median_val = cleaned_data.median()\n",
    "    std_val = cleaned_data.std()\n",
    "\n",
    "    # Calculate skewness using Pearson's First Coefficient of Skewness\n",
    "    skewness = 3 * (mean_val - median_val) / std_val\n",
    "\n",
    "    return skewness\n",
    "\n",
    "# File path and column name for the wind speed data\n",
    "file_path = \"./data/weather_train.csv\"\n",
    "column_name = \"wind speed\"\n",
    "\n",
    "# Calculate skewness\n",
    "skewness_value = calculate_skewness(file_path, column_name)\n",
    "\n",
    "# Determine the type of skewness if skewness is greater, less than or equal to 0\n",
    "if skewness_value > 0:\n",
    "    skewness_type = \"positive\"\n",
    "elif skewness_value < 0:\n",
    "    skewness_type = \"negative\"\n",
    "else:\n",
    "    skewness_type = \"symmetric\"\n",
    "\n",
    "(skewness_value, skewness_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a8408e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0    name   dates_active  max_storm_cat  max_sust_wind   min_p  \\\n",
       " 0           0  ARLENE  April19 21              1       43.44880   990.0   \n",
       " 1           1    BRET   June19 20              1       43.44880  1007.0   \n",
       " 2           2   CINDY   June20 23              1       52.13856   991.0   \n",
       " 3           3    FOUR     July5 7              0       26.06928  1009.0   \n",
       " 4           4     DON   July17 18              1       43.44880  1005.0   \n",
       " \n",
       "                                       areas_affected  damage_USD  deaths  \\\n",
       " 0                                                NaN         0.0     0.0   \n",
       " 1  Guyana, Venezuela, Trinidad and Tobago, Windwa...   3000000.0     2.0   \n",
       " 2  Honduras, Belize, Cayman Islands, Yucatn Peni...  25000000.0     2.0   \n",
       " 3                                                NaN         0.0     0.0   \n",
       " 4    Windward Islands, Barbados, Trinidad and Tobago         0.0     0.0   \n",
       " \n",
       "    year  damage_imputed  \n",
       " 0  2017               0  \n",
       " 1  2017               0  \n",
       " 2  2017               0  \n",
       " 3  2017               0  \n",
       " 4  2017               0  ,\n",
       " Unnamed: 0          0\n",
       " name                0\n",
       " dates_active        0\n",
       " max_storm_cat       0\n",
       " max_sust_wind      24\n",
       " min_p             101\n",
       " areas_affected    239\n",
       " damage_USD          0\n",
       " deaths              0\n",
       " year                0\n",
       " damage_imputed      0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data file\n",
    "data = pd.read_csv('./data/cost_data_with_errors.csv')\n",
    "\n",
    "# Display the first few rows and identify missing values\n",
    "data.head(), data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "23a5b120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_32829/3076965330.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
       "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
       "\n",
       "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
       "\n",
       "\n",
       "  data['max_sust_wind'].fillna(data['max_sust_wind'].mean(), inplace=True)\n",
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_32829/3076965330.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
       "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
       "\n",
       "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
       "\n",
       "\n",
       "  data['min_p'].fillna(data['min_p'].mean(), inplace=True)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing values in the relevant columns (max_sust_wind, min_p, damage_USD)\n",
    "data['max_sust_wind'].fillna(data['max_sust_wind'].mean(), inplace=True)\n",
    "data['min_p'].fillna(data['min_p'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f31814aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.18962970059766"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the features (damage_USD, min_p) and target variable (max_sust_wind)\n",
    "X = data[['damage_USD', 'min_p']]\n",
    "y = data['max_sust_wind']\n",
    "\n",
    "# Split the data into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "701cfc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'fare'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[358], line 8\n",
       "      5 data = pd.read_csv('./data/titanic.csv')\n",
       "      7 # Calculate skewness for 'fare' column, ignoring null values\n",
       "----> 8 fare_skewness = round(skew(data['fare'].dropna()), 2)\n",
       "      9 fare_skewness\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'fare'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Calculate skewness for 'fare' column, ignoring null values\n",
    "fare_skewness = round(skew(data['fare'].dropna()), 2)\n",
    "fare_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3a1388b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.78)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Calculate skewness for 'Fare' column, ignoring null values\n",
    "fare_skewness = round(skew(data['Fare'].dropna()), 2)\n",
    "fare_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fb2f8be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.58)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Read the Titanic dataset\n",
    "data_path = './data/titanic_test.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Drop rows with missing values in 'Pclass' and 'Fare'\n",
    "data_cleaned = data.dropna(subset=['Pclass', 'Fare'])\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation, _ = pearsonr(data_cleaned['Pclass'], data_cleaned['Fare'])\n",
    "round(correlation, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ae068676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[392], line 10\n",
       "      7 data = pd.read_csv('./data/titanic.csv')\n",
       "      9 # Select relevant features (Age, Gender, Fare) and target variable (Survived). Assuming \"Gender\" is the column name for gender.\n",
       "---> 10 data = data[['Age', 'Gender', 'Fare', 'Survived']]\n",
       "     12 # Drop rows with missing data\n",
       "     13 data = data.dropna()\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4119, in DataFrame.__getitem__(self, key)\n",
       "   4117     if is_iterator(key):\n",
       "   4118         key = list(key)\n",
       "-> 4119     indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n",
       "   4121 # take() does not accept boolean indexers\n",
       "   4122 if getattr(indexer, \"dtype\", None) == bool:\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:6212, in Index._get_indexer_strict(self, key, axis_name)\n",
       "   6209 else:\n",
       "   6210     keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)\n",
       "-> 6212 self._raise_if_missing(keyarr, indexer, axis_name)\n",
       "   6214 keyarr = self.take(indexer)\n",
       "   6215 if isinstance(key, Index):\n",
       "   6216     # GH 42790 - Preserve name from an Index\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:6264, in Index._raise_if_missing(self, key, indexer, axis_name)\n",
       "   6261     raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n",
       "   6263 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n",
       "-> 6264 raise KeyError(f\"{not_found} not in index\")\n",
       "\n",
       "KeyError: \"['Gender'] not in index\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Select relevant features (Age, Gender, Fare) and target variable (Survived). Assuming \"Gender\" is the column name for gender.\n",
    "data = data[['Age', 'Gender', 'Fare', 'Survived']]\n",
    "\n",
    "# Drop rows with missing data\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert categorical 'Gender' to numeric (0 for male, 1 for female)\n",
    "data['Gender'] = data['Gender'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data[['Age', 'Gender', 'Fare']]\n",
    "y = data['Survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1b413d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7627906976744186"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Select relevant features (Age, Sex, Fare) and target variable (Survived)\n",
    "data = data[['Age', 'Sex', 'Fare', 'Survived']]\n",
    "\n",
    "# Drop rows with missing data\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert categorical 'Sex' to numeric (0 for male, 1 for female)\n",
    "data['Gender'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data = data.drop(columns=['Sex'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data[['Age', 'Gender', 'Fare']]\n",
    "y = data['Survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "66206b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date     AAPL.O  MSFT.O  INTC.O  AMZN.O    GS.N     SPY     .SPX  \\\n",
       "0  2010-01-01        NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n",
       "1  2010-01-04  30.572827  30.950   20.88  133.90  173.08  113.33  1132.99   \n",
       "2  2010-01-05  30.625684  30.960   20.87  134.69  176.14  113.63  1136.52   \n",
       "3  2010-01-06  30.138541  30.770   20.80  132.25  174.26  113.71  1137.14   \n",
       "4  2010-01-07  30.082827  30.452   20.60  130.00  177.67  114.19  1141.69   \n",
       "\n",
       "    .VIX    EUR=     XAU=    GDX     GLD  \n",
       "0    NaN  1.4323  1096.35    NaN     NaN  \n",
       "1  20.04  1.4411  1120.00  47.71  109.80  \n",
       "2  19.35  1.4368  1118.65  48.17  109.70  \n",
       "3  19.16  1.4412  1138.50  49.34  111.51  \n",
       "4  19.06  1.4318  1131.90  49.10  110.82  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/tr_eikon_eod_data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e6bbe69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date  MSFT.O     SPY   .VIX\n",
       "1  2010-01-04  30.950  113.33  20.04\n",
       "2  2010-01-05  30.960  113.63  19.35\n",
       "3  2010-01-06  30.770  113.71  19.16\n",
       "4  2010-01-07  30.452  114.19  19.06\n",
       "5  2010-01-08  30.660  114.57  18.13"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting relevant columns\n",
    "selected_columns = ['Date', 'MSFT.O', 'SPY', '.VIX']\n",
    "data = data[selected_columns]\n",
    "\n",
    "# Dropping missing values\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fc6a2e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date    MSFT.O       SPY      .VIX\n",
       "1  2010-01-04  0.099899  0.060364  0.280494\n",
       "2  2010-01-05  0.100025  0.061992  0.262738\n",
       "3  2010-01-06  0.097635  0.062425  0.257849\n",
       "4  2010-01-07  0.093634  0.065029  0.255275\n",
       "5  2010-01-08  0.096251  0.067090  0.231343"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data[['MSFT.O', 'SPY', '.VIX']] = scaler.fit_transform(data[['MSFT.O', 'SPY', '.VIX']])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7c3e448c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          MSFT.O       SPY      .VIX\n",
       "MSFT.O  1.000000  0.937022 -0.431681\n",
       "SPY     0.937022  1.000000 -0.576503\n",
       ".VIX   -0.431681 -0.576503  1.000000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the correlation matrix\n",
    "correlation_matrix = data[['MSFT.O', 'SPY', '.VIX']].corr(method='pearson')\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "57c0da80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_32829/3641311067.py in ?()\n",
       "      4 file_path = './data/20170413_000000_group_statistics.csv'\n",
       "      5 df = pd.read_csv(file_path)\n",
       "      6 \n",
       "      7 # Data Cleaning: Remove rows with NaN or null values in the 'num. calls abandoned' column\n",
       "----> 8 cleaned_df = df.dropna(subset=['num. calls abandoned'])\n",
       "      9 \n",
       "     10 # Calculate the total number of abandoned calls\n",
       "     11 total_abandoned_calls = cleaned_df['num. calls abandoned'].sum()\n",
       "\n",
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py in ?(self, axis, how, thresh, subset, inplace, ignore_index)\n",
       "   6688             ax = self._get_axis(agg_axis)\n",
       "   6689             indices = ax.get_indexer_for(subset)\n",
       "   6690             check = indices == -1\n",
       "   6691             if check.any():\n",
       "-> 6692                 raise KeyError(np.array(subset)[check].tolist())\n",
       "   6693             agg_obj = self.take(indices, axis=agg_axis)\n",
       "   6694 \n",
       "   6695         if thresh is not lib.no_default:\n",
       "\n",
       "KeyError: ['num. calls abandoned']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = './data/20170413_000000_group_statistics.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Data Cleaning: Remove rows with NaN or null values in the 'num. calls abandoned' column\n",
    "cleaned_df = df.dropna(subset=['num. calls abandoned'])\n",
    "\n",
    "# Calculate the total number of abandoned calls\n",
    "total_abandoned_calls = cleaned_df['num. calls abandoned'].sum()\n",
    "total_abandoned_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "59c87225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming the column for easier access (it might contain leading/trailing spaces)\n",
    "df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "# Data Cleaning: Remove rows with NaN or null values in the 'num. calls abandoned' column\n",
    "cleaned_df = df.dropna(subset=['num. calls abandoned'])\n",
    "\n",
    "# Calculate the total number of abandoned calls\n",
    "total_abandoned_calls = cleaned_df['num. calls abandoned'].sum()\n",
    "total_abandoned_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6153e9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@num_of_outliers[ColumnNotFound]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/20170413_000000_group_statistics.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Detect outliers in the average wait time using Z-score method\n",
    "if 'average_wait_time' in df.columns:\n",
    "    # Calculate Z-scores\n",
    "    df['z_score'] = zscore(df['average_wait_time'])\n",
    "    # Count the number of outliers\n",
    "    outliers_count = (df['z_score'].abs() > 3).sum()\n",
    "    result = f\"@num_of_outliers[{outliers_count}]\"\n",
    "else:\n",
    "    result = \"@num_of_outliers[ColumnNotFound]\"\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b192ba63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(95.1219512195122)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./data/Current_Logan.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate missing value percentage for 'Unnamed: 8' column\n",
    "missing_percentage = df['Unnamed: 8'].isna().mean() * 100\n",
    "\n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ac25c123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the data file\n",
    "data_path = './data/auto-mpg.csv'\n",
    "motor_data = pd.read_csv(data_path)\n",
    "motor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6a14344d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "297  27.2          4         141.0        71.0  3190.0          24.8   \n",
       "388  44.0          4          97.0        52.0  2130.0          24.6   \n",
       "\n",
       "     modelyear  origin  acceleration_zscore  \n",
       "297         79       2             3.360262  \n",
       "388         82       2             3.287676  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Z-scores for the 'acceleration' column\n",
    "motor_data['acceleration_zscore'] = zscore(motor_data['acceleration'])\n",
    "\n",
    "# Identify outliers based on Z scores being outside the range [-3, 3]\n",
    "outliers = motor_data[(motor_data['acceleration_zscore'] < -3) | (motor_data['acceleration_zscore'] > 3)]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ba5a8ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(15.49), np.float64(2.68))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove outliers from the dataset\n",
    "cleaned_motor_data = motor_data[~((motor_data['acceleration_zscore'] < -3) | (motor_data['acceleration_zscore'] > 3))]\n",
    "\n",
    "# Recalculate mean and population standard deviation for 'acceleration' column\n",
    "mean_acceleration = round(cleaned_motor_data['acceleration'].mean(), 2)\n",
    "std_acceleration = round(cleaned_motor_data['acceleration'].std(ddof=0), 2)\n",
    "mean_acceleration, std_acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "47df5c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './data/auto-mpg.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "28e4a18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "displacement\n",
       "97.0     21\n",
       "350.0    18\n",
       "98.0     17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze unique values of 'displacement' and their frequencies\n",
    "displacement_counts = data['displacement'].value_counts()\n",
    "displacement_counts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c2c7bb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{97.0: (np.float64(28.73), np.float64(28.0)),\n",
       " 350.0: (np.float64(15.03), np.float64(13.5)),\n",
       " 98.0: (np.float64(31.42), np.float64(30.5))}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the three most common displacement values\n",
    "top_displacements = displacement_counts.head(3).index\n",
    "\n",
    "# Filtering the dataset for each displacement and computing mean and median for 'mpg'\n",
    "results = {}\n",
    "for displacement in top_displacements:\n",
    "    filtered_data = data[data['displacement'] == displacement]\n",
    "    mean_mpg = round(filtered_data['mpg'].mean(), 2)\n",
    "    median_mpg = round(filtered_data['mpg'].median(), 2)\n",
    "    results[displacement] = (mean_mpg, median_mpg)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cb474375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'Pop'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[4], line 10\n",
       "      7 data = pd.read_csv(data_path)\n",
       "      9 # Calculate the Pearson correlation coefficient between 'Pop' and 'Gdppercap'\n",
       "---> 10 correlation, p_value = pearsonr(data['Pop'], data['Gdppercap'])\n",
       "     12 # Display the results\n",
       "     13 correlation, p_value\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'Pop'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/gapminder_cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient between 'Pop' and 'Gdppercap'\n",
    "correlation, p_value = pearsonr(data['Pop'], data['Gdppercap'])\n",
    "\n",
    "# Display the results\n",
    "correlation, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e4d29000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.02559957513348601), np.float64(0.2909060045965562))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset columns use lowercase. Updating column references and recalculating using 'pop' and 'gdppercap'.\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/gapminder_cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient between 'pop' and 'gdppercap'\n",
    "correlation, p_value = pearsonr(data['pop'], data['gdppercap'])\n",
    "\n",
    "# Display the results\n",
    "correlation, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3d42a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 392 entries, 0 to 391\n",
       "Data columns (total 8 columns):\n",
       " #   Column        Non-Null Count  Dtype  \n",
       "---  ------        --------------  -----  \n",
       " 0   mpg           392 non-null    float64\n",
       " 1   cylinders     392 non-null    int64  \n",
       " 2   displacement  392 non-null    float64\n",
       " 3   horsepower    392 non-null    float64\n",
       " 4   weight        392 non-null    float64\n",
       " 5   acceleration  392 non-null    float64\n",
       " 6   modelyear     392 non-null    int64  \n",
       " 7   origin        392 non-null    int64  \n",
       "dtypes: float64(5), int64(3)\n",
       "memory usage: 24.6 KB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       " 0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       " 1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       " 2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       " 3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       " 4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       " \n",
       "    origin  \n",
       " 0       1  \n",
       " 1       1  \n",
       " 2       1  \n",
       " 3       1  \n",
       " 4       1  ,\n",
       " None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the auto-mpg.csv file and necessary libraries.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path = './data/auto-mpg.csv'\n",
    "auto_mpg = pd.read_csv(data_path)\n",
    "\n",
    "# Show the first few rows and information for validation\n",
    "auto_mpg.head(), auto_mpg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fccb63ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(mpg               16.0\n",
       " cylinders          8.0\n",
       " displacement     400.0\n",
       " horsepower       230.0\n",
       " weight          4278.0\n",
       " acceleration       9.5\n",
       " modelyear         73.0\n",
       " origin             1.0\n",
       " Name: 115, dtype: float64,\n",
       " 73)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the vehicle with the maximum horsepower and its model year\n",
    "max_horsepower_vehicle = auto_mpg.loc[auto_mpg['horsepower'].idxmax()]\n",
    "max_horsepower_vehicle_modelyear = max_horsepower_vehicle['modelyear']\n",
    "\n",
    "max_horsepower_vehicle, int(max_horsepower_vehicle_modelyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "721aac04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(130.48), np.float64(45.83))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate average horsepower and population standard deviation for the same model year\n",
    "data_same_year = auto_mpg[auto_mpg['modelyear'] == max_horsepower_vehicle_modelyear]\n",
    "\n",
    "# Calculate the mean and population standard deviation\n",
    "same_year_avg_hp = round(data_same_year['horsepower'].mean(), 2)\n",
    "same_year_std_hp = round(data_same_year['horsepower'].std(ddof=0), 2)\n",
    "\n",
    "same_year_avg_hp, same_year_std_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6d993689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the rounded accuracy score\\naccuracy_rounded = round(accuracy, 2)\\nprint(f'@prediction_accuracy[{accuracy_rounded}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ccde8fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the dataset for linear regression\\n# We will use 'Mar.2019' as the independent variable and 'Mar.2020 Total' as the dependent variable\\nX = data[['Mar.2019']]\\ny = data['Mar.2020 Total']\\n\\n# Split the dataset into training and testing sets with a 70-30 split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\n# Create and train the linear regression model\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions on the testing set\\ny_pred = model.predict(X_test)\\n\\n# Calculate Mean Squared Error (MSE)\\nmse = mean_squared_error(y_test, y_pred)\\nmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8025f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue from the previous code to ensure MSE calculation is defined and printed correctly\\nimport numpy as np\\n\\n# Calculate Mean Squared Error (MSE) again to confirm\\nmse = mean_squared_error(y_test, y_pred)\\n\\n# Display MSE with two decimal places\\nrounded_mse = round(mse, 2)\\nprint(f'@Mean_Squared_Error[{rounded_mse}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ece0d10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final output of MSE\\nmse_final = mean_squared_error(y_test, y_pred)\\nprint(f'@Mean_Squared_Error[{round(mse_final, 2)}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "557c80c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) at the final step\\nmse_final = mean_squared_error(y_test, y_pred)\\n\\n# Print the final result in the required format\\nrounded_mse_final = round(mse_final, 2)\\nprint(f'@Mean_Squared_Error[{rounded_mse_final}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "81661df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the entire code block runs successfully to determine MSE value\\nX = data[['Mar.2019']]\\ny = data['Mar.2020 Total']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\ny_pred = model.predict(X_test)\\nmse_final = mean_squared_error(y_test, y_pred)\\nrounded_mse_final = round(mse_final, 2)\\nprint(f'@Mean_Squared_Error[{rounded_mse_final}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d4a1b2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(34.65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "\n",
    "mean_fare = round(data['Fare'].mean(), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8a55c9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary functions and library for the analysis\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/GODREJIND.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Perform Shapiro-Wilk test on the \"Total Traded Quantity\" column\n",
    "alpha = 0.05  # significance level\n",
    "test_statistic, p_value = shapiro(df['Total Traded Quantity'])\n",
    "\n",
    "# Determine normality\n",
    "is_normal = \"yes\" if p_value > alpha else \"no\"\n",
    "is_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ad0ab922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "252055e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Serial                       Industry and class of worker  Mar.2019  \\\n",
       "0       0                        Total, 16 years and over(1)    6382.0   \n",
       "1       1  Nonagricultural private wage and salary worker...    4869.0   \n",
       "2       2      Mining, quarrying, and oil and gas extraction      26.0   \n",
       "3       3                                       Construction     490.0   \n",
       "4       4                                      Manufacturing     475.0   \n",
       "\n",
       "   Mar.2020 Total Mar.2019 Total Mar.2020 Men Mar.2019 Men Mar.2020  \\\n",
       "0    7370.0            3.9            4.5          4.3          4.8   \n",
       "1    5964.0            3.8            4.6          3.9          4.9   \n",
       "2      52.0            3.6            6.2            3          6.4   \n",
       "3     658.0            5.2            6.9          5.5          7.3   \n",
       "4     636.0            2.9            4.2          2.8          4.2   \n",
       "\n",
       "  Women Mar.2019 Women Mar.2020  \n",
       "0            3.5            4.2  \n",
       "1            3.6            4.3  \n",
       "2            7.3            4.6  \n",
       "3            2.9            3.3  \n",
       "4            3.2            4.1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('./data/unemployement_industry.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0ede5650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "ValueError                                Traceback (most recent call last)\n",
       "Cell In[21], line 10\n",
       "      8 # Train a simple Linear Regression model\n",
       "      9 model = LinearRegression()\n",
       "---> 10 model.fit(X_train, y_train)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/base.py:1365, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\n",
       "   1358     estimator._validate_params()\n",
       "   1360 with config_context(\n",
       "   1361     skip_parameter_validation=(\n",
       "   1362         prefer_skip_nested_validation or global_skip_validation\n",
       "   1363     )\n",
       "   1364 ):\n",
       "-> 1365     return fit_method(estimator, *args, **kwargs)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/linear_model/_base.py:618, in LinearRegression.fit(self, X, y, sample_weight)\n",
       "    614 n_jobs_ = self.n_jobs\n",
       "    616 accept_sparse = False if self.positive else [\"csr\", \"csc\", \"coo\"]\n",
       "--> 618 X, y = validate_data(\n",
       "    619     self,\n",
       "    620     X,\n",
       "    621     y,\n",
       "    622     accept_sparse=accept_sparse,\n",
       "    623     y_numeric=True,\n",
       "    624     multi_output=True,\n",
       "    625     force_writeable=True,\n",
       "    626 )\n",
       "    628 has_sw = sample_weight is not None\n",
       "    629 if has_sw:\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:2971, in validate_data(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\n",
       "   2969         y = check_array(y, input_name=\"y\", **check_y_params)\n",
       "   2970     else:\n",
       "-> 2971         X, y = check_X_y(X, y, **check_params)\n",
       "   2972     out = X, y\n",
       "   2974 if not no_val_X and check_params.get(\"ensure_2d\", True):\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:1368, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n",
       "   1362     raise ValueError(\n",
       "   1363         f\"{estimator_name} requires y to be passed, but the target y is None\"\n",
       "   1364     )\n",
       "   1366 ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n",
       "-> 1368 X = check_array(\n",
       "   1369     X,\n",
       "   1370     accept_sparse=accept_sparse,\n",
       "   1371     accept_large_sparse=accept_large_sparse,\n",
       "   1372     dtype=dtype,\n",
       "   1373     order=order,\n",
       "   1374     copy=copy,\n",
       "   1375     force_writeable=force_writeable,\n",
       "   1376     ensure_all_finite=ensure_all_finite,\n",
       "   1377     ensure_2d=ensure_2d,\n",
       "   1378     allow_nd=allow_nd,\n",
       "   1379     ensure_min_samples=ensure_min_samples,\n",
       "   1380     ensure_min_features=ensure_min_features,\n",
       "   1381     estimator=estimator,\n",
       "   1382     input_name=\"X\",\n",
       "   1383 )\n",
       "   1385 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n",
       "   1387 check_consistent_length(X, y)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:1105, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n",
       "   1099     raise ValueError(\n",
       "   1100         f\"Found array with dim {array.ndim},\"\n",
       "   1101         f\" while dim <= 2 is required{context}.\"\n",
       "   1102     )\n",
       "   1104 if ensure_all_finite:\n",
       "-> 1105     _assert_all_finite(\n",
       "   1106         array,\n",
       "   1107         input_name=input_name,\n",
       "   1108         estimator_name=estimator_name,\n",
       "   1109         allow_nan=ensure_all_finite == \"allow-nan\",\n",
       "   1110     )\n",
       "   1112 if copy:\n",
       "   1113     if _is_numpy_namespace(xp):\n",
       "   1114         # only make a copy if `array` and `array_orig` may share memory`\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:120, in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)\n",
       "    117 if first_pass_isfinite:\n",
       "    118     return\n",
       "--> 120 _assert_all_finite_element_wise(\n",
       "    121     X,\n",
       "    122     xp=xp,\n",
       "    123     allow_nan=allow_nan,\n",
       "    124     msg_dtype=msg_dtype,\n",
       "    125     estimator_name=estimator_name,\n",
       "    126     input_name=input_name,\n",
       "    127 )\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/sklearn/utils/validation.py:169, in _assert_all_finite_element_wise(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\n",
       "    152 if estimator_name and input_name == \"X\" and has_nan_error:\n",
       "    153     # Improve the error message on how to handle missing values in\n",
       "    154     # scikit-learn.\n",
       "    155     msg_err += (\n",
       "    156         f\"\\n{estimator_name} does not accept missing values\"\n",
       "    157         \" encoded as NaN natively. For supervised learning, you might want\"\n",
       "   (...)    167         \"#estimators-that-handle-nan-values\"\n",
       "    168     )\n",
       "--> 169 raise ValueError(msg_err)\n",
       "\n",
       "ValueError: Input X contains NaN.\n",
       "LinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select relevant columns for prediction\n",
    "X = data[['Mar.2019']]\n",
    "y = data['Mar.2020']\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a simple Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "69daa60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial                          0\n",
       "Industry and class of worker    1\n",
       "Mar.2019                        1\n",
       "Mar.2020                        1\n",
       "Total Mar.2019                  1\n",
       "Total Mar.2020                  1\n",
       "Men Mar.2019                    1\n",
       "Men Mar.2020                    1\n",
       "Women Mar.2019                  1\n",
       "Women Mar.2020                  1\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values in the dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "762cc379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with any missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Re-select relevant columns after dropping missing values\n",
    "X = data[['Mar.2019']]\n",
    "y = data['Mar.2020']\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Re-train a simple Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "12890438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11439.601104677069"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3dea86d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                               #featureID  row ID     row m/z  \\\n",
       "0      358.3677167129743_3.65612984126984     241  358.367717   \n",
       "1      423.2744890715284_4.29798541001065     695  423.274489   \n",
       "2     304.2993572401259_5.121302585521083     382  304.299357   \n",
       "3     389.2691196723436_3.383737479270316     300  389.269120   \n",
       "4    332.3307817246258_5.3103554720133594     612  332.330782   \n",
       "..                                    ...     ...         ...   \n",
       "372    556.530196125893_7.144701557632398     217  556.530196   \n",
       "373  597.2885244321143_3.4814098837209304     226  597.288524   \n",
       "374   734.5708848072682_7.252469799498749     572  734.570885   \n",
       "375  444.38406309814224_7.254970476190479      93  444.384063   \n",
       "376    431.383621750975_6.944787886178863     589  431.383622   \n",
       "\n",
       "     row retention time                                          LibraryID  \\\n",
       "0              3.656130                                                NaN   \n",
       "1              4.297985                                                NaN   \n",
       "2              5.121303  Spectral Match to Benzyldodecyldimethylammoniu...   \n",
       "3              3.383737                                                NaN   \n",
       "4              5.310355  Spectral Match to Benzyltetradecyldimethylammo...   \n",
       "..                  ...                                                ...   \n",
       "372            7.144702                                                NaN   \n",
       "373            3.481410                                                NaN   \n",
       "374            7.252470  Spectral Match to 1-Myristoyl-2-stearoyl-sn-gl...   \n",
       "375            7.254970                                                NaN   \n",
       "376            6.944788  Spectral Match to (+)-.alpha.-Tocopherol from ...   \n",
       "\n",
       "    standard_indentification_level_1                           _feature_id  \\\n",
       "0                                NaN    358.3677167129743_3.65612984126984   \n",
       "1                                NaN    423.2744890715284_4.29798541001065   \n",
       "2                                NaN   304.2993572401259_5.121302585521083   \n",
       "3                                NaN   389.2691196723436_3.383737479270316   \n",
       "4                                NaN  332.3307817246258_5.3103554720133594   \n",
       "..                               ...                                   ...   \n",
       "372                              NaN    556.530196125893_7.144701557632398   \n",
       "373                              NaN  597.2885244321143_3.4814098837209304   \n",
       "374                              NaN   734.5708848072682_7.252469799498749   \n",
       "375                              NaN  444.38406309814224_7.254970476190479   \n",
       "376                              NaN    431.383621750975_6.944787886178863   \n",
       "\n",
       "     importance.score  \n",
       "0            0.067052  \n",
       "1            0.040598  \n",
       "2            0.034141  \n",
       "3            0.032521  \n",
       "4            0.032257  \n",
       "..                ...  \n",
       "372          0.000000  \n",
       "373          0.000000  \n",
       "374          0.000000  \n",
       "375          0.000000  \n",
       "376          0.000000  \n",
       "\n",
       "[377 rows x 8 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('./data/imp.score.ldlr.metabolome.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "871c66ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.042941553351153135), np.float64(0.40575392556570095))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with NA values in 'row retention time' or 'importance.score'\n",
    "data_cleaned = data.dropna(subset=['row retention time', 'importance.score'])\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and its p-value\n",
    "r_value, p_value = pearsonr(data_cleaned['row retention time'], data_cleaned['importance.score'])\n",
    "\n",
    "r_value, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "969de4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@model_rmse[11464.74]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/insurance.csv')\n",
    "\n",
    "# Filter rows with missing values in 'age', 'bmi', and 'charges'\n",
    "data = data.dropna(subset=['age', 'bmi', 'charges'])\n",
    "\n",
    "# Define predictor variables (features) and target variable\n",
    "X = data[['age', 'bmi']]\n",
    "y = data['charges']\n",
    "\n",
    "# Split the data into training and testing sets with an 80-20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Output the RMSE\n",
    "print(f\"@model_rmse[{rmse:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "42a4bdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.65"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/test_ave.csv')\n",
    "\n",
    "# Calculate mean fare\n",
    "mean_fare = round(statistics.mean(data['Fare']), 2)\n",
    "mean_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8ef11086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@bmi_distribution[normal]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Load data\n",
    "file_path = './data/insurance.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate skewness of BMI\n",
    "bmi_skewness = skew(data['bmi'])\n",
    "\n",
    "# Check if distribution is normal based on skewness\n",
    "status = 'normal' if abs(bmi_skewness) < 0.5 else 'not_normal'\n",
    "\n",
    "# Result\n",
    "f\"@bmi_distribution[{status}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ead33b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Symbol Series         Date  Prev Close  Open Price  High Price  \\\n",
       "0  GODREJIND     EQ  15-May-2017      564.60      581.00      584.00   \n",
       "1  GODREJIND     EQ  16-May-2017      578.55      581.45      589.00   \n",
       "2  GODREJIND     EQ  17-May-2017      584.80      583.00      594.00   \n",
       "3  GODREJIND     EQ  18-May-2017      588.60      582.00      588.85   \n",
       "4  GODREJIND     EQ  19-May-2017      574.60      581.00      585.80   \n",
       "\n",
       "   Low Price  Last Price  Close Price  Average Price  Total Traded Quantity  \\\n",
       "0     568.50      578.90       578.55         578.09                 797171   \n",
       "1     572.25      583.80       584.80         583.60                 500223   \n",
       "2     576.85      584.90       588.60         588.74                 504155   \n",
       "3     571.20      572.25       574.60         580.90                 223583   \n",
       "4     567.55      579.85       578.00         577.31                 245436   \n",
       "\n",
       "       Turnover  No. of Trades  Deliverable Qty  % Dly Qt to Traded Qty  \n",
       "0  4.608362e+08          21649           360927                   45.28  \n",
       "1  2.919302e+08          17204           210364                   42.05  \n",
       "2  2.968149e+08           8567           261667                   51.90  \n",
       "3  1.298786e+08           7144            99785                   44.63  \n",
       "4  1.416925e+08           4969            68041                   27.72  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = './data/GODREJIND.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6c7d3f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(16.65), np.float64(15.67), np.float64(6.72))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the \"Price Range\" column and calculate mean, median, and standard deviation\n",
    "data['Price Range'] = data['High Price'] - data['Low Price']\n",
    "price_range_mean = round(data['Price Range'].mean(), 2)\n",
    "price_range_median = round(data['Price Range'].median(), 2)\n",
    "price_range_std_dev = round(data['Price Range'].std(), 2)\n",
    "price_range_mean, price_range_median, price_range_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "40ef144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                             #featureID  row ID     row m/z  \\\n",
       " 0    358.3677167129743_3.65612984126984     241  358.367717   \n",
       " 1    423.2744890715284_4.29798541001065     695  423.274489   \n",
       " 2   304.2993572401259_5.121302585521083     382  304.299357   \n",
       " 3   389.2691196723436_3.383737479270316     300  389.269120   \n",
       " 4  332.3307817246258_5.3103554720133594     612  332.330782   \n",
       " \n",
       "    row retention time                                          LibraryID  \\\n",
       " 0            3.656130                                                NaN   \n",
       " 1            4.297985                                                NaN   \n",
       " 2            5.121303  Spectral Match to Benzyldodecyldimethylammoniu...   \n",
       " 3            3.383737                                                NaN   \n",
       " 4            5.310355  Spectral Match to Benzyltetradecyldimethylammo...   \n",
       " \n",
       "   standard_indentification_level_1                           _feature_id  \\\n",
       " 0                              NaN    358.3677167129743_3.65612984126984   \n",
       " 1                              NaN    423.2744890715284_4.29798541001065   \n",
       " 2                              NaN   304.2993572401259_5.121302585521083   \n",
       " 3                              NaN   389.2691196723436_3.383737479270316   \n",
       " 4                              NaN  332.3307817246258_5.3103554720133594   \n",
       " \n",
       "    importance.score  \n",
       " 0          0.067052  \n",
       " 1          0.040598  \n",
       " 2          0.034141  \n",
       " 3          0.032521  \n",
       " 4          0.032257  ,\n",
       " Index(['#featureID', 'row ID', 'row m/z', 'row retention time', 'LibraryID',\n",
       "        'standard_indentification_level_1', '_feature_id', 'importance.score'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "data_path = './data/imp.score.ldlr.metabolome.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check column names and first few rows\n",
    "data.head(), data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "45a55a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
       "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
       "  sqr = _ensure_numeric((avg - values) ** 2)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(1.5179700725723566e-33), np.float64(-inf), np.float64(nan))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conduct a Shapiro-Wilk test on the `importance.score` column\n",
    "shapiro_test_stat, shapiro_p_value = shapiro(data['importance.score'])\n",
    "\n",
    "if shapiro_p_value < 0.05:\n",
    "    # Not normally distributed; apply log transformation\n",
    "    data['log_importance_score'] = np.log(data['importance.score'])\n",
    "    transformed_mean = data['log_importance_score'].mean()\n",
    "    transformed_std = data['log_importance_score'].std()\n",
    "else:\n",
    "    # Normally distributed; do not transform\n",
    "    transformed_mean = None\n",
    "    transformed_std = None\n",
    "\n",
    "shapiro_p_value, transformed_mean, transformed_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "363016c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(3.373582273291452e-33),\n",
       " np.float64(-6.958119818079467),\n",
       " np.float64(1.2651279341285906))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle infinite or problematic values due to log transformation\n",
    "# Remove rows where `importance.score` is less than or equal to zero because log is not defined for such values\n",
    "data = data[data['importance.score'] > 0]\n",
    "\n",
    "# Redo the Shapiro-Wilk test\n",
    "shapiro_test_stat, shapiro_p_value = shapiro(data['importance.score'])\n",
    "\n",
    "if shapiro_p_value < 0.05:\n",
    "    # Not normally distributed; apply log transformation\n",
    "    data['log_importance_score'] = np.log(data['importance.score'])\n",
    "    transformed_mean = data['log_importance_score'].mean()\n",
    "    transformed_std = data['log_importance_score'].std()\n",
    "else:\n",
    "    # Normally distributed; do not transform\n",
    "    transformed_mean = None\n",
    "    transformed_std = None\n",
    "\n",
    "shapiro_p_value, transformed_mean, transformed_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "09156282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       site  abs_diffsel  positive_diffsel  negative_diffsel  max_diffsel  \\\n",
       "0  (HA2)121     9.026365          4.147102         -4.879263     1.578739   \n",
       "1       326     9.002765          3.615601         -5.387164     0.716922   \n",
       "2       280     8.418638          5.146938         -3.271700     0.971071   \n",
       "3         9     8.185717          4.420441         -3.765276     1.000554   \n",
       "4       210     8.058663          5.824386         -2.234277     1.378964   \n",
       "\n",
       "   min_diffsel  \n",
       "0    -1.004167  \n",
       "1    -1.218422  \n",
       "2    -1.018267  \n",
       "3    -0.847152  \n",
       "4    -1.240547  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset for preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "data_path = './data/ferret-Pitt-2-preinf-lib2-100_sitediffsel.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "be23d89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicated records\n",
    "duplicated_records = df.duplicated().sum()\n",
    "duplicated_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "caf55d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site                0\n",
       "abs_diffsel         0\n",
       "positive_diffsel    0\n",
       "negative_diffsel    0\n",
       "max_diffsel         0\n",
       "min_diffsel         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "db7f6765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       site  abs_diffsel  positive_diffsel  negative_diffsel  max_diffsel  \\\n",
       "0  (HA2)121     9.026365          4.147102         -4.879263     1.578739   \n",
       "1       326     9.002765          3.615601         -5.387164     0.716922   \n",
       "2       280     8.418638          5.146938         -3.271700     0.971071   \n",
       "3         9     8.185717          4.420441         -3.765276     1.000554   \n",
       "4       210     8.058663          5.824386         -2.234277     1.378964   \n",
       "\n",
       "   min_diffsel  diff_range  \n",
       "0    -1.004167    2.582906  \n",
       "1    -1.218422    1.935344  \n",
       "2    -1.018267    1.989339  \n",
       "3    -0.847152    1.847706  \n",
       "4    -1.240547    2.619511  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data is clean and has no missing values, proceeding to add the 'diff_range' column.\n",
    "df['diff_range'] = df['max_diffsel'] - df['min_diffsel']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f26db4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/cleaned_ferret_data.csv'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the cleaned dataset as a CSV file with the new feature \"diff_range\"\n",
    "cleaned_dataset_path = './data/cleaned_ferret_data.csv'\n",
    "df.to_csv(cleaned_dataset_path, index=False)\n",
    "cleaned_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9b28eab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       site  abs_diffsel  positive_diffsel  negative_diffsel  max_diffsel  \\\n",
       "0  (HA2)121     9.026365          4.147102         -4.879263     1.578739   \n",
       "1       326     9.002765          3.615601         -5.387164     0.716922   \n",
       "2       280     8.418638          5.146938         -3.271700     0.971071   \n",
       "3         9     8.185717          4.420441         -3.765276     1.000554   \n",
       "4       210     8.058663          5.824386         -2.234277     1.378964   \n",
       "\n",
       "   min_diffsel  \n",
       "0    -1.004167  \n",
       "1    -1.218422  \n",
       "2    -1.018267  \n",
       "3    -0.847152  \n",
       "4    -1.240547  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"./data/ferret-Pitt-2-preinf-lib2-100_sitediffsel.csv\")\n",
    "\n",
    "# Check the first few rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cd203c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.14), np.float64(-0.29))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate skewness and kurtosis for the column 'abs_diffsel', rounded to 2 decimal places\n",
    "abs_diffsel_data = data['abs_diffsel']\n",
    "skewness_value = round(skew(abs_diffsel_data, bias=False), 2)\n",
    "kurtosis_value = round(kurtosis(abs_diffsel_data, bias=False), 2)\n",
    "\n",
    "skewness_value, kurtosis_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "de0068e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(57.41), np.float64(39.63))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "file_path = './data/baseball_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Group the data by the indicator of free agency eligibility\n",
    "# and calculate the average number of runs\n",
    "average_runs = data.groupby('indicator_of_free_agency_eligibility')['number_of_runs'].mean()\n",
    "\n",
    "# Fetch the results\n",
    "average_runs_by_eligible_for_free_agency = round(average_runs.get(1, 0), 2)\n",
    "average_runs_by_not_eligible_for_free_agency = round(average_runs.get(0, 0), 2)\n",
    "\n",
    "average_runs_by_eligible_for_free_agency, average_runs_by_not_eligible_for_free_agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fb173313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_47639/3183827385.py in ?()\n",
       "      4 # Load the baseball data\n",
       "      5 data = pd.read_csv('./data/baseball_data.csv')\n",
       "      6 \n",
       "      7 # Filter out rows with missing values in the 'home_runs' column\n",
       "----> 8 data = data.dropna(subset=['home_runs'])\n",
       "      9 \n",
       "     10 # Perform the Shapiro-Wilk test\n",
       "     11 stat, p_value = shapiro(data['home_runs'])\n",
       "\n",
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py in ?(self, axis, how, thresh, subset, inplace, ignore_index)\n",
       "   6688             ax = self._get_axis(agg_axis)\n",
       "   6689             indices = ax.get_indexer_for(subset)\n",
       "   6690             check = indices == -1\n",
       "   6691             if check.any():\n",
       "-> 6692                 raise KeyError(np.array(subset)[check].tolist())\n",
       "   6693             agg_obj = self.take(indices, axis=agg_axis)\n",
       "   6694 \n",
       "   6695         if thresh is not lib.no_default:\n",
       "\n",
       "KeyError: ['home_runs']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the baseball data\n",
    "data = pd.read_csv('./data/baseball_data.csv')\n",
    "\n",
    "# Filter out rows with missing values in the 'home_runs' column\n",
    "data = data.dropna(subset=['home_runs'])\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "stat, p_value = shapiro(data['home_runs'])\n",
    "\n",
    "# Determine normality\n",
    "alpha = 0.05\n",
    "normality_test = 'normal' if p_value >= alpha else 'not_normal'\n",
    "\n",
    "p_value, normality_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "078f5673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   salary_in_thousands_of_dollars  batting_average  on_base_percentage  \\\n",
       " 0                            3300            0.272               0.302   \n",
       " 1                            2600            0.269               0.335   \n",
       " 2                            2500            0.249               0.337   \n",
       " 3                            2475            0.260               0.292   \n",
       " 4                            2313            0.273               0.346   \n",
       " \n",
       "    number_of_runs  number_of_hits  number_of_doubles  number_of_triples  \\\n",
       " 0              69             153                 21                  4   \n",
       " 1              58             111                 17                  2   \n",
       " 2              54             115                 15                  1   \n",
       " 3              59             128                 22                  7   \n",
       " 4              87             169                 28                  5   \n",
       " \n",
       "    number_of_home_runs  number_of_runs_batted_in  number_of_walks  \\\n",
       " 0                 31.0                       104               22   \n",
       " 1                 18.0                        66               39   \n",
       " 2                 17.0                        73               63   \n",
       " 3                 12.0                        50               23   \n",
       " 4                  8.0                        58               70   \n",
       " \n",
       "    number_of_strike_outs  number_of_stolen_bases  number_of_errors  \\\n",
       " 0                   80.0                       4               3.0   \n",
       " 1                   69.0                       0               3.0   \n",
       " 2                  116.0                       6               5.0   \n",
       " 3                   64.0                      21              21.0   \n",
       " 4                   53.0                       3               8.0   \n",
       " \n",
       "    indicator_of_free_agency_eligibility  indicator_of_free_agent_in_1991_1992  \\\n",
       " 0                                   1.0                                     0   \n",
       " 1                                   1.0                                     1   \n",
       " 2                                   1.0                                     0   \n",
       " 3                                   0.0                                     0   \n",
       " 4                                   0.0                                     0   \n",
       " \n",
       "    indicator_of_arbitration_eligibility  indicator_of_arbitration_in_1991_1992  \n",
       " 0                                   0.0                                    0.0  \n",
       " 1                                   0.0                                    0.0  \n",
       " 2                                   0.0                                    0.0  \n",
       " 3                                   1.0                                    0.0  \n",
       " 4                                   1.0                                    0.0  ,\n",
       " Index(['salary_in_thousands_of_dollars', 'batting_average',\n",
       "        'on_base_percentage', 'number_of_runs', 'number_of_hits',\n",
       "        'number_of_doubles', 'number_of_triples', 'number_of_home_runs',\n",
       "        'number_of_runs_batted_in', 'number_of_walks', 'number_of_strike_outs',\n",
       "        'number_of_stolen_bases', 'number_of_errors',\n",
       "        'indicator_of_free_agency_eligibility',\n",
       "        'indicator_of_free_agent_in_1991_1992',\n",
       "        'indicator_of_arbitration_eligibility',\n",
       "        'indicator_of_arbitration_in_1991_1992'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check the structure of the data to confirm column names and identify issues\n",
    "data = pd.read_csv('./data/baseball_data.csv')\n",
    "data.head(), data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "830e36bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.0681817011261468e-16), 'not_normal')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Filter out rows with missing values in the 'number_of_home_runs' column\n",
    "data = data.dropna(subset=['number_of_home_runs'])\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "stat, p_value = shapiro(data['number_of_home_runs'])\n",
    "\n",
    "# Determine normality\n",
    "alpha = 0.05\n",
    "normality_test = 'normal' if p_value >= alpha else 'not_normal'\n",
    "\n",
    "p_value, normality_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6b003fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   salary_in_thousands_of_dollars  batting_average  on_base_percentage  \\\n",
       "0                            3300            0.272               0.302   \n",
       "1                            2600            0.269               0.335   \n",
       "2                            2500            0.249               0.337   \n",
       "3                            2475            0.260               0.292   \n",
       "4                            2313            0.273               0.346   \n",
       "\n",
       "   number_of_runs  number_of_hits  number_of_doubles  number_of_triples  \\\n",
       "0              69             153                 21                  4   \n",
       "1              58             111                 17                  2   \n",
       "2              54             115                 15                  1   \n",
       "3              59             128                 22                  7   \n",
       "4              87             169                 28                  5   \n",
       "\n",
       "   number_of_home_runs  number_of_runs_batted_in  number_of_walks  \\\n",
       "0                 31.0                       104               22   \n",
       "1                 18.0                        66               39   \n",
       "2                 17.0                        73               63   \n",
       "3                 12.0                        50               23   \n",
       "4                  8.0                        58               70   \n",
       "\n",
       "   number_of_strike_outs  number_of_stolen_bases  number_of_errors  \\\n",
       "0                   80.0                       4               3.0   \n",
       "1                   69.0                       0               3.0   \n",
       "2                  116.0                       6               5.0   \n",
       "3                   64.0                      21              21.0   \n",
       "4                   53.0                       3               8.0   \n",
       "\n",
       "   indicator_of_free_agency_eligibility  indicator_of_free_agent_in_1991_1992  \\\n",
       "0                                   1.0                                     0   \n",
       "1                                   1.0                                     1   \n",
       "2                                   1.0                                     0   \n",
       "3                                   0.0                                     0   \n",
       "4                                   0.0                                     0   \n",
       "\n",
       "   indicator_of_arbitration_eligibility  indicator_of_arbitration_in_1991_1992  \n",
       "0                                   0.0                                    0.0  \n",
       "1                                   0.0                                    0.0  \n",
       "2                                   0.0                                    0.0  \n",
       "3                                   1.0                                    0.0  \n",
       "4                                   1.0                                    0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = './data/baseball_data.csv'\n",
    "import pandas as pd\n",
    "baseball_data = pd.read_csv(data_path)\n",
    "baseball_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "eb6c99d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.5774226751694127), np.float64(2.3477411044561924e-31))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Pearson correlation coefficient and p-value\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Extract relevant columns\n",
    "doubles = baseball_data['number_of_doubles']\n",
    "salary = baseball_data['salary_in_thousands_of_dollars']\n",
    "\n",
    "# Calculate correlation coefficient and p-value\n",
    "r_value, p_value = pearsonr(doubles, salary)\n",
    "\n",
    "r_value, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb13bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of the Fare column\n",
    "mean_fare = data['Fare'].mean()\n",
    "std_fare = data['Fare'].std()\n",
    "mean_fare, std_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6ec5c33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(32.204207968574636), np.float64(49.6934285971809))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Z-scores for the Fare column and count outliers\n",
    "data['Fare_Z_score'] = (data['Fare'] - data['Fare'].mean()) / data['Fare'].std()\n",
    "outliers = data[(data['Fare_Z_score'] > 3) | (data['Fare_Z_score'] < -3)]\n",
    "outliers_count = outliers.shape[0]\n",
    "outliers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3815d66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 TRUE_TIME  TIME        USFLUX  MEANGAM  MEANGBT  MEANGBZ  \\\n",
       "0  2014.03.23_20:24:00_TAI  11.6  3.246502e+21   21.786   93.013   92.809   \n",
       "1  2014.03.23_20:36:00_TAI  11.8  3.908340e+21   21.740   89.953   89.779   \n",
       "2  2014.03.23_20:48:00_TAI  12.0  4.041844e+21   21.797   89.552   89.566   \n",
       "3  2014.03.23_21:00:00_TAI  12.2  4.096817e+21   21.654   89.355   89.499   \n",
       "4  2014.03.23_21:12:00_TAI  12.4  4.197154e+21   21.732   87.089   87.454   \n",
       "\n",
       "   MEANGBH   MEANJZD       TOTUSJZ   MEANJZH  TOTUSJH  ABSNJZH       SAVNCPP  \\\n",
       "0   31.210  0.087461  3.141588e+12  0.002863  143.341   14.092  2.248874e+11   \n",
       "1   31.535  0.151386  3.745627e+12  0.003097  173.704   18.216  4.651086e+11   \n",
       "2   30.425  0.139126  3.790352e+12  0.002931  174.009   18.001  4.464203e+11   \n",
       "3   30.440  0.234519  3.604093e+12  0.003071  164.412   19.141  7.636783e+11   \n",
       "4   29.875  0.266657  3.622492e+12  0.003420  163.141   22.204  9.045709e+11   \n",
       "\n",
       "    MEANPOT        TOTPOT MEANSHR  SHRGT45  R_VALUE   AREA_ACR  \n",
       "0  1185.247  7.747525e+21  18.695    0.061      0.0  69.264130  \n",
       "1  1155.593  9.025444e+21  18.172    0.000      0.0  83.896141  \n",
       "2  1132.300  9.235995e+21  18.322    0.016      0.0  86.314224  \n",
       "3  1100.275  9.107749e+21  18.134    0.048      0.0  87.762978  \n",
       "4  1032.512  8.903345e+21  17.850    0.046      0.0  84.621979  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and import necessary libraries.\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/3901.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b45bf113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Remove duplicate rows\n",
    "duplicate_count = data.duplicated().sum()\n",
    "data = data.drop_duplicates()\n",
    "duplicate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5c1c7047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.463837954032957e+21)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Fill missing values in the USFLUX column with the mean value\n",
    "usflux_mean = round(data['USFLUX'].mean(), 2)\n",
    "data['USFLUX'] = data['USFLUX'].fillna(usflux_mean)\n",
    "usflux_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bfad8267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-89.04)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Create the new feature MEANGAM_MEANGBZ_DIFF\n",
    "data['MEANGAM_MEANGBZ_DIFF'] = data['MEANGAM'] - data['MEANGBZ']\n",
    "new_feature_mean = round(data['MEANGAM_MEANGBZ_DIFF'].mean(), 2)\n",
    "new_feature_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b5d74f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.746"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Apply machine learning to predict TOTUSJH using Random Forest Regressor\n",
    "\n",
    "# Select features and target variables\n",
    "X = data[['MEANJZH', 'TOTUSJZ', 'MEANGBT']]\n",
    "y = data['TOTUSJH']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model with 100 trees\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy using R^2 score\n",
    "model_accuracy = round(r2_score(y_test, y_pred) * 100, 3)\n",
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1f1ed2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@mean_age[1.1]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = './data/test_x.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "mean_age = round(data['Age'].mean(), 2)\n",
    "\"@mean_age[{}]\".format(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "50e716a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   time  sun  temperature  chill  humidity wind direction  \\\n",
       "0  2017-01-01T00:00:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "1  2017-01-01T00:10:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "2  2017-01-01T00:20:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "3  2017-01-01T00:30:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "4  2017-01-01T00:40:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "\n",
       "   wind speed  visibility  air pressure  \n",
       "0         3.0       197.0        1026.0  \n",
       "1         3.0       195.0        1025.8  \n",
       "2         3.0       271.0        1025.6  \n",
       "3         3.0       316.0        1025.4  \n",
       "4         3.0       334.0        1025.3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/weather_train.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "793bb6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where 'temperature' or 'humidity' have missing values\n",
    "cleaned_data = data[['temperature', 'humidity']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "de98b49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.6363773815186452)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Pearson correlation coefficient\n",
    "correlation_matrix = cleaned_data.corr()\n",
    "r_value = correlation_matrix.loc['temperature', 'humidity']\n",
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "84f2bd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moderate'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the strength of the correlation\n",
    "if abs(r_value) >= 0.7:\n",
    "    strength = 'strong'\n",
    "elif 0.3 <= abs(r_value) < 0.7:\n",
    "    strength = 'moderate'\n",
    "else:\n",
    "    strength = 'weak'\n",
    "strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "502a82f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   time  sun  temperature  chill  humidity wind direction  \\\n",
       "0  2017-01-01T00:00:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "1  2017-01-01T00:10:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "2  2017-01-01T00:20:00Z  0.0         -1.7   -5.6      99.0            ZZW   \n",
       "3  2017-01-01T00:30:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "4  2017-01-01T00:40:00Z  0.0         -1.6   -5.4      99.0              Z   \n",
       "\n",
       "   wind speed  visibility  air pressure  \n",
       "0         3.0       197.0        1026.0  \n",
       "1         3.0       195.0        1025.8  \n",
       "2         3.0       271.0        1025.6  \n",
       "3         3.0       316.0        1025.4  \n",
       "4         3.0       334.0        1025.3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/weather_train.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4b96aaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(97)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Z-scores for the 'wind speed' column and identify outliers based on threshold\n",
    "z_scores = zscore(data['wind speed'].dropna())\n",
    "outliers = (z_scores > 3) | (z_scores < -3)\n",
    "outlier_count = outliers.sum()\n",
    "outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "76e04900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date  Trips over the past 24-hours (midnight to 11:59pm)  \\\n",
       "0  10/1/2014                                              31197    \n",
       "1  10/2/2014                                              38286    \n",
       "2  10/3/2014                                              38956    \n",
       "3  10/4/2014                                              15088    \n",
       "4  10/5/2014                                              26247    \n",
       "\n",
       "   Cumulative trips (since launch):  \\\n",
       "0                          13296973   \n",
       "1                          13335259   \n",
       "2                          13374215   \n",
       "3                          13389303   \n",
       "4                          13415550   \n",
       "\n",
       "   Miles traveled today (midnight to 11:59 pm)  Miles traveled to date:  \\\n",
       "0                                        44612                 23121175   \n",
       "1                                        60639                 23181814   \n",
       "2                                        65739                 23247553   \n",
       "3                                        24254                 23271807   \n",
       "4                                        48930                 23320737   \n",
       "\n",
       "   Total Annual Memberships Sold  \\\n",
       "0                         124846   \n",
       "1                         124959   \n",
       "2                         125024   \n",
       "3                         125058   \n",
       "4                         125109   \n",
       "\n",
       "  Annual Member Sign-Ups (midnight to 11:59 pm)  \\\n",
       "0                                           112   \n",
       "1                                           113   \n",
       "2                                            65   \n",
       "3                                            34   \n",
       "4                                            51   \n",
       "\n",
       "  24-Hour Passes Purchased (midnight to 11:59 pm)  \\\n",
       "0                                             330   \n",
       "1                                             602   \n",
       "2                                            1276   \n",
       "3                                             617   \n",
       "4                                            1470   \n",
       "\n",
       "  7-Day Passes Purchased (midnight to 11:59 pm)  \n",
       "0                                            48  \n",
       "1                                            86  \n",
       "2                                           107  \n",
       "3                                            26  \n",
       "4                                            90  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "data_file = './data/2014_q4.csv'\n",
    "df = pd.read_csv(data_file)  # Load the dataset\n",
    "# Inspect the dataset to locate required columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "16229d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'24-Hour Passes (Midnight to 11:59 pm)': np.int64(0),\n",
       " '7-Day Passes (Midnight to 11:59 pm)': np.int64(0),\n",
       " 'Trips over 24-hours (Midnight to 11:59 pm)': np.int64(0)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values in the required columns\n",
    "missing_values_count = {\n",
    "    '24-Hour Passes (Midnight to 11:59 pm)': df['24-Hour Passes Purchased (midnight to 11:59 pm)'].isnull().sum(),\n",
    "    '7-Day Passes (Midnight to 11:59 pm)': df['7-Day Passes Purchased (midnight to 11:59 pm)'].isnull().sum(),\n",
    "    'Trips over 24-hours (Midnight to 11:59 pm)': df['Trips over the past 24-hours (midnight to 11:59pm)'].isnull().sum()\n",
    "}\n",
    "\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0f98c32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "TypeError                                 Traceback (most recent call last)\n",
       "Cell In[5], line 3\n",
       "      1 # Since there are no missing values detected, we mimic the imputation process as imposed.\n",
       "      2 # Calculate mean of the relevant columns\n",
       "----> 3 mean_24_hour_passes = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].mean()\n",
       "      4 mean_7_day_passes = df['7-Day Passes Purchased (midnight to 11:59 pm)'].mean()\n",
       "      6 # Impute the data with the calculated mean to simulate the process.\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/series.py:6570, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n",
       "   6562 @doc(make_doc(\"mean\", ndim=1))\n",
       "   6563 def mean(\n",
       "   6564     self,\n",
       "   (...)   6568     **kwargs,\n",
       "   6569 ):\n",
       "-> 6570     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/generic.py:12485, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n",
       "  12478 def mean(\n",
       "  12479     self,\n",
       "  12480     axis: Axis | None = 0,\n",
       "   (...)  12483     **kwargs,\n",
       "  12484 ) -> Series | float:\n",
       "> 12485     return self._stat_function(\n",
       "  12486         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n",
       "  12487     )\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/generic.py:12442, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n",
       "  12438 nv.validate_func(name, (), kwargs)\n",
       "  12440 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n",
       "> 12442 return self._reduce(\n",
       "  12443     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n",
       "  12444 )\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/series.py:6478, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n",
       "   6473     # GH#47500 - change to TypeError to match other methods\n",
       "   6474     raise TypeError(\n",
       "   6475         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n",
       "   6476         \"with non-numeric dtypes.\"\n",
       "   6477     )\n",
       "-> 6478 return op(delegate, skipna=skipna, **kwds)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:147, in bottleneck_switch.__call__.<locals>.f(values, axis, skipna, **kwds)\n",
       "    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n",
       "    146 else:\n",
       "--> 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n",
       "    149 return result\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:404, in _datetimelike_compat.<locals>.new_func(values, axis, skipna, mask, **kwargs)\n",
       "    401 if datetimelike and mask is None:\n",
       "    402     mask = isna(values)\n",
       "--> 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n",
       "    406 if datetimelike:\n",
       "    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:720, in nanmean(values, axis, skipna, mask)\n",
       "    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n",
       "    719 the_sum = values.sum(axis, dtype=dtype_sum)\n",
       "--> 720 the_sum = _ensure_numeric(the_sum)\n",
       "    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n",
       "    723     count = cast(np.ndarray, count)\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/nanops.py:1701, in _ensure_numeric(x)\n",
       "   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n",
       "   1699     if isinstance(x, str):\n",
       "   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n",
       "-> 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n",
       "   1702     try:\n",
       "   1703         x = float(x)\n",
       "\n",
       "TypeError: Could not convert string '33060212766171470710593667709905528208374986950649911801806100051453911074722209312205196284665576539848056866551398416878952607599325252242522319257810216013134562837436514742092354492814565191153592546018231441513243261861191861191581451701566836\t456\t414645832637214216' to numeric"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since there are no missing values detected, we mimic the imputation process as imposed.\n",
    "# Calculate mean of the relevant columns\n",
    "mean_24_hour_passes = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].mean()\n",
    "mean_7_day_passes = df['7-Day Passes Purchased (midnight to 11:59 pm)'].mean()\n",
    "\n",
    "# Impute the data with the calculated mean to simulate the process.\n",
    "df['24-Hour Passes Purchased (midnight to 11:59 pm)'] = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].fillna(mean_24_hour_passes)\n",
    "df['7-Day Passes Purchased (midnight to 11:59 pm)'] = df['7-Day Passes Purchased (midnight to 11:59 pm)'].fillna(mean_7_day_passes)\n",
    "\n",
    "def calculate_distribution_statistics(series):\n",
    "    return {\n",
    "        'mean': round(series.mean(), 2),\n",
    "        'median': round(series.median(), 2),\n",
    "        'std_dev': round(series.std(), 2),\n",
    "        'skewness': round(skew(series), 2),\n",
    "        'kurtosis': round(kurtosis(series), 2)\n",
    "    }\n",
    "\n",
    "# Calculate statistics before imputation\n",
    "stats_before = calculate_distribution_statistics(df['Trips over the past 24-hours (midnight to 11:59pm)'])\n",
    "\n",
    "stats_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "98d300c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "NameError                                 Traceback (most recent call last)\n",
       "Cell In[6], line 14\n",
       "     11 mean_24_hour_passes = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].mean()\n",
       "     12 mean_7_day_passes = df['7-Day Passes Purchased (midnight to 11:59 pm)'].mean()\n",
       "---> 14 stats_before = calculate_distribution_statistics(df['Trips over the past 24-hours (midnight to 11:59pm)'])\n",
       "     16 stats_before\n",
       "\n",
       "NameError: name 'calculate_distribution_statistics' is not defined"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the error above, it appears that some columns may not be correctly read as numeric.\n",
    "# Converting needed columns to numeric explicitly.\n",
    "df['24-Hour Passes Purchased (midnight to 11:59 pm)'] = pd.to_numeric(\n",
    "    df['24-Hour Passes Purchased (midnight to 11:59 pm)'], errors='coerce')\n",
    "df['7-Day Passes Purchased (midnight to 11:59 pm)'] = pd.to_numeric(\n",
    "    df['7-Day Passes Purchased (midnight to 11:59 pm)'], errors='coerce')\n",
    "df['Trips over the past 24-hours (midnight to 11:59pm)'] = pd.to_numeric(\n",
    "    df['Trips over the past 24-hours (midnight to 11:59pm)'], errors='coerce')\n",
    "\n",
    "# Re-calculate means for imputation\n",
    "mean_24_hour_passes = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].mean()\n",
    "mean_7_day_passes = df['7-Day Passes Purchased (midnight to 11:59 pm)'].mean()\n",
    "\n",
    "stats_before = calculate_distribution_statistics(df['Trips over the past 24-hours (midnight to 11:59pm)'])\n",
    "\n",
    "stats_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3996441b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': np.float64(21144.08),\n",
       " 'median': np.float64(19711.0),\n",
       " 'std_dev': np.float64(9889.93),\n",
       " 'skewness': np.float64(0.15),\n",
       " 'kurtosis': np.float64(-1.13)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine the function since previous execution likely failed\n",
    "def calculate_distribution_statistics(series):\n",
    "    return {\n",
    "        'mean': round(series.mean(), 2),\n",
    "        'median': round(series.median(), 2),\n",
    "        'std_dev': round(series.std(), 2),\n",
    "        'skewness': round(skew(series, nan_policy='omit'), 2),\n",
    "        'kurtosis': round(kurtosis(series, nan_policy='omit'), 2)\n",
    "    }\n",
    "\n",
    "stats_before = calculate_distribution_statistics(df['Trips over the past 24-hours (midnight to 11:59pm)'])\n",
    "stats_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d296e0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': np.float64(21144.08),\n",
       " 'median': np.float64(19711.0),\n",
       " 'std_dev': np.float64(9889.93),\n",
       " 'skewness': np.float64(0.15),\n",
       " 'kurtosis': np.float64(-1.13)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute missing values using the mean\n",
    "df['Trips over the past 24-hours (midnight to 11:59pm)'] = df['Trips over the past 24-hours (midnight to 11:59pm)'].fillna(\n",
    "    df['Trips over the past 24-hours (midnight to 11:59pm)'].mean()\n",
    ")\n",
    "stats_after = calculate_distribution_statistics(\n",
    "    df['Trips over the past 24-hours (midnight to 11:59pm)']\n",
    ")\n",
    "\n",
    "stats_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "54513d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "NameError                                 Traceback (most recent call last)\n",
       "Cell In[1], line 3\n",
       "      1 # Loading the Titanic dataset\n",
       "      2 data_path = './data/titanic_train.csv'\n",
       "----> 3 data = pd.read_csv(data_path)\n",
       "      4 data.head()\n",
       "\n",
       "NameError: name 'pd' is not defined"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Titanic dataset\n",
    "data_path = './data/titanic_train.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d71091ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@skewness_value[0.66]\\n@skewness_type[positive]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_skewness(file_path):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Remove missing values in the 'wind speed' column\n",
    "    wind_speed = data['wind speed'].dropna()\n",
    "\n",
    "    # Calculate Pearson's First Coefficient of Skewness\n",
    "    mean_speed = wind_speed.mean()\n",
    "    median_speed = wind_speed.median()\n",
    "    std_dev = wind_speed.std()\n",
    "    skewness = 3 * (mean_speed - median_speed) / std_dev\n",
    "\n",
    "    # Determine the skewness type\n",
    "    if skewness > 0:\n",
    "        skewness_type = \"positive\"\n",
    "    elif skewness < 0:\n",
    "        skewness_type = \"negative\"\n",
    "    else:\n",
    "        skewness_type = \"symmetric\"\n",
    "\n",
    "    # Return results\n",
    "    return round(skewness, 2), skewness_type\n",
    "\n",
    "# Compute skewness using the function\n",
    "file_path = './data/weather_train.csv'\n",
    "skewness_value, skewness_type = compute_skewness(file_path)\n",
    "\n",
    "output = f\"@skewness_value[{skewness_value}]\\n@skewness_type[{skewness_type}]\"\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d43eae01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.1896"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/cost_data_with_errors.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Handle missing values by imputing with column means\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "columns_to_impute = ['max_sust_wind', 'damage_USD', 'min_p']\n",
    "df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])\n",
    "\n",
    "# Prepare the data for training\n",
    "X = df[['damage_USD', 'min_p']]\n",
    "y = df['max_sust_wind']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "round(mse, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e02b2728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(12.89)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/percent-bachelors-degrees-women-usa.csv')\n",
    "\n",
    "# Calculate the mean percentage for the Engineering column\n",
    "mean_engineering = data['Engineering'].mean()\n",
    "\n",
    "# Print the rounded mean value\n",
    "round(mean_engineering, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "113e4dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight  Rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Load the data\n",
    "file_path = './data/abalone.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "19eb186e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex               0\n",
       "Length            0\n",
       "Diameter          0\n",
       "Height            0\n",
       "Whole weight      0\n",
       "Shucked weight    0\n",
       "Viscera weight    0\n",
       "Shell weight      0\n",
       "Rings             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "110db815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sex    Length  Diameter    Height  Whole weight  Shucked weight  \\\n",
       "0   M  0.513514  0.521008  0.084071      0.181335        0.150303   \n",
       "1   M  0.371622  0.352941  0.079646      0.079157        0.066241   \n",
       "2   F  0.614865  0.613445  0.119469      0.239065        0.171822   \n",
       "3   M  0.493243  0.521008  0.110619      0.182044        0.144250   \n",
       "4   I  0.344595  0.336134  0.070796      0.071897        0.059516   \n",
       "\n",
       "   Viscera weight  Shell weight  Rings  \n",
       "0        0.132324      0.147982     15  \n",
       "1        0.063199      0.068261      7  \n",
       "2        0.185648      0.207773      9  \n",
       "3        0.149440      0.152965     10  \n",
       "4        0.051350      0.053313      7  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Min-Max Scaling on specified columns\n",
    "scale_columns = ['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight']\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data[scale_columns] = scaler.fit_transform(data[scale_columns])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f12586a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                Skewness   Kurtosis Distribution Type\n",
       "Length         -0.639643   0.063108        Non-Normal\n",
       "Diameter       -0.608979  -0.046857        Non-Normal\n",
       "Height          3.127694  75.933099        Non-Normal\n",
       "Whole weight    0.530768  -0.025051        Non-Normal\n",
       "Shucked weight  0.718840   0.592975        Non-Normal\n",
       "Viscera weight  0.591640   0.082475        Non-Normal\n",
       "Shell weight    0.620704   0.529854        Non-Normal"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate skewness and kurtosis for scaled columns\n",
    "skewness = data[scale_columns].apply(skew)\n",
    "kurtosis_values = data[scale_columns].apply(kurtosis)\n",
    "distribution_analysis = pd.DataFrame({'Skewness': skewness, 'Kurtosis': kurtosis_values})\n",
    "distribution_analysis['Distribution Type'] = distribution_analysis.apply(lambda x: 'Normal' if -0.5 <= x['Skewness'] <= 0.5 and -2 <= x['Kurtosis'] <= 2 else 'Non-Normal', axis=1)\n",
    "distribution_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8cff51f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Normal'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the data\n",
    "file_path = './data/well_2_complete.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "stat, p_value = shapiro(data['RHO_OLD'])\n",
    "\n",
    "# Determine the normality status\n",
    "if p_value > 0.05:\n",
    "    normality_status = 'Normal'\n",
    "else:\n",
    "    normality_status = 'Not Normal'\n",
    "\n",
    "normality_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "fe93a422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  timestamp  num. busy overflows  num. calls answered  \\\n",
       "0  Apr 13  2017 12:00:00 AM                    0                    0   \n",
       "1  Apr 13  2017 12:15:00 AM                    0                    0   \n",
       "2  Apr 13  2017 12:30:00 AM                    0                    0   \n",
       "3  Apr 13  2017 12:45:00 AM                    0                    0   \n",
       "4   Apr 13  2017 1:00:00 AM                    0                    0   \n",
       "\n",
       "   num. calls abandoned   num. calls transferred   num. calls timed out   \\\n",
       "0                      0                        0                      0   \n",
       "1                      0                        0                      0   \n",
       "2                      0                        0                      0   \n",
       "3                      0                        0                      0   \n",
       "4                      0                        0                      0   \n",
       "\n",
       "   avg. num. agents talking   avg. num. agents staffed  avg. wait time   \\\n",
       "0                        0.0                          4        00:00:00   \n",
       "1                        0.0                          4        00:00:00   \n",
       "2                        0.0                          4        00:00:00   \n",
       "3                        0.0                          4        00:00:00   \n",
       "4                        0.0                          4        00:00:00   \n",
       "\n",
       "  avg. abandonment time  \n",
       "0              00:00:00  \n",
       "1              00:00:00  \n",
       "2              00:00:00  \n",
       "3              00:00:00  \n",
       "4              00:00:00  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './data/20170413_000000_group_statistics.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows and columns of the dataset for exploration\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e9ac6742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_50338/3008320002.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
       "  df['normalized_timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce').dt.strftime('%Y%m%d_%H%M%S')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'num. calls abandoned'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[232], line 8\n",
       "      5 filter_timestamp = df[df['normalized_timestamp'] == '20170413_080000']\n",
       "      7 # Fetch number of calls abandoned and total calls\n",
       "----> 8 num_calls_abandoned = filter_timestamp['num. calls abandoned'].sum()\n",
       "      9 num_calls_answered = filter_timestamp['num. calls answered'].sum()\n",
       "     11 total_calls = num_calls_abandoned + num_calls_answered\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'num. calls abandoned'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize timestamp column format to match '20170413_080000' format\n",
    "df['normalized_timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce').dt.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Filter data for the specific timestamp of interest\n",
    "filter_timestamp = df[df['normalized_timestamp'] == '20170413_080000']\n",
    "\n",
    "# Fetch number of calls abandoned and total calls\n",
    "num_calls_abandoned = filter_timestamp['num. calls abandoned'].sum()\n",
    "num_calls_answered = filter_timestamp['num. calls answered'].sum()\n",
    "\n",
    "total_calls = num_calls_abandoned + num_calls_answered\n",
    "\n",
    "# Calculate call abandonment rate if total_calls > 0\n",
    "if total_calls > 0:\n",
    "    abandonment_rate = (num_calls_abandoned / total_calls) * 100\n",
    "else:\n",
    "    abandonment_rate = 0\n",
    "\n",
    "round(abandonment_rate, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "bcf85526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.25)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust column names to remove extra whitespace\n",
    "df.columns = df.columns.str.strip()  # Stripping any leading/trailing spaces from column names\n",
    "\n",
    "# Reattempt the calculation with updated column names\n",
    "filter_timestamp = df[df['normalized_timestamp'] == '20170413_080000']\n",
    "\n",
    "# Fetch number of calls abandoned and total calls\n",
    "num_calls_abandoned = filter_timestamp['num. calls abandoned'].sum()\n",
    "num_calls_answered = filter_timestamp['num. calls answered'].sum()\n",
    "\n",
    "total_calls = num_calls_abandoned + num_calls_answered\n",
    "\n",
    "# Calculate call abandonment rate if total_calls > 0\n",
    "if total_calls > 0:\n",
    "    abandonment_rate = (num_calls_abandoned / total_calls) * 100\n",
    "else:\n",
    "    abandonment_rate = 0\n",
    "\n",
    "round(abandonment_rate, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "227f57a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_50338/801011774.py in ?()\n",
       "      4 data_file_path = './data/20170413_000000_group_statistics.csv'\n",
       "      5 df = pd.read_csv(data_file_path)\n",
       "      6 \n",
       "      7 # Clean data by removing rows with null/NaN values in 'num. calls abandoned'\n",
       "----> 8 df_cleaned = df.dropna(subset=['num. calls abandoned'])\n",
       "      9 \n",
       "     10 # Convert 'num. calls abandoned' column to numeric just in case it's not\n",
       "     11 # and calculate the total\n",
       "\n",
       "/opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py in ?(self, axis, how, thresh, subset, inplace, ignore_index)\n",
       "   6688             ax = self._get_axis(agg_axis)\n",
       "   6689             indices = ax.get_indexer_for(subset)\n",
       "   6690             check = indices == -1\n",
       "   6691             if check.any():\n",
       "-> 6692                 raise KeyError(np.array(subset)[check].tolist())\n",
       "   6693             agg_obj = self.take(indices, axis=agg_axis)\n",
       "   6694 \n",
       "   6695         if thresh is not lib.no_default:\n",
       "\n",
       "KeyError: ['num. calls abandoned']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "data_file_path = './data/20170413_000000_group_statistics.csv'\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Clean data by removing rows with null/NaN values in 'num. calls abandoned'\n",
    "df_cleaned = df.dropna(subset=['num. calls abandoned'])\n",
    "\n",
    "# Convert 'num. calls abandoned' column to numeric just in case it's not\n",
    "# and calculate the total\n",
    "if df_cleaned['num. calls abandoned'].dtype != 'int64' and df_cleaned['num. calls abandoned'].dtype != 'float64':\n",
    "    df_cleaned['num. calls abandoned'] = pd.to_numeric(df_cleaned['num. calls abandoned'])\n",
    "    \n",
    "total_abandoned_calls = df_cleaned['num. calls abandoned'].sum()\n",
    "total_abandoned_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "47e22ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming the columns to handle the issue with trailing spaces for consistency\n",
    "df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "# Clean data by removing rows with null/NaN values in 'num. calls abandoned'\n",
    "df_cleaned = df.dropna(subset=['num. calls abandoned'])\n",
    "\n",
    "# Convert 'num. calls abandoned' column to numeric just in case it's not\n",
    "# and calculate the total\n",
    "if df_cleaned['num. calls abandoned'].dtype != 'int64' and df_cleaned['num. calls abandoned'].dtype != 'float64':\n",
    "    df_cleaned['num. calls abandoned'] = pd.to_numeric(df_cleaned['num. calls abandoned'])\n",
    "    \n",
    "total_abandoned_calls = df_cleaned['num. calls abandoned'].sum()\n",
    "total_abandoned_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d82ab70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.137722924950291e-15), 'not normal')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "data = pd.read_csv('./data/my_test_01.csv')\n",
    "medinc_data = data['MedInc'].dropna()\n",
    "\n",
    "stat, p_value = shapiro(medinc_data)\n",
    "distribution_type = 'normal' if p_value > 0.05 else 'not normal'\n",
    "\n",
    "p_value, distribution_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "19fc69ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  0.9298      36.0  3.676162   1.100450      2664.0  3.994003     33.93   \n",
       "1  2.7006      17.0  4.499388   1.039780      3331.0  2.038556     32.79   \n",
       "2  5.0286      30.0  6.184375   1.068750       999.0  3.121875     34.89   \n",
       "3  3.9038      21.0  3.586357   0.982583      1486.0  2.156749     37.36   \n",
       "4  7.1754      52.0  7.429487   1.051282       779.0  2.496795     37.90   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -118.25             1.000  \n",
       "1    -117.03             1.663  \n",
       "2    -120.43             1.580  \n",
       "3    -122.02             2.438  \n",
       "4    -122.28             3.629  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/my_test_01.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b531b214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/nx/p2ppjpbj3633dx7v29vl9rqm0000gn/T/ipykernel_50338/3841878384.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
       "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
       "\n",
       "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
       "\n",
       "\n",
       "  data['MedInc'].fillna(mean_medinc, inplace=True)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "MedInc              0\n",
       "HouseAge            0\n",
       "AveRooms            0\n",
       "AveBedrms           0\n",
       "Population          0\n",
       "AveOccup            0\n",
       "Latitude            0\n",
       "Longitude           0\n",
       "MedianHouseValue    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Replace any missing values in the MedInc column with the mean value.\n",
    "mean_medinc = data['MedInc'].mean()\n",
    "data['MedInc'].fillna(mean_medinc, inplace=True)\n",
    "data.isnull().sum()  # Verify no missing values remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4c0bc41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   AveOccup  AveOccup_zscore\n",
       "0  3.994003         1.527273\n",
       "1  2.038556        -1.245173\n",
       "2  3.121875         0.290764\n",
       "3  2.156749        -1.077598\n",
       "4  2.496795        -0.595479"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Standardize the values in the AveOccup column using z-scores.\n",
    "scaler = StandardScaler()\n",
    "data['AveOccup_zscore'] = scaler.fit_transform(data[['AveOccup']])\n",
    "data[['AveOccup', 'AveOccup_zscore']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "64d90a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   AveRooms  Population  RoomsPerPerson\n",
       "0  3.676162      2664.0        0.001380\n",
       "1  4.499388      3331.0        0.001351\n",
       "2  6.184375       999.0        0.006191\n",
       "3  3.586357      1486.0        0.002413\n",
       "4  7.429487       779.0        0.009537"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Create a new feature called \"RoomsPerPerson\" by dividing the AveRooms column by the Population column.\n",
    "data['RoomsPerPerson'] = data['AveRooms'] / data['Population']\n",
    "data[['AveRooms', 'Population', 'RoomsPerPerson']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "bbf8ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0382)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Calculate the Pearson correlation coefficient between the MedianHouseValue and RoomsPerPerson columns.\n",
    "correlation_coefficient = np.corrcoef(data['MedianHouseValue'], data['RoomsPerPerson'])[0, 1]\n",
    "round(correlation_coefficient, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9601b152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.1226), np.float64(1.2186))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Calculate the mean and standard deviation of the MedianHouseValue column.\n",
    "mean_median_house_value = round(np.mean(data['MedianHouseValue']), 4)\n",
    "stddev_median_house_value = round(np.std(data['MedianHouseValue']), 4)\n",
    "mean_median_house_value, stddev_median_house_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6de6787d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          JDTDB              Calendar Date (TDB)             X             Y  \\\n",
       "0  2.451215e+06   A.D. 1999-Feb-05 00:58:29.0000 -2.887479e+08  2.475795e+07   \n",
       "1  2.451216e+06   A.D. 1999-Feb-06 00:58:29.0000 -2.894466e+08  2.303593e+07   \n",
       "2  2.451217e+06   A.D. 1999-Feb-07 00:58:29.0000 -2.901336e+08  2.131300e+07   \n",
       "3  2.451218e+06   A.D. 1999-Feb-08 00:58:29.0000 -2.908092e+08  1.958922e+07   \n",
       "4  2.451219e+06   A.D. 1999-Feb-09 00:58:29.0000 -2.914731e+08  1.786466e+07   \n",
       "\n",
       "              Z  Unnamed: 5  \n",
       "0  2.212686e+07         NaN  \n",
       "1  2.286679e+07         NaN  \n",
       "2  2.360580e+07         NaN  \n",
       "3  2.434388e+07         NaN  \n",
       "4  2.508099e+07         NaN  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing pandas and loading the dataset `DES=+2006261.csv` for analysis\n",
    "import pandas as pd \n",
    "\n",
    "# Load dataset\n",
    "dataset_path = './data/DES=+2006261.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c91e7529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.87), np.float64(0.83))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating correlation between X and Y, and X and Z\n",
    "correlation_XY = data['X'].corr(data['Y'])\n",
    "correlation_XZ = data['X'].corr(data['Z'])\n",
    "\n",
    "round(correlation_XY, 2), round(correlation_XZ, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2ef12752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ff429c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  0.9298      36.0  3.676162   1.100450      2664.0  3.994003     33.93   \n",
       "1  2.7006      17.0  4.499388   1.039780      3331.0  2.038556     32.79   \n",
       "2  5.0286      30.0  6.184375   1.068750       999.0  3.121875     34.89   \n",
       "3  3.9038      21.0  3.586357   0.982583      1486.0  2.156749     37.36   \n",
       "4  7.1754      52.0  7.429487   1.051282       779.0  2.496795     37.90   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -118.25             1.000  \n",
       "1    -117.03             1.663  \n",
       "2    -120.43             1.580  \n",
       "3    -122.02             2.438  \n",
       "4    -122.28             3.629  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = './data/my_test_01.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ff2637c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the relevant features and target variable\n",
    "X = data[['MedInc', 'AveRooms', 'Population', 'Latitude', 'Longitude']]\n",
    "y = data['MedianHouseValue']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e92f66e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "bd869c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.653"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "round(mse, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6e4b64d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0         70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5         70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0         70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0         70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5         70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/auto-mpg.csv')\n",
    "# Display the data to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e944086b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(73.0), np.float64(130.48), np.float64(45.83))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the vehicle with the highest horsepower\n",
    "max_hp_vehicle = data.loc[data['horsepower'].idxmax()]\n",
    "\n",
    "max_hp_year = max_hp_vehicle['modelyear']\n",
    "max_hp_value = max_hp_vehicle['horsepower']\n",
    "\n",
    "# Filter vehicles from the same year\n",
    "same_year_vehicles = data[data['modelyear'] == max_hp_year]\n",
    "\n",
    "# Calculate average horsepower and population standard deviation\n",
    "mean_hp = round(same_year_vehicles['horsepower'].mean(), 2)\n",
    "std_hp = round(same_year_vehicles['horsepower'].std() * ((len(same_year_vehicles) - 1) / len(same_year_vehicles)) ** 0.5, 2)\n",
    "\n",
    "max_hp_year, mean_hp, std_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1c30afc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0   temp  humidity  pressure    description          dt  \\\n",
       " 0           0  32.18        54      1010  moderate rain  1437730851   \n",
       " 1           1  32.37        62      1010  moderate rain  1437734494   \n",
       " 2           2  32.79        75      1009  moderate rain  1437738046   \n",
       " 3           3  32.75        79      1009  moderate rain  1437741580   \n",
       " 4           4  32.72        70      1008  moderate rain  1437745188   \n",
       " \n",
       "    wind_speed  wind_deg     city                  day  dist  \n",
       " 0        2.11   330.003  Ravenna  2015-07-24 11:40:51     8  \n",
       " 1        2.60    20.000  Ravenna  2015-07-24 12:41:34     8  \n",
       " 2        3.60    70.000  Ravenna  2015-07-24 13:40:46     8  \n",
       " 3        5.10    70.000  Ravenna  2015-07-24 14:39:40     8  \n",
       " 4        3.10    10.000  Ravenna  2015-07-24 15:39:48     8  ,\n",
       " ['Unnamed: 0',\n",
       "  'temp',\n",
       "  'humidity',\n",
       "  'pressure',\n",
       "  'description',\n",
       "  'dt',\n",
       "  'wind_speed',\n",
       "  'wind_deg',\n",
       "  'city',\n",
       "  'day',\n",
       "  'dist'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/ravenna_250715.csv')\n",
    "\n",
    "# Ensure the relevant columns exist if the data is unknown\n",
    "# Adjust column names as necessary for \"atmospheric pressure\" and \"wind speed\"\n",
    "columns_available = data.columns.tolist()\n",
    "data.head(), columns_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "24f814a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.34), np.float64(0.1023), 'not significant')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Pearson correlation coefficient for pressure and wind_speed\n",
    "r_value, p_value = pearsonr(data['pressure'], data['wind_speed'])\n",
    "relationship_significance = 'significant' if p_value < 0.05 else 'not significant'\n",
    "round(r_value, 2), round(p_value, 4), relationship_significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "db56cbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   temp  humidity  pressure    description          dt  \\\n",
       "0           0  32.18        54      1010  moderate rain  1437730851   \n",
       "1           1  32.37        62      1010  moderate rain  1437734494   \n",
       "2           2  32.79        75      1009  moderate rain  1437738046   \n",
       "3           3  32.75        79      1009  moderate rain  1437741580   \n",
       "4           4  32.72        70      1008  moderate rain  1437745188   \n",
       "\n",
       "   wind_speed  wind_deg     city                  day  dist  \n",
       "0        2.11   330.003  Ravenna  2015-07-24 11:40:51     8  \n",
       "1        2.60    20.000  Ravenna  2015-07-24 12:41:34     8  \n",
       "2        3.60    70.000  Ravenna  2015-07-24 13:40:46     8  \n",
       "3        5.10    70.000  Ravenna  2015-07-24 14:39:40     8  \n",
       "4        3.10    10.000  Ravenna  2015-07-24 15:39:48     8  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/ravenna_250715.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7a4581f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9811810902077764), np.float64(0.9165830498668011), 'normal')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing Shapiro-Wilk test for normality on the 'humidity' column\n",
    "alpha = 0.05\n",
    "shapiro_test_stat, shapiro_p_value = shapiro(data['humidity'])\n",
    "\n",
    "# Determine distribution type\n",
    "distribution_type = \"normal\" if shapiro_p_value > alpha else \"not normal\"\n",
    "\n",
    "shapiro_test_stat, shapiro_p_value, distribution_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "02a4cc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0   temp  humidity  pressure    description          dt  \\\n",
       "0           0  32.18        54      1010  moderate rain  1437730851   \n",
       "1           1  32.37        62      1010  moderate rain  1437734494   \n",
       "2           2  32.79        75      1009  moderate rain  1437738046   \n",
       "3           3  32.75        79      1009  moderate rain  1437741580   \n",
       "4           4  32.72        70      1008  moderate rain  1437745188   \n",
       "\n",
       "   wind_speed  wind_deg     city                  day  dist  \n",
       "0        2.11   330.003  Ravenna  2015-07-24 11:40:51     8  \n",
       "1        2.60    20.000  Ravenna  2015-07-24 12:41:34     8  \n",
       "2        3.60    70.000  Ravenna  2015-07-24 13:40:46     8  \n",
       "3        5.10    70.000  Ravenna  2015-07-24 14:39:40     8  \n",
       "4        3.10    10.000  Ravenna  2015-07-24 15:39:48     8  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/ravenna_250715.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "61ea309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0), array([], dtype=float64), np.float64(2.29), np.float64(1.15))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Z-score calculation and detect outliers\n",
    "wind_speed_zscores = zscore(data['wind_speed'])\n",
    "\n",
    "# Identify outliers using the Z-score threshold criteria\n",
    "data['is_outlier'] = (wind_speed_zscores > 3) | (wind_speed_zscores < -3)\n",
    "number_of_outliers = data['is_outlier'].sum()\n",
    "\n",
    "# Extract the values of the outliers\n",
    "outliers = data.loc[data['is_outlier'], 'wind_speed'].values\n",
    "\n",
    "# Remove the outliers from the dataset\n",
    "filtered_data = data.loc[~data['is_outlier'], :]\n",
    "\n",
    "# Calculate mean and standard deviation after outlier removal\n",
    "mean_wind_speed = round(filtered_data['wind_speed'].mean(), 2)\n",
    "std_deviation_wind_speed = round(filtered_data['wind_speed'].std(), 2)\n",
    "\n",
    "number_of_outliers, outliers, mean_wind_speed, std_deviation_wind_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2610164e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.6)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('./data/Current_Logan.csv')\n",
    "\n",
    "# Convert the 'JAMES LOGAN' column to numeric and drop NaN values (invalid/non-numeric values will be converted to NaN and dropped)\n",
    "data['JAMES LOGAN'] = pd.to_numeric(data['JAMES LOGAN'], errors='coerce')\n",
    "data = data.dropna(subset=['JAMES LOGAN'])\n",
    "\n",
    "# Calculate the mean of the 'JAMES LOGAN' column\n",
    "mean_wins = data['JAMES LOGAN'].mean()\n",
    "mean_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4c49ad0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(15.49), np.float64(2.68))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def calculate_metrics_after_outlier_removal(file_path, column_name, z_threshold=3):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate Z-scores for the specified column\n",
    "    df['z_score'] = zscore(df[column_name])\n",
    "    \n",
    "    # Identify and remove outliers based on Z-score threshold\n",
    "    filtered_df = df[(df['z_score'] >= -z_threshold) & (df['z_score'] <= z_threshold)]\n",
    "    \n",
    "    # Recalculate the mean and population standard deviation on the filtered data\n",
    "    mean_value = round(filtered_df[column_name].mean(), 2)\n",
    "    std_dev_value = round(filtered_df[column_name].std(ddof=0), 2)  # ddof=0 for population standard deviation\n",
    "    \n",
    "    return mean_value, std_dev_value\n",
    "\n",
    "file_path = './data/auto-mpg.csv'\n",
    "column_name = 'acceleration'\n",
    "mean_acceleration, std_acceleration = calculate_metrics_after_outlier_removal(file_path, column_name)\n",
    "mean_acceleration, std_acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "87d5e4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------------------------------------------------------------------------\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
       "   3811 try:\n",
       "-> 3812     return self._engine.get_loc(casted_key)\n",
       "   3813 except KeyError as err:\n",
       "\n",
       "File pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
       "\n",
       "KeyError: 'GDP_per_capita'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "KeyError                                  Traceback (most recent call last)\n",
       "Cell In[346], line 9\n",
       "      6 df = pd.read_csv('./data/gapminder_cleaned.csv')\n",
       "      8 # Ensure no missing values in GDP per capita column\n",
       "----> 9 df['GDP_per_capita'] = df['GDP_per_capita'].fillna(0)  # Replacing NaN with 0 for this step (any invalid will be fixed later with mask)\n",
       "     10 df.head()\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
       "   4111 if self.columns.nlevels > 1:\n",
       "   4112     return self._getitem_multilevel(key)\n",
       "-> 4113 indexer = self.columns.get_loc(key)\n",
       "   4114 if is_integer(indexer):\n",
       "   4115     indexer = [indexer]\n",
       "\n",
       "File /opt/miniconda3/envs/dab-step/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n",
       "   3814     if isinstance(casted_key, slice) or (\n",
       "   3815         isinstance(casted_key, abc.Iterable)\n",
       "   3816         and any(isinstance(x, slice) for x in casted_key)\n",
       "   3817     ):\n",
       "   3818         raise InvalidIndexError(key)\n",
       "-> 3819     raise KeyError(key) from err\n",
       "   3820 except TypeError:\n",
       "   3821     # If we have a listlike key, _check_indexing_error will raise\n",
       "   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
       "   3823     #  the TypeError.\n",
       "   3824     self._check_indexing_error(key)\n",
       "\n",
       "KeyError: 'GDP_per_capita'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the necessary dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/gapminder_cleaned.csv')\n",
    "\n",
    "# Ensure no missing values in GDP per capita column\n",
    "df['GDP_per_capita'] = df['GDP_per_capita'].fillna(0)  # Replacing NaN with 0 for this step (any invalid will be fixed later with mask)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2850a3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.False_, np.float64(3.54), np.float64(0.54))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correcting the column name for GDP per capita and proceeding with the transformation\n",
    "import numpy as np\n",
    "\n",
    "# Applying the log transformation on the \"gdppercap\" column\n",
    "df['log_gdp_per_capita'] = np.log10(df['gdppercap'])\n",
    "\n",
    "# Checking for NaN values, mean, and standard deviation in the new feature\n",
    "has_nan_values = df['log_gdp_per_capita'].isna().any()\n",
    "new_feature_mean = round(df['log_gdp_per_capita'].mean(), 2)\n",
    "new_feature_std = round(df['log_gdp_per_capita'].std(), 2)\n",
    "\n",
    "(has_nan_values, new_feature_mean, new_feature_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d70dccb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Cell In[13], line 1\n",
       "    @removed_outliers_count[0]\n",
       "                              ^\n",
       "SyntaxError: incomplete input\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@removed_outliers_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "513481d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Cell In[14], line 3\n",
       "    @removed_outliers_count[{}] = removed_outliers_count\n",
       "     ^\n",
       "SyntaxError: cannot assign to subscript here. Maybe you meant '==' instead of '='?\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outputting the count of removed outliers\n",
    "removed_outliers_count = removed_outliers_count.item()  # Convert np.int64 to int\n",
    "@removed_outliers_count[{}] = removed_outliers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "0d684ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@removed_outliers_count[0]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outputting the count of removed outliers\n",
    "removed_outliers_count = removed_outliers_count.item()  \n",
    "print(f'@removed_outliers_count[{removed_outliers_count}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "55cccab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-7.299079423719196), np.float64(2.6349482264279254))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the p-value is less than 0.05, we will apply a log transformation\n",
    "import numpy as np\n",
    "\n",
    "# Apply natural log transformation to 'importance.score' column, adding a small constant to avoid log(0)\n",
    "data['log_importance_score'] = np.log(data['importance.score'] + 1e-10)\n",
    "\n",
    "# Calculate mean and standard deviation of the transformed column\n",
    "transformed_mean = data['log_importance_score'].mean()\n",
    "transformed_std = data['log_importance_score'].std()\n",
    "\n",
    "transformed_mean, transformed_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "6db51f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No duplicates found.\n",
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 566 entries, 0 to 565\n",
       "Data columns (total 7 columns):\n",
       " #   Column            Non-Null Count  Dtype  \n",
       "---  ------            --------------  -----  \n",
       " 0   site              566 non-null    object \n",
       " 1   abs_diffsel       566 non-null    float64\n",
       " 2   positive_diffsel  566 non-null    float64\n",
       " 3   negative_diffsel  566 non-null    float64\n",
       " 4   max_diffsel       566 non-null    float64\n",
       " 5   min_diffsel       566 non-null    float64\n",
       " 6   diff_range        566 non-null    float64\n",
       "dtypes: float64(6), object(1)\n",
       "memory usage: 31.1+ KB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "       site  abs_diffsel  positive_diffsel  negative_diffsel  max_diffsel  \\\n",
       "0  (HA2)121     9.026365          4.147102         -4.879263     1.578739   \n",
       "1       326     9.002765          3.615601         -5.387164     0.716922   \n",
       "2       280     8.418638          5.146938         -3.271700     0.971071   \n",
       "3         9     8.185717          4.420441         -3.765276     1.000554   \n",
       "4       210     8.058663          5.824386         -2.234277     1.378964   \n",
       "\n",
       "   min_diffsel  diff_range  \n",
       "0    -1.004167    2.582906  \n",
       "1    -1.218422    1.935344  \n",
       "2    -1.018267    1.989339  \n",
       "3    -0.847152    1.847706  \n",
       "4    -1.240547    2.619511  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the dataset\n",
    "# Since there are no missing values, we will proceed with removing duplicates\n",
    "if duplicates_count > 0:\n",
    "    df = df.drop_duplicates()\n",
    "else:\n",
    "    print('No duplicates found.')  \n",
    "\n",
    "# Create the 'diff_range' feature  \n",
    "df['diff_range'] = df['max_diffsel'] - df['min_diffsel']  \n",
    "\n",
    "# Check the final cleaned dataset structure\n",
    "cleaned_data_structure = df.info()  \n",
    "cleaned_data_structure  \n",
    "\n",
    "# Display the cleaned dataset\n",
    "cleaned_data_head = df.head()  \n",
    "cleaned_data_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8c04b414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@correlation_coefficient[0.7366]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Pearson correlation coefficient between 'MedInd' and 'LarInd'\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlation_coefficient, _ = pearsonr(data['MedInd'], data['LarInd'])\n",
    "\n",
    "# Round the coefficient to 4 decimal places\n",
    "correlation_coefficient = round(correlation_coefficient, 4)\n",
    "\n",
    "# Output the result in the expected format\n",
    "f'@correlation_coefficient[{correlation_coefficient}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "0d2cb386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 3141 entries, 0 to 3140\n",
       "Data columns (total 10 columns):\n",
       " #   Column          Non-Null Count  Dtype  \n",
       "---  ------          --------------  -----  \n",
       " 0   votes_dem       3141 non-null   float64\n",
       " 1   votes_gop       3141 non-null   float64\n",
       " 2   total_votes     3141 non-null   float64\n",
       " 3   per_dem         3141 non-null   float64\n",
       " 4   per_gop         3141 non-null   float64\n",
       " 5   diff            3141 non-null   object \n",
       " 6   per_point_diff  3141 non-null   object \n",
       " 7   state_abbr      3141 non-null   object \n",
       " 8   county_name     3141 non-null   object \n",
       " 9   combined_fips   3141 non-null   int64  \n",
       "dtypes: float64(5), int64(1), object(4)\n",
       "memory usage: 245.5+ KB\n",
       "None\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "   votes_dem  votes_gop  total_votes   per_dem  per_gop    diff  \\\n",
       "0    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "1    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "2    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "3    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "4    93003.0   130413.0     246588.0  0.377159  0.52887  37,410   \n",
       "\n",
       "  per_point_diff state_abbr county_name  combined_fips  \n",
       "0         15.17%         AK      Alaska           2013  \n",
       "1         15.17%         AK      Alaska           2016  \n",
       "2         15.17%         AK      Alaska           2020  \n",
       "3         15.17%         AK      Alaska           2050  \n",
       "4         15.17%         AK      Alaska           2060  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load data\n",
    "file_path = './data/election2016.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show basic info and first few rows\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "97500956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 891 entries, 0 to 890\n",
       "Data columns (total 12 columns):\n",
       " #   Column       Non-Null Count  Dtype  \n",
       "---  ------       --------------  -----  \n",
       " 0   PassengerId  891 non-null    int64  \n",
       " 1   Survived     891 non-null    int64  \n",
       " 2   Pclass       891 non-null    int64  \n",
       " 3   Name         891 non-null    object \n",
       " 4   Sex          891 non-null    object \n",
       " 5   Age          714 non-null    float64\n",
       " 6   SibSp        891 non-null    int64  \n",
       " 7   Parch        891 non-null    int64  \n",
       " 8   Ticket       891 non-null    object \n",
       " 9   Fare         891 non-null    float64\n",
       " 10  Cabin        204 non-null    object \n",
       " 11  Embarked     889 non-null    object \n",
       "dtypes: float64(2), int64(5), object(5)\n",
       "memory usage: 83.7+ KB\n",
       "None\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# Display basic info and first few rows to understand the structure\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "98c8bcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 891 entries, 0 to 890\n",
       "Data columns (total 12 columns):\n",
       " #   Column       Non-Null Count  Dtype  \n",
       "---  ------       --------------  -----  \n",
       " 0   PassengerId  891 non-null    int64  \n",
       " 1   Survived     891 non-null    int64  \n",
       " 2   Pclass       891 non-null    int64  \n",
       " 3   Name         891 non-null    object \n",
       " 4   Sex          891 non-null    object \n",
       " 5   Age          714 non-null    float64\n",
       " 6   SibSp        891 non-null    int64  \n",
       " 7   Parch        891 non-null    int64  \n",
       " 8   Ticket       891 non-null    object \n",
       " 9   Fare         891 non-null    float64\n",
       " 10  Cabin        204 non-null    object \n",
       " 11  Embarked     889 non-null    object \n",
       "dtypes: float64(2), int64(5), object(5)\n",
       "memory usage: 83.7+ KB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "    PassengerId  Survived  Pclass  \\\n",
       " 0            1         0       3   \n",
       " 1            2         1       1   \n",
       " 2            3         1       3   \n",
       " 3            4         1       1   \n",
       " 4            5         0       3   \n",
       " \n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       " 0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       " 1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       " 2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       " 3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       " 4                           Allen, Mr. William Henry    male  35.0      0   \n",
       " \n",
       "    Parch            Ticket     Fare Cabin Embarked  \n",
       " 0      0         A/5 21171   7.2500   NaN        S  \n",
       " 1      0          PC 17599  71.2833   C85        C  \n",
       " 2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       " 3      0            113803  53.1000  C123        S  \n",
       " 4      0            373450   8.0500   NaN        S  )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/titanic_train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information and first few rows to understand the data\n",
    "(df.info(), df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e1dee651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Year  Agriculture  Architecture  Art and Performance    Biology   Business  \\\n",
       "0  1970     4.229798     11.921005                 59.7  29.088363   9.064439   \n",
       "1  1971     5.452797     12.003106                 59.9  29.394403   9.503187   \n",
       "2  1972     7.420710     13.214594                 60.4  29.810221  10.558962   \n",
       "3  1973     9.653602     14.791613                 60.2  31.147915  12.804602   \n",
       "4  1974    14.074623     17.444688                 61.9  32.996183  16.204850   \n",
       "\n",
       "   Communications and Journalism  Computer Science  Education  Engineering  \\\n",
       "0                           35.3              13.6  74.535328          0.8   \n",
       "1                           35.5              13.6  74.149204          1.0   \n",
       "2                           36.6              14.9  73.554520          1.2   \n",
       "3                           38.4              16.4  73.501814          1.6   \n",
       "4                           40.5              18.9  73.336811          2.2   \n",
       "\n",
       "     English  Foreign Languages  Health Professions  Math and Statistics  \\\n",
       "0  65.570923               73.8                77.1                 38.0   \n",
       "1  64.556485               73.9                75.5                 39.0   \n",
       "2  63.664263               74.6                76.9                 40.2   \n",
       "3  62.941502               74.9                77.4                 40.9   \n",
       "4  62.413412               75.3                77.9                 41.8   \n",
       "\n",
       "   Physical Sciences  Psychology  Public Administration  \\\n",
       "0               13.8        44.4                   68.4   \n",
       "1               14.9        46.2                   65.5   \n",
       "2               14.8        47.6                   62.6   \n",
       "3               16.5        50.4                   64.3   \n",
       "4               18.2        52.6                   66.1   \n",
       "\n",
       "   Social Sciences and History  \n",
       "0                         36.8  \n",
       "1                         36.2  \n",
       "2                         36.1  \n",
       "3                         36.4  \n",
       "4                         37.3  \n",
       "Index(['Year', 'Agriculture', 'Architecture', 'Art and Performance', 'Biology',\n",
       "       'Business', 'Communications and Journalism', 'Computer Science',\n",
       "       'Education', 'Engineering', 'English', 'Foreign Languages',\n",
       "       'Health Professions', 'Math and Statistics', 'Physical Sciences',\n",
       "       'Psychology', 'Public Administration', 'Social Sciences and History'],\n",
       "      dtype='object')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data/percent-bachelors-degrees-women-usa.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and columns to understand the structure\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "95d00e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/titanic.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate Z-scores for the 'Fare' column\n",
    "z_scores = zscore(df['Fare'].dropna())\n",
    "\n",
    "# Identify outliers based on Z-score > 3\n",
    "outliers = abs(z_scores) > 3\n",
    "\n",
    "# Remove outliers from the dataframe\n",
    "# Use the index from the original dataframe associated with the Z-score calculation\n",
    "non_outlier_df = df.loc[df['Fare'].dropna().index[~outliers]]\n",
    "\n",
    "# Calculate the number of entries left after outlier removal\n",
    "number_of_entries_left = non_outlier_df.shape[0]\n",
    "\n",
    "number_of_entries_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "6314176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 4177 entries, 0 to 4176\n",
       "Data columns (total 9 columns):\n",
       " #   Column          Non-Null Count  Dtype  \n",
       "---  ------          --------------  -----  \n",
       " 0   Sex             4177 non-null   object \n",
       " 1   Length          4177 non-null   float64\n",
       " 2   Diameter        4177 non-null   float64\n",
       " 3   Height          4177 non-null   float64\n",
       " 4   Whole weight    4177 non-null   float64\n",
       " 5   Shucked weight  4177 non-null   float64\n",
       " 6   Viscera weight  4177 non-null   float64\n",
       " 7   Shell weight    4177 non-null   float64\n",
       " 8   Rings           4177 non-null   int64  \n",
       "dtypes: float64(7), int64(1), object(1)\n",
       "memory usage: 293.8+ KB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "   Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       " 0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       " 1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       " 2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       " 3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       " 4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       " \n",
       "    Shell weight  Rings  \n",
       " 0         0.150     15  \n",
       " 1         0.070      7  \n",
       " 2         0.210      9  \n",
       " 3         0.155     10  \n",
       " 4         0.055      7  )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load and examine the abalone dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/abalone.csv')\n",
    "\n",
    "# Display basic information and first few rows\n",
    "(df.info(), df.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
